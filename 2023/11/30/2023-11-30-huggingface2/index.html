

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Bill Ding">
  <meta name="keywords" content="">
  
    <meta name="description" content="æ­å¼€å®ƒç¥ç§˜çš„é¢çº±">
<meta property="og:type" content="article">
<meta property="og:title" content="huggingfaceä¸‹llamaä»£ç ç»†è¯»ï¼ˆä¸‹ï¼‰">
<meta property="og:url" content="https://dingfen.github.io/2023/11/30/2023-11-30-huggingface2/index.html">
<meta property="og:site_name" content="å³°å­çš„ä¹å›­">
<meta property="og:description" content="æ­å¼€å®ƒç¥ç§˜çš„é¢çº±">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://dingfen.github.io/img/LLM/llama.png">
<meta property="article:published_time" content="2023-11-30T04:00:00.000Z">
<meta property="article:modified_time" content="2025-01-26T11:49:09.209Z">
<meta property="article:author" content="Bill Ding">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://dingfen.github.io/img/LLM/llama.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>huggingfaceä¸‹llamaä»£ç ç»†è¯»ï¼ˆä¸‹ï¼‰ - å³°å­çš„ä¹å›­</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/KaTeX/0.16.2/katex.min.css" />



<!-- ä¸»é¢˜ä¾èµ–çš„å›¾æ ‡åº“ï¼Œä¸è¦è‡ªè¡Œä¿®æ”¹ -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"dingfen.github.io","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"|","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":null,"onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.1.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>å³°å­çš„ä¹å›­</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>é¦–é¡µ</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>å½’æ¡£</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>åˆ†ç±»</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>æ ‡ç­¾</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>å…³äº</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="huggingfaceä¸‹llamaä»£ç ç»†è¯»ï¼ˆä¸‹ï¼‰"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-11-30 12:00" pubdate>
          2023å¹´11æœˆ30æ—¥ ä¸­åˆ
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          3.4k å­—
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          29 åˆ†é’Ÿ
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">huggingfaceä¸‹llamaä»£ç ç»†è¯»ï¼ˆä¸‹ï¼‰</h1>
            
              <p id="updated-time" class="note note-info" style="display: none">
                
                  
                    <!-- compatible with older versions-->
                    æ›´æ–°äºï¼š2025-01-26T19:49:09+08:00
                  
                  

                
              </p>
            
            
              <div class="markdown-body">
                
                <h1>huggingfaceä¸‹llamaä»£ç ç»†è¯»ï¼ˆä¸‹ï¼‰</h1>
<h2 id="å‰è¨€">å‰è¨€</h2>
<p>ä¸Šç¯‡åšå®¢æˆ‘ä»¬é‡ç‚¹ä»‹ç»äº† llama æ¨¡å‹ï¼Œå¹¶è®¨è®ºäº†å®ƒçš„æ¶æ„ã€åŸºä»¶å’Œä¸­é—´ä»¶ç­‰ã€‚ç¢äºç¯‡å¹…å…³ç³»ï¼Œæˆ‘å°† transformer llama çš„ä»£ç è§£è¯»ä¸‹åŠéƒ¨åˆ†ç§»åŠ¨åˆ°äº†æœ¬ç¯‡åšå®¢ä¸­ï¼Œè¦æƒ³ä»å¤´å¼€å§‹çš„è¯»è€…ä»¬å¯ä»¥å‚è€ƒ<a href="https://dingfen.github.io/ai/2023/10/30/huggingface1.html">è¿™ç¯‡åšå®¢</a>ã€‚</p>
<h2 id="llama-æ¨¡å‹">llama æ¨¡å‹</h2>
<center>
<img src="/img/LLM/llama.png" srcset="/img/loading.gif" lazyload>
</center>
<h3 id="è¯‘ç å±‚">è¯‘ç å±‚</h3>
<p>åœ¨äº†è§£äº†æ„æˆ llama çš„åŸºæœ¬ç»„ä»¶åï¼Œè¦å¦‚ä½•æ­å»ºèµ·å¤§æ¨¡å‹çš„â€œé«˜æ¥¼å¤§å¦â€ï¼Ÿå½“ç„¶ä¸èƒ½ä¸€æ­¥ç™»å¤©ï¼Œè€Œè¦æ­¥æ­¥ä¸ºè¥ã€‚åœ¨å¤§æ¨¡å‹æ¨ç†é˜¶æ®µï¼Œè¾“å…¥çš„æ–‡æœ¬åºåˆ—ä¼šç»è¿‡å¤šä¸ªè¯‘ç å±‚ï¼Œæ‰§è¡Œè‡ªæ³¨æ„åŠ›ç­‰è¿ç®—ã€‚è¯‘ç å±‚ç”± <code>LlamaDecoderLayer</code> ç±»è¡¨ç¤ºï¼Œå®ƒå°† <code>LlamaAttention</code> <code>LlamaRMSNorm</code> ç­‰åŸºä»¶ç»„åˆèµ·æ¥ã€‚ä¸Šå›¾æ‰€å±•ç¤ºçš„æ¶æ„å°±æ˜¯ä¸€ä¸ªè¯‘ç å±‚çš„æ¶æ„ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LlamaDecoderLayer</span>(nn.Module):<br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config: LlamaConfig</span>):<br>    <span class="hljs-built_in">super</span>().__init__()<br>    self.hidden_size = config.hidden_size<br>    self.self_attn = (<br>        LlamaAttention(config=config)<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">getattr</span>(config, <span class="hljs-string">&quot;_flash_attn_2_enabled&quot;</span>, <span class="hljs-literal">False</span>)<br>        <span class="hljs-keyword">else</span> LlamaFlashAttention2(config=config)<br>    )<br>    self.mlp = LlamaMLP(config)<br>    self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)<br>    self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)<br></code></pre></td></tr></table></figure>
<p>ä¸Šé¢çš„æ„é€ å‡½æ•°ç½—åˆ—äº†è¯‘ç å±‚çš„å‡ ä¸ªç»„ä»¶ï¼šè‡ªæ³¨æ„åŠ›å±‚ï¼ŒMLP å±‚å’Œä¸¤ä¸ª RMSNorm å±‚ï¼Œè€Œå…¶ forward å‡½æ•°åˆ™æ›´è¯¦ç»†åœ°å±•ç¤ºäº†æ¶æ„å›¾å†…å¼ é‡çš„æ‰§è¡Œæƒ…å†µï¼š13 è¡Œå…ˆå°†è¾“å…¥æ‰§è¡Œä¸€æ¬¡ RMSNorm å½’ä¸€ï¼Œ16-25 è¡Œæ‰§è¡Œä¸€æ¬¡æ³¨æ„åŠ›è¿ç®—ï¼ŒåŠ ä¸Šäº†æ®‹å·®ç»“æ„ï¼Œå†æ‰§è¡Œä¸€æ¬¡ RMSNorm å½’ä¸€ï¼Œ27-33 è¡Œå°†æ³¨æ„åŠ›ç»“æœè¾“å‡ºåˆ° MLP ä¸­ï¼Œæœ€åè¿”å›ç»“æœğŸ‘‡ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">  self,</span><br><span class="hljs-params">  hidden_states: torch.Tensor,</span><br><span class="hljs-params">  attention_mask: <span class="hljs-type">Optional</span>[torch.Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">  position_ids: <span class="hljs-type">Optional</span>[torch.LongTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">  past_key_value: <span class="hljs-type">Optional</span>[<span class="hljs-type">Tuple</span>[torch.Tensor]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">  output_attentions: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">  use_cache: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">  **kwargs,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Tuple</span>[torch.FloatTensor, <span class="hljs-type">Optional</span>[<span class="hljs-type">Tuple</span>[torch.FloatTensor, torch.FloatTensor]]]:<br><br>  residual = hidden_states<br><br>  hidden_states = self.input_layernorm(hidden_states)<br><br>  <span class="hljs-comment"># Self Attention</span><br>  hidden_states, self_attn_weights, present_key_value = self.self_attn(<br>      hidden_states=hidden_states,<br>      attention_mask=attention_mask,<br>      position_ids=position_ids,<br>      past_key_value=past_key_value,<br>      output_attentions=output_attentions,<br>      use_cache=use_cache,<br>      **kwargs,<br>  )<br>  hidden_states = residual + hidden_states<br><br>  <span class="hljs-comment"># Fully Connected</span><br>  residual = hidden_states<br>  hidden_states = self.post_attention_layernorm(hidden_states)<br>  hidden_states = self.mlp(hidden_states)<br>  hidden_states = residual + hidden_states<br><br>  outputs = (hidden_states,)<br><br>  <span class="hljs-keyword">if</span> output_attentions:<br>      outputs += (self_attn_weights,)<br><br>  <span class="hljs-keyword">if</span> use_cache:<br>      outputs += (present_key_value,)<br><br>  <span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure>
<h3 id="llama-Model">llama Model</h3>
<p><code>LlamaModel</code> ç”±ä¸Šé¢ä»‹ç»çš„å¤šä¸ª <code>LlamaDecoderLayer</code> å †å è€Œæˆã€‚ä»¥ä¹‹å‰çš„ llama-7b-hf å‚æ•°ä¸ºä¾‹ï¼Œ<code>num_hidden_layers</code> ä¸º32ï¼Œæ„æ€æ˜¯è¯¥æ¨¡å‹ä¸€å…±å †å äº† 32 å±‚è¯‘ç å±‚ã€‚</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>  <span class="hljs-attr">&quot;architecture&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-string">&quot;LlamaForCausalLM&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;hidden_act&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;silu&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;hidden_size&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">4096</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;initializer_range&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">0.02</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;intermediate_size&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">11008</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;max_position_embeddings&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">4096</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;model_type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;llama&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;num_attention_heads&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">32</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;num_hidden_layers&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">32</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;num_key_value_heads&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">32</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;pretraining_tp&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">2</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;rms_norm_eps&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">1e-05</span><span class="hljs-punctuation">,</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure>
<p>é™¤æ­¤ä¹‹å¤–ï¼ŒLlama æ¨¡å‹åœ¨è¾“å…¥çš„æœ€å‰å¤´åŠ å…¥äº†ä¸€ä¸ªåµŒå…¥å±‚ï¼ˆEmbeddingï¼‰ï¼Œæœ€ååˆåŠ äº†ä¸€å±‚ RMSNorm è¿›è¡Œå½’ä¸€ï¼Œä¸‹é¢æ˜¯å®ƒçš„æ„é€ å‡½æ•°ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LlamaModel</span>(<span class="hljs-title class_ inherited__">LlamaPreTrainedModel</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config: LlamaConfig</span>):<br>    <span class="hljs-built_in">super</span>().__init__(config)<br>    self.padding_idx = config.pad_token_id<br>    self.vocab_size = config.vocab_size<br><br>    self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)<br>    self.layers = nn.ModuleList([LlamaDecoderLayer(config) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(config.num_hidden_layers)])<br>    self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)<br><br>    self.gradient_checkpointing = <span class="hljs-literal">False</span><br>    <span class="hljs-comment"># Initialize weights and apply final processing</span><br>    self.post_init()<br></code></pre></td></tr></table></figure>
<p><code>LlamaModel</code> ç±»ç»§æ‰¿è‡ª <code>LlamaPreTrainedModel</code>ï¼Œ<code>LlamaPreTrainedModel</code> æ²¡æœ‰é‚£ä¹ˆç¥ç§˜ï¼Œåªä¸è¿‡æ˜¯åœ¨ <code>LlamaDecoderLayer</code> çš„åŸºç¡€ä¸ŠåŒ…è£¹äº†ä¸€äº›åˆå§‹åŒ–æ“ä½œè€Œå·²ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LlamaPreTrainedModel</span>(<span class="hljs-title class_ inherited__">PreTrainedModel</span>):<br>  config_class = LlamaConfig<br>  base_model_prefix = <span class="hljs-string">&quot;model&quot;</span><br>  supports_gradient_checkpointing = <span class="hljs-literal">True</span><br>  _no_split_modules = [<span class="hljs-string">&quot;LlamaDecoderLayer&quot;</span>]<br>  _skip_keys_device_placement = <span class="hljs-string">&quot;past_key_values&quot;</span><br>  _supports_flash_attn_2 = <span class="hljs-literal">True</span><br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">_init_weights</span>(<span class="hljs-params">self, module</span>):<br>    std = self.config.initializer_range<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(module, nn.Linear):<br>        module.weight.data.normal_(mean=<span class="hljs-number">0.0</span>, std=std)<br>        <span class="hljs-keyword">if</span> module.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            module.bias.data.zero_()<br>    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(module, nn.Embedding):<br>        module.weight.data.normal_(mean=<span class="hljs-number">0.0</span>, std=std)<br>        <span class="hljs-keyword">if</span> module.padding_idx <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            module.weight.data[module.padding_idx].zero_()<br></code></pre></td></tr></table></figure>
<h4 id="Llama-æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹">Llama æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹</h4>
<p>æˆ‘ä»¬ä» <code>LlamaModel</code> ä»£ç ä¸­å·²ç»äº†è§£åˆ°ï¼ŒLlama æ¨¡å‹å°† 32 å±‚è¯‘ç å±‚å †å èµ·æ¥ï¼Œè¾“å…¥çš„æ–‡æœ¬åºåˆ—ç»è¿‡ä¸€å±‚å±‚è¯‘ç è¢«æœ€ç»ˆè½¬åŒ–æˆè¾“å‡ºåºåˆ—ã€‚è€Œ <code>LlamaModel::forward</code> ä½œä¸ºæ•´ä¸ªå¤§æ¨¡å‹â€œæœªå°è£…çš„â€å…¥å£ï¼Œæ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚</p>
<p><strong>å‚æ•°ä¸€è§ˆ</strong></p>
<p>æ—¢ç„¶æ˜¯ä¸ªå…¥å£ï¼Œæˆ‘ä»¬é¦–å…ˆä»å®ƒçš„å‚æ•°å…¥æ‰‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    input_ids: torch.LongTensor = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    attention_mask: <span class="hljs-type">Optional</span>[torch.Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    position_ids: <span class="hljs-type">Optional</span>[torch.LongTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    past_key_values: <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[torch.FloatTensor]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    inputs_embeds: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    use_cache: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    output_attentions: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    output_hidden_states: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    return_dict: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">  </span>) -&gt; <span class="hljs-type">Union</span>[<span class="hljs-type">Tuple</span>, BaseModelOutputWithPast]:<br></code></pre></td></tr></table></figure>
<ul>
<li>input_ids å¯ç®€å•ç†è§£ä¸ºè¾“å…¥çš„æ–‡æœ¬åºåˆ—</li>
<li>attention_mask æ³¨æ„åŠ›æ©ç ï¼Œ1 è¡¨ç¤ºæœªé®æ©ï¼Œ0 è¡¨ç¤ºé®æ©ã€‚é€šå¸¸ä¼šä½¿ç”¨ä¸‹ä¸‰è§’çŸ©é˜µå¯¹è¾“å‡ºè¿›è¡Œé®ç›–ï¼Œé˜²æ­¢æ¨¡å‹ä½œå¼Šã€‚</li>
<li>position_ids è¾“å…¥æ–‡æœ¬åºåˆ—çš„ä½ç½®ç¼–å·ï¼Œä» 0 å¼€å§‹</li>
<li>past_key_values è‹¥ use_cache ä¸ºçœŸåˆ™ä¹‹å‰çš„kvå€¼ä¼šè¢«ç”¨äºåŠ é€Ÿæ¨ç†</li>
<li>inputs_embeds æ¨¡å‹æ”¯æŒç›´æ¥ä¼ å…¥ input çš„åµŒå…¥å¼ é‡ï¼Œä»£æ›¿ input_ids</li>
<li>use_cache æ˜¯å¦ä½¿ç”¨ KV cache åŠ é€Ÿæ¨ç†ï¼Œé€šè¿‡ä½¿ç”¨ cache ç¼“å­˜æƒé‡ç­‰å€¼åŠ é€Ÿæ¨ç†</li>
<li>output_attentions æ˜¯å¦è¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡</li>
<li>output_hidden_states æ˜¯å¦è¦è¿”å›æ‰€æœ‰å±‚çš„éšè—å±‚çŠ¶æ€å¼ é‡</li>
<li>return_dict æŒ‡ç¤ºè¿”å›çš„ç±»å‹æ˜¯ <code>~utils.ModelOutput</code> è¿˜æ˜¯ tuple</li>
</ul>
<p><strong>masked Attention</strong></p>
<p>æ¥ä¸‹æ¥çœ‹çœ‹ forward çš„å®ç°ã€‚ç•¥å»é”™è¯¯æœºåˆ¶å’Œå…¶ä»–å‡†å¤‡è¿‡ç¨‹ï¼Œæ¥çœ‹çœ‹ç¥ç§˜çš„ <code>attention_mask</code> å¦‚ä½•è¢«å‡†å¤‡çš„ã€‚å›é¡¾ä¸€ä¸‹ transformers æœºåˆ¶é‡Œçš„æ³¨æ„åŠ›æ©ç ï¼Œå®ƒæ˜¯ç”¨æ¥åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶é®æŒ¡åç»­éƒ¨åˆ†çš„è¾“å‡ºï¼Œé˜²æ­¢æ¨¡å‹çœ‹åˆ°æœªæ¥çš„è¾“å‡ºè€Œâ€œä½œå¼Šâ€ç”¨çš„ã€‚ä»å…¬å¼çš„è§’åº¦çœ‹ï¼Œ$ MaskAttn=softmax(\frac{QK^T}{\sqrt{d_k}}+masked)V $ ï¼Œå…¬å¼çš„å‰åŠéƒ¨åˆ†ä¸»è¦åœ¨è®¡ç®—æ³¨æ„åŠ›çŸ©é˜µï¼Œè€Œ <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>æ˜¯æˆ‘ä»¬è¾“å…¥çš„çŸ©é˜µï¼Œæ—¢ç„¶æ©ç æ˜¯ä¸ºäº†é˜²æ­¢<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>å·çœ‹åˆ°æœªæ¥è¾“å‡ºçš„æƒé‡çš„ï¼Œé‚£ä¹ˆæ©ç å°±åº”è¯¥æ˜¯ä¸‹ä¸‰è§’çŸ©é˜µï¼Œå³ä¸Šéƒ¨åˆ†åº”è¯¥ç½®ä¸ºå¾ˆå°çš„è´Ÿæ•°ã€‚</p>
<p>è¿™ç‚¹ä»ä»£ç ä¸­ä¹Ÿå¯ä»¥å¾—åˆ°éªŒè¯ï¼Œåœ¨æ­¤ä¸å¾—ä¸æ„Ÿæ…¨ä»£ç æ›´æ–°é€Ÿåº¦ä¹‹å¿«ï¼šç°åœ¨æœ€æ–°ç‰ˆæœ¬çš„ transformers ä»£ç ä½¿ç”¨ <code>_prepare_4d_causal_attention_mask</code>ï¼Œè€Œ 4.37 ä¹‹å‰çš„ç‰ˆæœ¬ä½¿ç”¨çš„å‡½æ•°è¿˜æ˜¯ <code>_expand_mask</code>ã€<code>_make_causal_mask</code> ğŸ™Œï¼š</p>
<p><code>input_ids</code> æ˜¯æˆ‘ä»¬è¾“å…¥çš„æ–‡æœ¬çŸ©é˜µï¼Œç»´åº¦é€šå¸¸æ˜¯ <code>(batch_size, seq_len)</code>ã€‚å› æ­¤ä»£ç å‰å››è¡Œä¹Ÿæ˜¯å¦‚æ­¤æå–å‡º <code>batch_size</code> å’Œ <code>seq_len</code> çš„ã€‚<code>position_ios</code> ç»™æˆ‘ä»¬è¾“å…¥çš„æ–‡æœ¬å•è¯ä» 0 æˆ– <code>past_key_values_length</code> å¼€å§‹ç¼–å·ã€‚éšåï¼Œ15 è¡Œå°† <code>input_ids</code> æ¨å…¥åµŒå…¥å±‚ï¼Œæ¨ç†æ­£å¼å¼€å§‹ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># ... to be continued some are omitted</span><br>  input_ids <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>    batch_size, seq_length = input_ids.shape[:<span class="hljs-number">2</span>]<br>  <span class="hljs-keyword">elif</span> inputs_embeds <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>    batch_size, seq_length = inputs_embeds.shape[:<span class="hljs-number">2</span>]<br><br>  past_key_values_length = past_key_values[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>].shape[<span class="hljs-number">2</span>]<br><br>  <span class="hljs-keyword">if</span> position_ids <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>    position_ids = torch.arange(<br>        past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device<br>    )<br>    position_ids = position_ids.unsqueeze(<span class="hljs-number">0</span>)<br><br>  <span class="hljs-keyword">if</span> inputs_embeds <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>    inputs_embeds = self.embed_tokens(input_ids)<br><br>  <span class="hljs-keyword">if</span> <span class="hljs-built_in">getattr</span>(self.config, <span class="hljs-string">&quot;_flash_attn_2_enabled&quot;</span>, <span class="hljs-literal">False</span>):<br>    <span class="hljs-comment"># 2d mask is passed through the layers</span><br>    attention_mask = attention_mask <span class="hljs-keyword">if</span> (attention_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> <span class="hljs-number">0</span> <span class="hljs-keyword">in</span> attention_mask) <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>  <span class="hljs-keyword">else</span>:<br>    <span class="hljs-comment"># 4d mask is passed through the layers</span><br>    attention_mask = _prepare_4d_causal_attention_mask(<br>      attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length<br>    )<br><br>  <span class="hljs-comment"># embed positions</span><br>  hidden_states = inputs_embeds<br></code></pre></td></tr></table></figure>
<p>å†ä¹‹åï¼Œä»£ç å¼€å§‹ä½¿ç”¨åº“å†…å‡½æ•°å‡†å¤‡æ³¨æ„åŠ›æ©ç äº†ï¼Œè¡Œï¼Œé‚£å°±è®©æˆ‘ä»¬çœ‹çœ‹ <code>_prepare_4d_causal_attention_mask</code> å‡½æ•°é‡Œåˆ°åº•å–çš„ä»€ä¹ˆè¯ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_prepare_4d_causal_attention_mask</span>(<span class="hljs-params"></span><br><span class="hljs-params">  attention_mask: <span class="hljs-type">Optional</span>[torch.Tensor],</span><br><span class="hljs-params">  input_shape: <span class="hljs-type">Union</span>[torch.Size, <span class="hljs-type">Tuple</span>, <span class="hljs-type">List</span>],</span><br><span class="hljs-params">  inputs_embeds: torch.Tensor,</span><br><span class="hljs-params">  past_key_values_length: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">  sliding_window: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>):<br>  attn_mask_converter = AttentionMaskConverter(is_causal=<span class="hljs-literal">True</span>, sliding_window=sliding_window)<br>  key_value_length = input_shape[-<span class="hljs-number">1</span>] + past_key_values_length<br><br>  <span class="hljs-comment"># 4d mask is passed through the layers</span><br>  <span class="hljs-keyword">if</span> attention_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>    attention_mask = attn_mask_converter.to_4d(<br>      attention_mask, input_shape[-<span class="hljs-number">1</span>], key_value_length, dtype=inputs_embeds.dtype<br>    )<br>  <span class="hljs-keyword">else</span>:<br>    attention_mask = attn_mask_converter.to_causal_4d(<br>      input_shape[<span class="hljs-number">0</span>], input_shape[-<span class="hljs-number">1</span>], key_value_length, dtype=inputs_embeds.dtype, device=inputs_embeds.device<br>    )<br><br>  <span class="hljs-keyword">return</span> attention_mask<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">to_causal_4d</span>(<span class="hljs-params"></span><br><span class="hljs-params">  self,</span><br><span class="hljs-params">  batch_size: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">  query_length: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">  key_value_length: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">  dtype: torch.dtype = torch.float32,</span><br><span class="hljs-params">  device: <span class="hljs-type">Union</span>[torch.device, <span class="hljs-string">&quot;str&quot;</span>] = <span class="hljs-string">&quot;cpu&quot;</span>,</span><br><span class="hljs-params"></span>) -&gt; torch.Tensor:<br>  <span class="hljs-comment"># If shape is not cached, create a new causal mask and cache it</span><br>  input_shape = (batch_size, query_length)<br>  past_key_values_length = key_value_length - query_length<br><br>  <span class="hljs-comment"># create causal mask</span><br>  <span class="hljs-comment"># [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]</span><br>  causal_4d_mask = <span class="hljs-literal">None</span><br>  <span class="hljs-keyword">if</span> input_shape[-<span class="hljs-number">1</span>] &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> self.sliding_window <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>      causal_4d_mask = self._make_causal_mask(<br>          input_shape,<br>          dtype,<br>          device=device,<br>          past_key_values_length=past_key_values_length,<br>          sliding_window=self.sliding_window,<br>      )<br><br>  <span class="hljs-keyword">return</span> causal_4d_mask<br></code></pre></td></tr></table></figure>
<p>ä¸Šé¢çš„å‡½æ•°è¾“å…¥æ˜¯ä¸€ä¸ªäºŒç»´å¼ é‡<code>(batch_size, seq_len)</code>ï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ªå››ç»´<code>(batch_size, 1, seq_len, key_value_len)</code>ã€‚è¯¥å‡½æ•°ä¼šå»è°ƒç”¨ <code>AttentionMaskConverter</code> çš„ <code>to_causal_4d</code> æˆ– <code>to_4d</code>ï¼Œè€Œå®ƒä»¬ä¿©å¼¯å¼¯ç»•ç»•çš„ï¼Œä½†æœ€ç»ˆä»ç¦»ä¸å¼€ <code>_make_causal_mask</code>ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_make_causal_mask</span>(<span class="hljs-params"></span><br><span class="hljs-params">  input_ids_shape: torch.Size,</span><br><span class="hljs-params">  dtype: torch.dtype,</span><br><span class="hljs-params">  device: torch.device,</span><br><span class="hljs-params">  past_key_values_length: <span class="hljs-built_in">int</span> = <span class="hljs-number">0</span>,</span><br><span class="hljs-params">  sliding_window: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>):<br>  bsz, tgt_len = input_ids_shape<br>  mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).<span class="hljs-built_in">min</span>, device=device)<br>  mask_cond = torch.arange(mask.size(-<span class="hljs-number">1</span>), device=device)<br>  mask.masked_fill_(mask_cond &lt; (mask_cond + <span class="hljs-number">1</span>).view(mask.size(-<span class="hljs-number">1</span>), <span class="hljs-number">1</span>), <span class="hljs-number">0</span>)<br>  mask = mask.to(dtype)<br><br>  <span class="hljs-keyword">if</span> past_key_values_length &gt; <span class="hljs-number">0</span>:<br>    mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype, device=device), mask], dim=-<span class="hljs-number">1</span>)<br><br>  <span class="hljs-comment"># add lower triangular sliding window mask if necessary</span><br>  <span class="hljs-keyword">if</span> sliding_window <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>    diagonal = past_key_values_length - sliding_window + <span class="hljs-number">1</span><br>    context_mask = <span class="hljs-number">1</span> - torch.triu(torch.ones_like(mask, dtype=torch.<span class="hljs-built_in">int</span>), diagonal=diagonal)<br>    mask.masked_fill_(context_mask.<span class="hljs-built_in">bool</span>(), torch.finfo(dtype).<span class="hljs-built_in">min</span>)<br><br>  <span class="hljs-keyword">return</span> mask[<span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, :, :].expand(bsz, <span class="hljs-number">1</span>, tgt_len, tgt_len + past_key_values_length)<br></code></pre></td></tr></table></figure>
<p><code>_make_causal_mask</code> å‡½æ•°æœ€å…³é”®çš„æ˜¯å‰å‡ å¥è¯ã€‚é¦–å…ˆ <code>mask</code> ä¼šè¢«åˆå§‹åŒ–æˆ <code>(batch_size, seq_len)</code> ç»´åº¦çš„çŸ©é˜µï¼Œåˆå§‹å€¼ä¸ºå¾ˆå¤§çš„è´Ÿæ•°ğŸ‘‡ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).<span class="hljs-built_in">min</span>, device=device)<br></code></pre></td></tr></table></figure>
<p>ç„¶åä½¿ç”¨ <code>mask_cond</code> æ¥å°†<strong>çŸ©é˜µä¸‹åŠè§’çŸ©é˜µå½’é›¶</strong>ï¼Œé‡ç‚¹åœ¨<code>mask_cond &lt; (mask_cond + 1).view(mask.size(-1), 1)</code>ã€‚æ­¤å¤„ä¸¤ä¸ªæ¨ªçºµå‘é‡ä¸€æ¯”è¾ƒä¼šäº§ç”Ÿä¸€ä¸ªä¸Šä¸‰è§’çŸ©é˜µã€‚éšåå°†å…¨é›¶çŸ©é˜µä¸ <code>mask</code> ç›¸è¿æ¥ï¼Œæœ€åæ”¹å˜çŸ©é˜µç»´åº¦ä¸º <code>(bsz, 1, seq_len, key_value_len)</code>è¿”å›ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">mask_cond = torch.arange(mask.size(-<span class="hljs-number">1</span>), device=device)<br>mask.masked_fill_(mask_cond &lt; (mask_cond + <span class="hljs-number">1</span>).view(mask.size(-<span class="hljs-number">1</span>), <span class="hljs-number">1</span>), <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure>
<p>è¿™é‡Œå¤§å®¶ä¸å¦¨æ€è€ƒä¸€ä¸‹ï¼šä¸ºä»€ä¹ˆ Mask çŸ©é˜µå¿…é¡»æ˜¯ä¸‹ä¸‰è§’å½’é›¶ï¼Œè€Œä¸Šä¸‰è§’å…¨ä¸ºæœ€å°å€¼ï¼Ÿä¸Šæ–‡åªæ˜¯ç®€å•åœ°è¯´æ˜¯ä¸ºäº†é˜²æ­¢â€œä½œå¼Šâ€ã€‚å…·ä½“åŸå› æ˜¯ï¼šç”Ÿæˆæ–‡æœ¬åºåˆ—æ—¶ï¼Œæ¨¡å‹åªèƒ½å‚è€ƒä¹‹å‰çš„è¯ï¼Œè€Œä¸èƒ½å‚è€ƒæœªæ¥ç”Ÿæˆçš„è¯ã€‚å½“ decoder ä½¿ç”¨ masked æ³¨æ„åŠ›æœºåˆ¶ç”Ÿæˆè¾“å‡ºæ—¶ï¼Œè®¡ç®— $ QK^T $ çš„æ³¨æ„åŠ›æƒé‡æ—¶ï¼Œæˆ‘ä»¬å…è®¸ query å»æŸ¥çœ‹ä¹‹å‰ç”Ÿæˆè¯çš„ä¿¡æ¯ï¼Œä½†ä¸å…è®¸ query æŸ¥çœ‹ä¹‹åç”Ÿæˆçš„è¯ï¼ˆå› ä¸ºå®ƒä»¬è¿˜æœªè¢«äº§ç”Ÿï¼‰ã€‚å¯¹åº”åˆ°çŸ©é˜µä¹˜æ³•ä¸­ï¼Œå°±æ„å‘³ç€ query å¯¹åº”çš„è¡Œå‘é‡åºå·å¿…é¡»å¤§äºç­‰äº key çš„è¡Œå‘é‡åºå·ã€‚</p>
<p><strong>è¯‘ç å±‚</strong></p>
<p>å¥½ï¼Œæˆ‘ä»¬åœ¨æ³¨æ„åŠ›æ©ç è¿™è¾¹èŠ±äº†å¤ªå¤šåŠŸå¤«äº†ã€‚æ¥ä¸‹æ¥ç»§ç»­çœ‹ <code>forward</code> å‡½æ•°çš„å®ç°ğŸ‘‡ï¼šé¦–å…ˆæ˜¯åˆå§‹åŒ–å¼ é‡ï¼Œç„¶åå°±æ˜¯ <code>llamaModel</code> å¯¹è¯‘ç å±‚çš„å…·ä½“å¤„ç†ã€‚å¯¹äºè¿™ä¸ªä¸» for å¾ªç¯ï¼Œå…ˆæŠ›å¼€ä½¿ç”¨æ£€æŸ¥ç‚¹çš„é€»è¾‘éƒ¨åˆ†ï¼Œç›´æ¥çœ‹è°ƒç”¨ <code>decoder_layer</code> éƒ¨åˆ†ï¼Œå°±ä¼šå‘ç°å¾ªç¯åªæ˜¯åœ¨ä¸æ–­åœ°è°ƒç”¨ <code>decoder_layer</code>ï¼ˆä¹Ÿå°±æ˜¯ <code>LlamaDecoderLayer:forward</code>ï¼‰æ¥è¿›è¡Œæ¨ç†ï¼Œç„¶åæŠŠå¾—åˆ°çš„è¾“å‡ºç»“æœå†ä½œä¸ºä¸‹ä¸€å±‚çš„è¾“å…¥ç»§ç»­å‰æ¨ï¼Œç›´åˆ°æ‰€æœ‰å­å±‚çš„å‰æ¨ç»“æŸã€‚å¾ªç¯é€€å‡ºåï¼Œ<code>hidden_states</code> ä¼šæœ€ååŠ ä¸€å±‚å½’ä¸€åŒ–ï¼Œæœ€åé€šè¿‡ transformers è‡ªå¸¦çš„ <code>BaseModelOutputWithPast</code> å°†æœ€åçš„è¾“å‡ºå¼ é‡å’Œ kv ç›¸å…³ä¿¡æ¯è¿”å›ï¼Œè¯¥ç±»æ˜¯æ¡†æ¶ä¸­åŒ…å« past kv å€¼çš„åŸºç¡€æ¨¡å‹è¾“å‡ºç±»ï¼Œå…³äºæ­¤ç±»å°±ä¸è¯¦ç»†å±•å¼€è®²äº†ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># decoder layers</span><br><span class="hljs-comment"># initialize  all_hidden_states all_self_attns next_decoder_cache</span><br><span class="hljs-keyword">for</span> idx, decoder_layer <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(self.layers):<br>  <span class="hljs-keyword">if</span> output_hidden_states:<br>    all_hidden_states += (hidden_states,)<br>  past_key_value = past_key_values[idx] <span class="hljs-keyword">if</span> past_key_values <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br><br>  <span class="hljs-keyword">if</span> self.gradient_checkpointing <span class="hljs-keyword">and</span> self.training:<br>    <span class="hljs-comment">#layer_outputs = self._gradient_checkpointing_func(</span><br>  <span class="hljs-keyword">else</span>:<br>      layer_outputs = decoder_layer(<br>          hidden_states,<br>          attention_mask=attention_mask,<br>          position_ids=position_ids,<br>          past_key_value=past_key_value,<br>          output_attentions=output_attentions,<br>          use_cache=use_cache,<br>      )<br><br>  hidden_states = layer_outputs[<span class="hljs-number">0</span>]<br><br>  <span class="hljs-keyword">if</span> use_cache:<br>      next_decoder_cache += (layer_outputs[<span class="hljs-number">2</span> <span class="hljs-keyword">if</span> output_attentions <span class="hljs-keyword">else</span> <span class="hljs-number">1</span>],)<br><br>  <span class="hljs-keyword">if</span> output_attentions:<br>      all_self_attns += (layer_outputs[<span class="hljs-number">1</span>],)<br><br>hidden_states = self.norm(hidden_states)<br><br><span class="hljs-comment"># add hidden states from the last decoder layer</span><br><span class="hljs-keyword">if</span> output_hidden_states:<br>  all_hidden_states += (hidden_states,)<br><br>next_cache = next_decoder_cache <span class="hljs-keyword">if</span> use_cache <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> return_dict:<br>  <span class="hljs-keyword">return</span> <span class="hljs-built_in">tuple</span>(v <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> [hidden_states, next_cache, all_hidden_states, all_self_attns] <span class="hljs-keyword">if</span> v <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>)<br><span class="hljs-keyword">return</span> BaseModelOutputWithPast(<br>  last_hidden_state=hidden_states,<br>  past_key_values=next_cache,<br>  hidden_states=all_hidden_states,<br>  attentions=all_self_attns,<br>)<br></code></pre></td></tr></table></figure>
<p>ç¯‡å¤–ï¼šfor å¾ªç¯ä¸­ä½¿ç”¨ <code>gradient_checkpointing</code> å¯ä»¥æœ‰æ•ˆèŠ‚çº¦æ˜¾å­˜ï¼Œè¯¦ç»†å†…å®¹å¯ä»¥å‚è€ƒ <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/checkpoint.html"><code>torch.utils.checkpoint.checkpoint</code></a>ã€‚å®ƒçš„åŸç†éå¸¸ç®€å•ï¼šè§„å®šç¨‹åºåœ¨å¯¹ <code>decoderLayer</code> è¿›è¡Œå‰æ¨æ—¶ï¼Œä¸ä¿å­˜ä¸­é—´è®¡ç®—å€¼ã€‚è€Œè‹¥æ¨¡å‹éœ€è¦è‡ªåŠ¨å¾®åˆ†ä»¥å®Œæˆ backwardï¼Œç¨‹åºä¼šé‡æ–°è®¡ç®—è¿™äº›ä¸­é—´å€¼ï¼Œä»è€ŒèŠ‚çœäº†æ¨¡å‹è¿ç®—éœ€è¦çš„å†…å­˜ç©ºé—´ã€‚å› æ­¤ï¼Œ<code>use_cache</code> å’Œ <code>gradient_checkpointing</code>æœ€å¥½ä¸è¦åŒæ—¶è®¾ç½®ä¸º trueï¼Œå› ä¸ºä¸€ä¸ªæ˜¯ç”¨ç©ºé—´æ¢æ—¶é—´ï¼Œä¸€ä¸ªæ˜¯æ—¶é—´æ¢ç©ºé—´ï¼Œä¸¤è€…å¯èƒ½ä¼šäº’ç›¸æŠµæ¶ˆä¼˜åŒ–å½±å“ã€‚</p>
<h2 id="æœ€ç»ˆæˆå‹">æœ€ç»ˆæˆå‹</h2>
<p>ç”±äºç¯‡å¹…å…³ç³»ï¼Œè¿™é‡Œä»…ä»‹ç»æœ€å¸¸ç”¨çš„ <code>LlamaForCausalLM</code>ï¼Œè¯¥æ¨¡å‹æ˜¯å› æœç±»è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥æ ¹æ®ç”¨æˆ·ç»™çš„ä¸Šæ–‡æ¥ç»­å†™ä¸‹æ–‡ï¼Œä¹Ÿå¯ä»¥å›ç­”ç”¨æˆ·æå‡ºçš„é—®é¢˜ã€‚</p>
<h3 id="LlamaForCausalLM">LlamaForCausalLM</h3>
<p><code>LlamaForCausalLM</code> æ˜¯ Llama å› æœç±»è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥æ ¹æ®ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬è¾“å‡ºç›¸åº”çš„å›ç­”ã€‚æŠ€æœ¯ä¸Šçœ‹ï¼Œå®ƒåœ¨ <code>LlamaModel</code> çš„åŸºç¡€ä¸Šå¢åŠ äº†ä¸€ä¸ªçº¿æ€§å±‚ <code>lm_head</code> ä½œä¸º Generatorï¼Œä»è€Œå®ç°äº†ä¸€ä¸ªå®Œæ•´çš„è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹å®ƒçš„ <code>forward</code> å‡½æ•°ğŸ‘‡ï¼Œè‹¥ä»”ç»†å¯¹æ¯”ä¹‹å‰æ¨¡å‹çš„è¾“å…¥å‚æ•°ï¼Œä¼šå‘ç°å¤šäº†ä¸€ä¸ªå¯é€‰ä¼ å…¥çš„ <code>label</code> å¼ é‡ï¼Œè¯¥å¼ é‡å½¢çŠ¶æ˜¯ <code>(batch_size, seq_len)</code>ï¼Œå®ƒæ˜¯ç”¨äºè®¡ç®— masked è¯­è¨€æ¨¡å‹çš„æŸå¤±å€¼ã€‚è¯¥æ¨¡å‹çš„ <code>forward</code> å‡½æ•°åœ¨å‡†å¤‡å¥½è¾“å…¥çš„å‚æ•°åï¼Œå°±ç›´æ¥è°ƒç”¨äº† <code>LlamaModel:forward()</code>ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">  self,</span><br><span class="hljs-params">  input_ids: torch.LongTensor = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">  attention_mask: <span class="hljs-type">Optional</span>[torch.Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">  position_ids: <span class="hljs-type">Optional</span>[torch.LongTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">  past_key_values: <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[torch.FloatTensor]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">  inputs_embeds: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">  labels: <span class="hljs-type">Optional</span>[torch.LongTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">  use_cache: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">  output_attentions: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">  output_hidden_states: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">  return_dict: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Union</span>[<span class="hljs-type">Tuple</span>, CausalLMOutputWithPast]:<br><br>output_attentions = output_attentions <span class="hljs-keyword">if</span> output_attentions <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> self.config.output_attentions<br>output_hidden_states = (<br>    output_hidden_states <span class="hljs-keyword">if</span> output_hidden_states <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> self.config.output_hidden_states<br>)<br>return_dict = return_dict <span class="hljs-keyword">if</span> return_dict <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> self.config.use_return_dict<br><br><span class="hljs-comment"># decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)</span><br>outputs = self.model(<br>    input_ids=input_ids,<br>    attention_mask=attention_mask,<br>    position_ids=position_ids,<br>    past_key_values=past_key_values,<br>    inputs_embeds=inputs_embeds,<br>    use_cache=use_cache,<br>    output_attentions=output_attentions,<br>    output_hidden_states=output_hidden_states,<br>    return_dict=return_dict,<br>)<br></code></pre></td></tr></table></figure>
<p>éšåï¼Œå°†æ‹¿åˆ°æ‰‹çš„ <code>outputs</code> æ”¾å…¥åˆ°æ·»åŠ çš„çº¿æ€§å±‚ <code>lm_head</code> è¿›è¡Œè¿ç®—ã€‚åŒç†ï¼ŒTP å¹¶è¡Œæ—¶ä¼šå°†çº¿æ€§å±‚çš„çŸ©é˜µåœ¨ dim=0 ç»´åº¦æ‹†åˆ†ã€‚è‹¥ä¼ å…¥äº† <code>label</code>ï¼Œé‚£ä¹ˆå¾—åˆ°çš„ç»“æœ <code>logits</code> åœ¨ç»è¿‡ç§»ä½åè®¡ç®—äº¤å‰ç†µ <code>CrossEntropyLoss()</code>ï¼Œè‹¥æ— åˆ™å¯ç›´æ¥é€šè¿‡ <code>CausalLMOutputWithPast</code> è¿”å›ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python">hidden_states = outputs[<span class="hljs-number">0</span>]<br><span class="hljs-keyword">if</span> self.config.pretraining_tp &gt; <span class="hljs-number">1</span>:<br>  lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=<span class="hljs-number">0</span>)<br>  logits = [F.linear(hidden_states, lm_head_slices[i]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.config.pretraining_tp)]<br>  logits = torch.cat(logits, dim=-<span class="hljs-number">1</span>)<br><span class="hljs-keyword">else</span>:<br>  logits = self.lm_head(hidden_states)<br>logits = logits.<span class="hljs-built_in">float</span>()<br>loss = <span class="hljs-literal">None</span><br><span class="hljs-keyword">if</span> labels <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>  <span class="hljs-comment"># Shift so that tokens &lt; n predict n</span><br>  shift_logits = logits[..., :-<span class="hljs-number">1</span>, :].contiguous()<br>  shift_labels = labels[..., <span class="hljs-number">1</span>:].contiguous()<br>  <span class="hljs-comment"># Flatten the tokens</span><br>  loss_fct = CrossEntropyLoss()<br>  shift_logits = shift_logits.view(-<span class="hljs-number">1</span>, self.config.vocab_size)<br>  shift_labels = shift_labels.view(-<span class="hljs-number">1</span>)<br>  <span class="hljs-comment"># Enable model parallelism</span><br>  shift_labels = shift_labels.to(shift_logits.device)<br>  loss = loss_fct(shift_logits, shift_labels)<br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> return_dict:<br>  output = (logits,) + outputs[<span class="hljs-number">1</span>:]<br>  <span class="hljs-keyword">return</span> (loss,) + output <span class="hljs-keyword">if</span> loss <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> output<br><span class="hljs-keyword">return</span> CausalLMOutputWithPast(<br>    loss=loss,<br>    logits=logits,<br>    past_key_values=outputs.past_key_values,<br>    hidden_states=outputs.hidden_states,<br>    attentions=outputs.attentions,<br>)<br></code></pre></td></tr></table></figure>
<p>ä½¿ç”¨ huggingface æ¡†æ¶å®ç°çš„ <code>LlamaForCausalLM</code> è¿›è¡Œæ¨ç†çš„ç¤ºä¾‹å¦‚ä¸‹ï¼Œä»è¯¥ç¤ºä¾‹ä¸­æˆ‘ä»¬å¯ä»¥æ›´å¥½åœ°ç†è§£å¤§æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼šæ¯”å¦‚è¯´ï¼Œä¸Šé¢çš„è¾“å…¥å¼ é‡<code>input_ids</code> æ˜¯ç”¨æˆ·è¾“å…¥çš„å¤„ç†åçš„â€œæ–‡æœ¬â€ã€‚è€Œæœ€åˆç”¨æˆ·è¾“å…¥çš„å­—ç¬¦ä¸²<code>prompt</code> å…ˆè¿›å…¥ tokenizer è¿›è¡Œåˆ†è¯ï¼Œéšåç¼–ç ã€åµŒå…¥æŠ€æœ¯å˜ä¸ºå¼ é‡ã€‚è¿™é‡Œæœ€é‡è¦çš„å‡½æ•°è«å±äº <code>model.generate</code>ï¼Œä½†å®ƒåªèƒ½åœ¨æ¨ç†æ—¶ä½¿ç”¨ã€‚å®ƒé™¤äº†åœ¨èƒŒåé»˜é»˜è°ƒç”¨äº†ä¸Šé¢çš„ <code>forward</code>ï¼Œè¿˜åšäº†å¾ˆå¤šï¼šç”¨äºå¤šç§è§£ç ç­–ç•¥ï¼Œä¾‹å¦‚ beam searchã€top-k é‡‡æ ·ç­‰â€¦â€¦è¯¦ç»†çš„æ–‡ç« å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/blog/how-to-generate">è¿™ç¯‡åšå®¢</a>ä¸­æ‰¾åˆ°ã€‚ç”Ÿæˆäº§ç”Ÿçš„å¼ é‡äººç±»æ— æ³•ç›´æ¥çœ‹æ‡‚ï¼Œè¿˜éœ€è¦ç»è¿‡è§£ç  <code>batch_decode</code> æ‰èƒ½å‘ˆç°æµåˆ©çš„è‹±è¯­ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, LlamaForCausalLM<br><br>model = LlamaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)<br>tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)<br>prompt = <span class="hljs-string">&quot;Hey, are you conscious? Can you talk to me?&quot;</span><br>inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)<br><span class="hljs-comment"># Generate</span><br>generate_ids = model.generate(inputs.input_ids, max_length=<span class="hljs-number">30</span>)<br>tokenizer.batch_decode(generate_ids, skip_special_tokens=<span class="hljs-literal">True</span>, clean_up_tokenization_spaces=<span class="hljs-literal">False</span>)[<span class="hljs-number">0</span>]<br><span class="hljs-string">&quot;Hey, are you conscious? Can you talk to me?\nI&#x27;m not conscious, but I can talk to you.&quot;</span><br></code></pre></td></tr></table></figure>
<h2 id="å°ç»“">å°ç»“</h2>
<p>æœ¬æ–‡ä» llama æ˜¯ä»€ä¹ˆå‡ºå‘ï¼Œæ·±å…¥è§£è¯»äº† huggingface æ¡†æ¶å¯¹ llama çš„ä»£ç å®ç°ã€‚æˆ‘å…ˆä» llama è®ºæ–‡å¼€å§‹è§£è¯»ï¼Œè¯•å›¾è®©æ‰€æœ‰æœªæ¥è§¦è¿‡å¤§æ¨¡å‹çš„å¤–è¡Œäººèƒ½ç†è§£å¤§æ¨¡å‹æ˜¯å¦‚ä½•è¢«è®­ç»ƒäº§ç”Ÿçš„ã€‚éšåï¼Œæˆ‘ç»™å‡ºäº† llama çš„æ¶æ„å›¾ï¼Œå¹¶ç®€è¦è¯´æ˜äº† llama1/2 å’Œ transformer æ¡†æ¶çš„åŒºåˆ«ï¼Œä»¥åŠä¸ºä½•è¦è¿™æ ·æ”¹è¿›ã€‚ç„¶åï¼Œæˆ‘ä»åŸºç¡€åˆ°ä¸Šå±‚é€ä¸ªåˆ†æäº† llama çš„ä»£ç å®ç°ï¼Œç”±äºæˆ‘ä¹Ÿæ˜¯ç¬¬ä¸€æ¬¡å¦‚æ­¤ç»†è‡´åœ°é˜…è¯»å¤§æ¨¡å‹çš„ä»£ç ï¼Œå› æ­¤å¾ˆå¤šåœ°æ–¹å¯èƒ½ä¼šæ¯”è¾ƒå•°å—¦ã€‚ä½†ä¸‡äº‹å¼€å¤´éš¾ï¼Œåœ¨ç ”ç©¶çš„æœ€åˆé˜¶æ®µå°½å¯èƒ½ææ¸…æ¥šæœ€åŸºæœ¬çš„ä¸œè¥¿ï¼Œæ­¥æ­¥ä¸ºè¥æ–¹èƒ½è±ç„¶å¼€æœ—ã€‚å¦å¤–ï¼Œæœ¬ç¯‡åšæ–‡å¿…ä¸å¯èƒ½è¦†ç›– llama ä¹ƒè‡³å¤§æ¨¡å‹çš„æ–¹æ–¹é¢é¢ï¼Œæˆ‘ç­‰è¿˜éœ€ç»§ç»­åŠªåŠ›ï¼Œè¿›ä¸€æ­¥æ­å¼€å¤§æ¨¡å‹çš„ç¥ç§˜é¢çº±ã€‚</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/AI/" class="category-chain-item">AI</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/AI/" class="print-no-link">#AI</a>
      
        <a href="/tags/Transformer/" class="print-no-link">#Transformer</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>huggingfaceä¸‹llamaä»£ç ç»†è¯»ï¼ˆä¸‹ï¼‰</div>
      <div>https://dingfen.github.io/2023/11/30/2023-11-30-huggingface2/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>ä½œè€…</div>
          <div>Bill Ding</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>å‘å¸ƒäº</div>
          <div>2023å¹´11æœˆ30æ—¥</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>æ›´æ–°äº</div>
          <div>2025å¹´1æœˆ26æ—¥</div>
        </div>
      
      
        <div class="license-meta-item">
          <div>è®¸å¯åè®®</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - ç½²å">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="SA - ç›¸åŒæ–¹å¼å…±äº«">
                    <i class="iconfont icon-sa"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/11/30/2023-11-30-LLM-KVCache/" title="å¤§æ¨¡å‹æ¨ç†ä¼˜åŒ–æŠ€æœ¯ä¹‹æ˜¾å­˜ä¼˜åŒ–">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">å¤§æ¨¡å‹æ¨ç†ä¼˜åŒ–æŠ€æœ¯ä¹‹æ˜¾å­˜ä¼˜åŒ–</span>
                        <span class="visible-mobile">ä¸Šä¸€ç¯‡</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/10/30/2023-10-30-huggingface1/" title="huggingfaceä¸‹llamaä»£ç ç»†è¯»ï¼ˆä¸Šï¼‰">
                        <span class="hidden-mobile">huggingfaceä¸‹llamaä»£ç ç»†è¯»ï¼ˆä¸Šï¼‰</span>
                        <span class="visible-mobile">ä¸‹ä¸€ç¯‡</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>ç›®å½•</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  








    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">æœç´¢</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">å…³é”®è¯</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        æ€»è®¿é—®é‡ 
        <span id="busuanzi_value_site_pv"></span>
         æ¬¡
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        æ€»è®¿å®¢æ•° 
        <span id="busuanzi_value_site_uv"></span>
         äºº
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  var relativeDate = function() {
    var updatedTime = document.getElementById('updated-time');
    if (updatedTime) {
      var text = updatedTime.textContent;
      var reg = /\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:Z|[+-]\d{2}:\d{2})/;
      var matchs = text.match(reg);
      if (matchs) {
        var relativeTime = moment(matchs[0]).fromNow();
        updatedTime.textContent = text.replace(reg, relativeTime);
      }
      updatedTime.style.display = '';
    }
  };
  Fluid.utils.createScript('https://lib.baomitu.com/moment.js/2.29.4/moment.min.js', function() {
    if (!'zh-cn'.startsWith('en')) {
      Fluid.utils.createScript('https://lib.baomitu.com/moment.js/2.29.4/locale/zh-cn.min.js', function() {
        relativeDate();
      });
    } else {
      relativeDate();
    }
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  
<script src="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/Ribbon.min.js"></script>



<!-- ä¸»é¢˜çš„å¯åŠ¨é¡¹ï¼Œå°†å®ƒä¿æŒåœ¨æœ€åº•éƒ¨ -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">åšå®¢åœ¨å…è®¸ JavaScript è¿è¡Œçš„ç¯å¢ƒä¸‹æµè§ˆæ•ˆæœæ›´ä½³</div>
  </noscript>
<!-- hexo injector body_end start -->
  <link rel="stylesheet" crossorigin href="https://g.alicdn.com/aliyun-documentation/web-chatbot-ui/0.0.11/index.css" />
  <script type="module" crossorigin src="https://g.alicdn.com/aliyun-documentation/web-chatbot-ui/0.0.11/index.js"></script>
  <script>
    window.CHATBOT_CONFIG = {
      endpoint: "https://web-chatbot-syz-knthhrjfeq.cn-hangzhou.fcapp.run/chat", // å¯ä»¥æ›¿æ¢ä¸º https://{your-fc-http-trigger-domain}/chat
      displayByDefault: false, // é»˜è®¤ä¸æ˜¾ç¤º AI åŠ©æ‰‹å¯¹è¯æ¡†
      aiChatOptions: { // è‡ªå®šä¹‰å–å€¼å‚è€ƒï¼šhttps://docs.nlkit.com/nlux/reference/ui/ai-chat#conversation-options
        conversationOptions: { // è‡ªå®šä¹‰å–å€¼å‚è€ƒï¼šhttps://docs.nlkit.com/nlux/reference/ui/ai-chat#conversation-options
          conversationStarters: [
            {prompt: 'è¯·é—®ä½ æ˜¯è°ï¼Œèƒ½ä¸ºæˆ‘åšä»€ä¹ˆï¼Ÿ'},
            {prompt: 'è¯·ä»‹ç»ä¸€ä¸‹åšå®¢çš„ä¸»äºº'}
          ],
          layout: 'bubbles'
        },
        displayOptions: { // è‡ªå®šä¹‰å–å€¼å‚è€ƒï¼šhttps://docs.nlkit.com/nlux/reference/ui/ai-chat#display-options
          height: 550,
          // width: 400,
        },
        personaOptions: { // è‡ªå®šä¹‰å–å€¼å‚è€ƒï¼šhttps://docs.nlkit.com/nlux/reference/ui/ai-chat#chat-personas
          assistant: {
            name: 'ä½ å¥½ï¼Œæˆ‘æ˜¯æœ¬ç½‘ç«™çš„ AI åŠ©æ‰‹',
            // AI åŠ©æ‰‹çš„å›¾æ ‡
            avatar: 'https://img.alicdn.com/imgextra/i2/O1CN01Pda9nq1YDV0mnZ31H_!!6000000003025-54-tps-120-120.apng',
            tagline: 'è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œæˆ‘ä¼šå°½åŠ›å¸®ä½ è§£ç­”ï¼',
          }
        }
      }
    };
  </script>
  <style>
    :root {
      /* webchat å·¥å…·æ çš„é¢œè‰² */
      --webchat-toolbar-background-color: #1464E4;
      /* webchat å·¥å…·æ æ–‡å­—å’ŒæŒ‰é’®çš„é¢œè‰² */
      --webchat-toolbar-text-color: #FFF;
    }
    /* webchat å¯¹è¯æ¡†å¦‚æœè¢«é®æŒ¡ï¼Œå¯ä»¥å°è¯•é€šè¿‡ z-indexã€bottomã€right ç­‰è®¾ç½® æ¥è°ƒæ•´*/
    .webchat-container {
      z-index: 100;
      bottom: 10px;
      right: 10px;
    }
    /* webchat çš„å”¤èµ·æŒ‰é’®å¦‚æœè¢«é®æŒ¡ï¼Œå¯ä»¥å°è¯•é€šè¿‡ z-indexã€bottomã€right ç­‰è®¾ç½® æ¥è°ƒæ•´ã€‚ä¹Ÿå¯ä»¥é€šè¿‡ CSS è¿›ä¸€æ­¥å®šåˆ¶å”¤èµ·æŒ‰é’®çš„å½¢çŠ¶ã€å¤§å°ç­‰ã€‚ */
    .webchat-bubble-tip {
      z-index: 99;
      bottom: 20px;
      right: 20px;
    }
  </style>
  <!-- hexo injector body_end end --></body>
</html>
