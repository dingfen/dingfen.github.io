

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Bill Ding">
  <meta name="keywords" content="">
  
    <meta name="description" content="揭开它神秘的面纱">
<meta property="og:type" content="article">
<meta property="og:title" content="huggingface下llama代码细读（下）">
<meta property="og:url" content="https://dingfen.github.io/2023/11/30/2023-11-30-huggingface2/index.html">
<meta property="og:site_name" content="峰子的乐园">
<meta property="og:description" content="揭开它神秘的面纱">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://dingfen.github.io/img/LLM/llama.png">
<meta property="article:published_time" content="2023-11-30T04:00:00.000Z">
<meta property="article:modified_time" content="2025-01-26T11:49:09.209Z">
<meta property="article:author" content="Bill Ding">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://dingfen.github.io/img/LLM/llama.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>huggingface下llama代码细读（下） - 峰子的乐园</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/KaTeX/0.16.2/katex.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"dingfen.github.io","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"|","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":null,"onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.1.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>峰子的乐园</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="huggingface下llama代码细读（下）"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-11-30 12:00" pubdate>
          2023年11月30日 中午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          3.4k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          29 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">huggingface下llama代码细读（下）</h1>
            
              <p id="updated-time" class="note note-info" style="display: none">
                
                  
                    <!-- compatible with older versions-->
                    更新于：2025-01-26T19:49:09+08:00
                  
                  

                
              </p>
            
            
              <div class="markdown-body">
                
                <h1>huggingface下llama代码细读（下）</h1>
<h2 id="前言">前言</h2>
<p>上篇博客我们重点介绍了 llama 模型，并讨论了它的架构、基件和中间件等。碍于篇幅关系，我将 transformer llama 的代码解读下半部分移动到了本篇博客中，要想从头开始的读者们可以参考<a href="https://dingfen.github.io/ai/2023/10/30/huggingface1.html">这篇博客</a>。</p>
<h2 id="llama-模型">llama 模型</h2>
<center>
<img src="/img/LLM/llama.png" srcset="/img/loading.gif" lazyload>
</center>
<h3 id="译码层">译码层</h3>
<p>在了解了构成 llama 的基本组件后，要如何搭建起大模型的“高楼大厦”？当然不能一步登天，而要步步为营。在大模型推理阶段，输入的文本序列会经过多个译码层，执行自注意力等运算。译码层由 <code>LlamaDecoderLayer</code> 类表示，它将 <code>LlamaAttention</code> <code>LlamaRMSNorm</code> 等基件组合起来。上图所展示的架构就是一个译码层的架构。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LlamaDecoderLayer</span>(nn.Module):<br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config: LlamaConfig</span>):<br>    <span class="hljs-built_in">super</span>().__init__()<br>    self.hidden_size = config.hidden_size<br>    self.self_attn = (<br>        LlamaAttention(config=config)<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">getattr</span>(config, <span class="hljs-string">&quot;_flash_attn_2_enabled&quot;</span>, <span class="hljs-literal">False</span>)<br>        <span class="hljs-keyword">else</span> LlamaFlashAttention2(config=config)<br>    )<br>    self.mlp = LlamaMLP(config)<br>    self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)<br>    self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)<br></code></pre></td></tr></table></figure>
<p>上面的构造函数罗列了译码层的几个组件：自注意力层，MLP 层和两个 RMSNorm 层，而其 forward 函数则更详细地展示了架构图内张量的执行情况：13 行先将输入执行一次 RMSNorm 归一，16-25 行执行一次注意力运算，加上了残差结构，再执行一次 RMSNorm 归一，27-33 行将注意力结果输出到 MLP 中，最后返回结果👇。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">  self,</span><br><span class="hljs-params">  hidden_states: torch.Tensor,</span><br><span class="hljs-params">  attention_mask: <span class="hljs-type">Optional</span>[torch.Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">  position_ids: <span class="hljs-type">Optional</span>[torch.LongTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">  past_key_value: <span class="hljs-type">Optional</span>[<span class="hljs-type">Tuple</span>[torch.Tensor]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">  output_attentions: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">  use_cache: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">  **kwargs,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Tuple</span>[torch.FloatTensor, <span class="hljs-type">Optional</span>[<span class="hljs-type">Tuple</span>[torch.FloatTensor, torch.FloatTensor]]]:<br><br>  residual = hidden_states<br><br>  hidden_states = self.input_layernorm(hidden_states)<br><br>  <span class="hljs-comment"># Self Attention</span><br>  hidden_states, self_attn_weights, present_key_value = self.self_attn(<br>      hidden_states=hidden_states,<br>      attention_mask=attention_mask,<br>      position_ids=position_ids,<br>      past_key_value=past_key_value,<br>      output_attentions=output_attentions,<br>      use_cache=use_cache,<br>      **kwargs,<br>  )<br>  hidden_states = residual + hidden_states<br><br>  <span class="hljs-comment"># Fully Connected</span><br>  residual = hidden_states<br>  hidden_states = self.post_attention_layernorm(hidden_states)<br>  hidden_states = self.mlp(hidden_states)<br>  hidden_states = residual + hidden_states<br><br>  outputs = (hidden_states,)<br><br>  <span class="hljs-keyword">if</span> output_attentions:<br>      outputs += (self_attn_weights,)<br><br>  <span class="hljs-keyword">if</span> use_cache:<br>      outputs += (present_key_value,)<br><br>  <span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure>
<h3 id="llama-Model">llama Model</h3>
<p><code>LlamaModel</code> 由上面介绍的多个 <code>LlamaDecoderLayer</code> 堆叠而成。以之前的 llama-7b-hf 参数为例，<code>num_hidden_layers</code> 为32，意思是该模型一共堆叠了 32 层译码层。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>  <span class="hljs-attr">&quot;architecture&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-string">&quot;LlamaForCausalLM&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;hidden_act&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;silu&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;hidden_size&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">4096</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;initializer_range&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">0.02</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;intermediate_size&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">11008</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;max_position_embeddings&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">4096</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;model_type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;llama&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;num_attention_heads&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">32</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;num_hidden_layers&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">32</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;num_key_value_heads&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">32</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;pretraining_tp&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">2</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;rms_norm_eps&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">1e-05</span><span class="hljs-punctuation">,</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure>
<p>除此之外，Llama 模型在输入的最前头加入了一个嵌入层（Embedding），最后又加了一层 RMSNorm 进行归一，下面是它的构造函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LlamaModel</span>(<span class="hljs-title class_ inherited__">LlamaPreTrainedModel</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config: LlamaConfig</span>):<br>    <span class="hljs-built_in">super</span>().__init__(config)<br>    self.padding_idx = config.pad_token_id<br>    self.vocab_size = config.vocab_size<br><br>    self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)<br>    self.layers = nn.ModuleList([LlamaDecoderLayer(config) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(config.num_hidden_layers)])<br>    self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)<br><br>    self.gradient_checkpointing = <span class="hljs-literal">False</span><br>    <span class="hljs-comment"># Initialize weights and apply final processing</span><br>    self.post_init()<br></code></pre></td></tr></table></figure>
<p><code>LlamaModel</code> 类继承自 <code>LlamaPreTrainedModel</code>，<code>LlamaPreTrainedModel</code> 没有那么神秘，只不过是在 <code>LlamaDecoderLayer</code> 的基础上包裹了一些初始化操作而已：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LlamaPreTrainedModel</span>(<span class="hljs-title class_ inherited__">PreTrainedModel</span>):<br>  config_class = LlamaConfig<br>  base_model_prefix = <span class="hljs-string">&quot;model&quot;</span><br>  supports_gradient_checkpointing = <span class="hljs-literal">True</span><br>  _no_split_modules = [<span class="hljs-string">&quot;LlamaDecoderLayer&quot;</span>]<br>  _skip_keys_device_placement = <span class="hljs-string">&quot;past_key_values&quot;</span><br>  _supports_flash_attn_2 = <span class="hljs-literal">True</span><br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">_init_weights</span>(<span class="hljs-params">self, module</span>):<br>    std = self.config.initializer_range<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(module, nn.Linear):<br>        module.weight.data.normal_(mean=<span class="hljs-number">0.0</span>, std=std)<br>        <span class="hljs-keyword">if</span> module.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            module.bias.data.zero_()<br>    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(module, nn.Embedding):<br>        module.weight.data.normal_(mean=<span class="hljs-number">0.0</span>, std=std)<br>        <span class="hljs-keyword">if</span> module.padding_idx <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            module.weight.data[module.padding_idx].zero_()<br></code></pre></td></tr></table></figure>
<h4 id="Llama-模型的推理过程">Llama 模型的推理过程</h4>
<p>我们从 <code>LlamaModel</code> 代码中已经了解到，Llama 模型将 32 层译码层堆叠起来，输入的文本序列经过一层层译码被最终转化成输出序列。而 <code>LlamaModel::forward</code> 作为整个大模型“未封装的”入口，显得尤为重要。</p>
<p><strong>参数一览</strong></p>
<p>既然是个入口，我们首先从它的参数入手：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    input_ids: torch.LongTensor = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    attention_mask: <span class="hljs-type">Optional</span>[torch.Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    position_ids: <span class="hljs-type">Optional</span>[torch.LongTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    past_key_values: <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[torch.FloatTensor]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    inputs_embeds: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    use_cache: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    output_attentions: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    output_hidden_states: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    return_dict: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">  </span>) -&gt; <span class="hljs-type">Union</span>[<span class="hljs-type">Tuple</span>, BaseModelOutputWithPast]:<br></code></pre></td></tr></table></figure>
<ul>
<li>input_ids 可简单理解为输入的文本序列</li>
<li>attention_mask 注意力掩码，1 表示未遮掩，0 表示遮掩。通常会使用下三角矩阵对输出进行遮盖，防止模型作弊。</li>
<li>position_ids 输入文本序列的位置编号，从 0 开始</li>
<li>past_key_values 若 use_cache 为真则之前的kv值会被用于加速推理</li>
<li>inputs_embeds 模型支持直接传入 input 的嵌入张量，代替 input_ids</li>
<li>use_cache 是否使用 KV cache 加速推理，通过使用 cache 缓存权重等值加速推理</li>
<li>output_attentions 是否要返回所有注意力层的注意力张量</li>
<li>output_hidden_states 是否要返回所有层的隐藏层状态张量</li>
<li>return_dict 指示返回的类型是 <code>~utils.ModelOutput</code> 还是 tuple</li>
</ul>
<p><strong>masked Attention</strong></p>
<p>接下来看看 forward 的实现。略去错误机制和其他准备过程，来看看神秘的 <code>attention_mask</code> 如何被准备的。回顾一下 transformers 机制里的注意力掩码，它是用来在训练和推理时遮挡后续部分的输出，防止模型看到未来的输出而“作弊”用的。从公式的角度看，$ MaskAttn=softmax(\frac{QK^T}{\sqrt{d_k}}+masked)V $ ，公式的前半部分主要在计算注意力矩阵，而 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>是我们输入的矩阵，既然掩码是为了防止<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>偷看到未来输出的权重的，那么掩码就应该是下三角矩阵，右上部分应该置为很小的负数。</p>
<p>这点从代码中也可以得到验证，在此不得不感慨代码更新速度之快：现在最新版本的 transformers 代码使用 <code>_prepare_4d_causal_attention_mask</code>，而 4.37 之前的版本使用的函数还是 <code>_expand_mask</code>、<code>_make_causal_mask</code> 🙌：</p>
<p><code>input_ids</code> 是我们输入的文本矩阵，维度通常是 <code>(batch_size, seq_len)</code>。因此代码前四行也是如此提取出 <code>batch_size</code> 和 <code>seq_len</code> 的。<code>position_ios</code> 给我们输入的文本单词从 0 或 <code>past_key_values_length</code> 开始编号。随后，15 行将 <code>input_ids</code> 推入嵌入层，推理正式开始。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># ... to be continued some are omitted</span><br>  input_ids <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>    batch_size, seq_length = input_ids.shape[:<span class="hljs-number">2</span>]<br>  <span class="hljs-keyword">elif</span> inputs_embeds <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>    batch_size, seq_length = inputs_embeds.shape[:<span class="hljs-number">2</span>]<br><br>  past_key_values_length = past_key_values[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>].shape[<span class="hljs-number">2</span>]<br><br>  <span class="hljs-keyword">if</span> position_ids <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>    position_ids = torch.arange(<br>        past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device<br>    )<br>    position_ids = position_ids.unsqueeze(<span class="hljs-number">0</span>)<br><br>  <span class="hljs-keyword">if</span> inputs_embeds <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>    inputs_embeds = self.embed_tokens(input_ids)<br><br>  <span class="hljs-keyword">if</span> <span class="hljs-built_in">getattr</span>(self.config, <span class="hljs-string">&quot;_flash_attn_2_enabled&quot;</span>, <span class="hljs-literal">False</span>):<br>    <span class="hljs-comment"># 2d mask is passed through the layers</span><br>    attention_mask = attention_mask <span class="hljs-keyword">if</span> (attention_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> <span class="hljs-number">0</span> <span class="hljs-keyword">in</span> attention_mask) <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>  <span class="hljs-keyword">else</span>:<br>    <span class="hljs-comment"># 4d mask is passed through the layers</span><br>    attention_mask = _prepare_4d_causal_attention_mask(<br>      attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length<br>    )<br><br>  <span class="hljs-comment"># embed positions</span><br>  hidden_states = inputs_embeds<br></code></pre></td></tr></table></figure>
<p>再之后，代码开始使用库内函数准备注意力掩码了，行，那就让我们看看 <code>_prepare_4d_causal_attention_mask</code> 函数里到底卖的什么药：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_prepare_4d_causal_attention_mask</span>(<span class="hljs-params"></span><br><span class="hljs-params">  attention_mask: <span class="hljs-type">Optional</span>[torch.Tensor],</span><br><span class="hljs-params">  input_shape: <span class="hljs-type">Union</span>[torch.Size, <span class="hljs-type">Tuple</span>, <span class="hljs-type">List</span>],</span><br><span class="hljs-params">  inputs_embeds: torch.Tensor,</span><br><span class="hljs-params">  past_key_values_length: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">  sliding_window: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>):<br>  attn_mask_converter = AttentionMaskConverter(is_causal=<span class="hljs-literal">True</span>, sliding_window=sliding_window)<br>  key_value_length = input_shape[-<span class="hljs-number">1</span>] + past_key_values_length<br><br>  <span class="hljs-comment"># 4d mask is passed through the layers</span><br>  <span class="hljs-keyword">if</span> attention_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>    attention_mask = attn_mask_converter.to_4d(<br>      attention_mask, input_shape[-<span class="hljs-number">1</span>], key_value_length, dtype=inputs_embeds.dtype<br>    )<br>  <span class="hljs-keyword">else</span>:<br>    attention_mask = attn_mask_converter.to_causal_4d(<br>      input_shape[<span class="hljs-number">0</span>], input_shape[-<span class="hljs-number">1</span>], key_value_length, dtype=inputs_embeds.dtype, device=inputs_embeds.device<br>    )<br><br>  <span class="hljs-keyword">return</span> attention_mask<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">to_causal_4d</span>(<span class="hljs-params"></span><br><span class="hljs-params">  self,</span><br><span class="hljs-params">  batch_size: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">  query_length: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">  key_value_length: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">  dtype: torch.dtype = torch.float32,</span><br><span class="hljs-params">  device: <span class="hljs-type">Union</span>[torch.device, <span class="hljs-string">&quot;str&quot;</span>] = <span class="hljs-string">&quot;cpu&quot;</span>,</span><br><span class="hljs-params"></span>) -&gt; torch.Tensor:<br>  <span class="hljs-comment"># If shape is not cached, create a new causal mask and cache it</span><br>  input_shape = (batch_size, query_length)<br>  past_key_values_length = key_value_length - query_length<br><br>  <span class="hljs-comment"># create causal mask</span><br>  <span class="hljs-comment"># [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]</span><br>  causal_4d_mask = <span class="hljs-literal">None</span><br>  <span class="hljs-keyword">if</span> input_shape[-<span class="hljs-number">1</span>] &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> self.sliding_window <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>      causal_4d_mask = self._make_causal_mask(<br>          input_shape,<br>          dtype,<br>          device=device,<br>          past_key_values_length=past_key_values_length,<br>          sliding_window=self.sliding_window,<br>      )<br><br>  <span class="hljs-keyword">return</span> causal_4d_mask<br></code></pre></td></tr></table></figure>
<p>上面的函数输入是一个二维张量<code>(batch_size, seq_len)</code>，输出是一个四维<code>(batch_size, 1, seq_len, key_value_len)</code>。该函数会去调用 <code>AttentionMaskConverter</code> 的 <code>to_causal_4d</code> 或 <code>to_4d</code>，而它们俩弯弯绕绕的，但最终仍离不开 <code>_make_causal_mask</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_make_causal_mask</span>(<span class="hljs-params"></span><br><span class="hljs-params">  input_ids_shape: torch.Size,</span><br><span class="hljs-params">  dtype: torch.dtype,</span><br><span class="hljs-params">  device: torch.device,</span><br><span class="hljs-params">  past_key_values_length: <span class="hljs-built_in">int</span> = <span class="hljs-number">0</span>,</span><br><span class="hljs-params">  sliding_window: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>):<br>  bsz, tgt_len = input_ids_shape<br>  mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).<span class="hljs-built_in">min</span>, device=device)<br>  mask_cond = torch.arange(mask.size(-<span class="hljs-number">1</span>), device=device)<br>  mask.masked_fill_(mask_cond &lt; (mask_cond + <span class="hljs-number">1</span>).view(mask.size(-<span class="hljs-number">1</span>), <span class="hljs-number">1</span>), <span class="hljs-number">0</span>)<br>  mask = mask.to(dtype)<br><br>  <span class="hljs-keyword">if</span> past_key_values_length &gt; <span class="hljs-number">0</span>:<br>    mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype, device=device), mask], dim=-<span class="hljs-number">1</span>)<br><br>  <span class="hljs-comment"># add lower triangular sliding window mask if necessary</span><br>  <span class="hljs-keyword">if</span> sliding_window <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>    diagonal = past_key_values_length - sliding_window + <span class="hljs-number">1</span><br>    context_mask = <span class="hljs-number">1</span> - torch.triu(torch.ones_like(mask, dtype=torch.<span class="hljs-built_in">int</span>), diagonal=diagonal)<br>    mask.masked_fill_(context_mask.<span class="hljs-built_in">bool</span>(), torch.finfo(dtype).<span class="hljs-built_in">min</span>)<br><br>  <span class="hljs-keyword">return</span> mask[<span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, :, :].expand(bsz, <span class="hljs-number">1</span>, tgt_len, tgt_len + past_key_values_length)<br></code></pre></td></tr></table></figure>
<p><code>_make_causal_mask</code> 函数最关键的是前几句话。首先 <code>mask</code> 会被初始化成 <code>(batch_size, seq_len)</code> 维度的矩阵，初始值为很大的负数👇。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).<span class="hljs-built_in">min</span>, device=device)<br></code></pre></td></tr></table></figure>
<p>然后使用 <code>mask_cond</code> 来将<strong>矩阵下半角矩阵归零</strong>，重点在<code>mask_cond &lt; (mask_cond + 1).view(mask.size(-1), 1)</code>。此处两个横纵向量一比较会产生一个上三角矩阵。随后将全零矩阵与 <code>mask</code> 相连接，最后改变矩阵维度为 <code>(bsz, 1, seq_len, key_value_len)</code>返回。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">mask_cond = torch.arange(mask.size(-<span class="hljs-number">1</span>), device=device)<br>mask.masked_fill_(mask_cond &lt; (mask_cond + <span class="hljs-number">1</span>).view(mask.size(-<span class="hljs-number">1</span>), <span class="hljs-number">1</span>), <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure>
<p>这里大家不妨思考一下：为什么 Mask 矩阵必须是下三角归零，而上三角全为最小值？上文只是简单地说是为了防止“作弊”。具体原因是：生成文本序列时，模型只能参考之前的词，而不能参考未来生成的词。当 decoder 使用 masked 注意力机制生成输出时，计算 $ QK^T $ 的注意力权重时，我们允许 query 去查看之前生成词的信息，但不允许 query 查看之后生成的词（因为它们还未被产生）。对应到矩阵乘法中，就意味着 query 对应的行向量序号必须大于等于 key 的行向量序号。</p>
<p><strong>译码层</strong></p>
<p>好，我们在注意力掩码这边花了太多功夫了。接下来继续看 <code>forward</code> 函数的实现👇：首先是初始化张量，然后就是 <code>llamaModel</code> 对译码层的具体处理。对于这个主 for 循环，先抛开使用检查点的逻辑部分，直接看调用 <code>decoder_layer</code> 部分，就会发现循环只是在不断地调用 <code>decoder_layer</code>（也就是 <code>LlamaDecoderLayer:forward</code>）来进行推理，然后把得到的输出结果再作为下一层的输入继续前推，直到所有子层的前推结束。循环退出后，<code>hidden_states</code> 会最后加一层归一化，最后通过 transformers 自带的 <code>BaseModelOutputWithPast</code> 将最后的输出张量和 kv 相关信息返回，该类是框架中包含 past kv 值的基础模型输出类，关于此类就不详细展开讲了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># decoder layers</span><br><span class="hljs-comment"># initialize  all_hidden_states all_self_attns next_decoder_cache</span><br><span class="hljs-keyword">for</span> idx, decoder_layer <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(self.layers):<br>  <span class="hljs-keyword">if</span> output_hidden_states:<br>    all_hidden_states += (hidden_states,)<br>  past_key_value = past_key_values[idx] <span class="hljs-keyword">if</span> past_key_values <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br><br>  <span class="hljs-keyword">if</span> self.gradient_checkpointing <span class="hljs-keyword">and</span> self.training:<br>    <span class="hljs-comment">#layer_outputs = self._gradient_checkpointing_func(</span><br>  <span class="hljs-keyword">else</span>:<br>      layer_outputs = decoder_layer(<br>          hidden_states,<br>          attention_mask=attention_mask,<br>          position_ids=position_ids,<br>          past_key_value=past_key_value,<br>          output_attentions=output_attentions,<br>          use_cache=use_cache,<br>      )<br><br>  hidden_states = layer_outputs[<span class="hljs-number">0</span>]<br><br>  <span class="hljs-keyword">if</span> use_cache:<br>      next_decoder_cache += (layer_outputs[<span class="hljs-number">2</span> <span class="hljs-keyword">if</span> output_attentions <span class="hljs-keyword">else</span> <span class="hljs-number">1</span>],)<br><br>  <span class="hljs-keyword">if</span> output_attentions:<br>      all_self_attns += (layer_outputs[<span class="hljs-number">1</span>],)<br><br>hidden_states = self.norm(hidden_states)<br><br><span class="hljs-comment"># add hidden states from the last decoder layer</span><br><span class="hljs-keyword">if</span> output_hidden_states:<br>  all_hidden_states += (hidden_states,)<br><br>next_cache = next_decoder_cache <span class="hljs-keyword">if</span> use_cache <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> return_dict:<br>  <span class="hljs-keyword">return</span> <span class="hljs-built_in">tuple</span>(v <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> [hidden_states, next_cache, all_hidden_states, all_self_attns] <span class="hljs-keyword">if</span> v <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>)<br><span class="hljs-keyword">return</span> BaseModelOutputWithPast(<br>  last_hidden_state=hidden_states,<br>  past_key_values=next_cache,<br>  hidden_states=all_hidden_states,<br>  attentions=all_self_attns,<br>)<br></code></pre></td></tr></table></figure>
<p>篇外：for 循环中使用 <code>gradient_checkpointing</code> 可以有效节约显存，详细内容可以参考 <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/checkpoint.html"><code>torch.utils.checkpoint.checkpoint</code></a>。它的原理非常简单：规定程序在对 <code>decoderLayer</code> 进行前推时，不保存中间计算值。而若模型需要自动微分以完成 backward，程序会重新计算这些中间值，从而节省了模型运算需要的内存空间。因此，<code>use_cache</code> 和 <code>gradient_checkpointing</code>最好不要同时设置为 true，因为一个是用空间换时间，一个是时间换空间，两者可能会互相抵消优化影响。</p>
<h2 id="最终成型">最终成型</h2>
<p>由于篇幅关系，这里仅介绍最常用的 <code>LlamaForCausalLM</code>，该模型是因果类语言模型，可以根据用户给的上文来续写下文，也可以回答用户提出的问题。</p>
<h3 id="LlamaForCausalLM">LlamaForCausalLM</h3>
<p><code>LlamaForCausalLM</code> 是 Llama 因果类语言模型，可以根据用户输入的文本输出相应的回答。技术上看，它在 <code>LlamaModel</code> 的基础上增加了一个线性层 <code>lm_head</code> 作为 Generator，从而实现了一个完整的语言模型。我们来看一下它的 <code>forward</code> 函数👇，若仔细对比之前模型的输入参数，会发现多了一个可选传入的 <code>label</code> 张量，该张量形状是 <code>(batch_size, seq_len)</code>，它是用于计算 masked 语言模型的损失值。该模型的 <code>forward</code> 函数在准备好输入的参数后，就直接调用了 <code>LlamaModel:forward()</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">  self,</span><br><span class="hljs-params">  input_ids: torch.LongTensor = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">  attention_mask: <span class="hljs-type">Optional</span>[torch.Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">  position_ids: <span class="hljs-type">Optional</span>[torch.LongTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">  past_key_values: <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[torch.FloatTensor]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">  inputs_embeds: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">  labels: <span class="hljs-type">Optional</span>[torch.LongTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">  use_cache: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">  output_attentions: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">  output_hidden_states: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">  return_dict: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Union</span>[<span class="hljs-type">Tuple</span>, CausalLMOutputWithPast]:<br><br>output_attentions = output_attentions <span class="hljs-keyword">if</span> output_attentions <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> self.config.output_attentions<br>output_hidden_states = (<br>    output_hidden_states <span class="hljs-keyword">if</span> output_hidden_states <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> self.config.output_hidden_states<br>)<br>return_dict = return_dict <span class="hljs-keyword">if</span> return_dict <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> self.config.use_return_dict<br><br><span class="hljs-comment"># decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)</span><br>outputs = self.model(<br>    input_ids=input_ids,<br>    attention_mask=attention_mask,<br>    position_ids=position_ids,<br>    past_key_values=past_key_values,<br>    inputs_embeds=inputs_embeds,<br>    use_cache=use_cache,<br>    output_attentions=output_attentions,<br>    output_hidden_states=output_hidden_states,<br>    return_dict=return_dict,<br>)<br></code></pre></td></tr></table></figure>
<p>随后，将拿到手的 <code>outputs</code> 放入到添加的线性层 <code>lm_head</code> 进行运算。同理，TP 并行时会将线性层的矩阵在 dim=0 维度拆分。若传入了 <code>label</code>，那么得到的结果 <code>logits</code> 在经过移位后计算交叉熵 <code>CrossEntropyLoss()</code>，若无则可直接通过 <code>CausalLMOutputWithPast</code> 返回。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python">hidden_states = outputs[<span class="hljs-number">0</span>]<br><span class="hljs-keyword">if</span> self.config.pretraining_tp &gt; <span class="hljs-number">1</span>:<br>  lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=<span class="hljs-number">0</span>)<br>  logits = [F.linear(hidden_states, lm_head_slices[i]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.config.pretraining_tp)]<br>  logits = torch.cat(logits, dim=-<span class="hljs-number">1</span>)<br><span class="hljs-keyword">else</span>:<br>  logits = self.lm_head(hidden_states)<br>logits = logits.<span class="hljs-built_in">float</span>()<br>loss = <span class="hljs-literal">None</span><br><span class="hljs-keyword">if</span> labels <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>  <span class="hljs-comment"># Shift so that tokens &lt; n predict n</span><br>  shift_logits = logits[..., :-<span class="hljs-number">1</span>, :].contiguous()<br>  shift_labels = labels[..., <span class="hljs-number">1</span>:].contiguous()<br>  <span class="hljs-comment"># Flatten the tokens</span><br>  loss_fct = CrossEntropyLoss()<br>  shift_logits = shift_logits.view(-<span class="hljs-number">1</span>, self.config.vocab_size)<br>  shift_labels = shift_labels.view(-<span class="hljs-number">1</span>)<br>  <span class="hljs-comment"># Enable model parallelism</span><br>  shift_labels = shift_labels.to(shift_logits.device)<br>  loss = loss_fct(shift_logits, shift_labels)<br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> return_dict:<br>  output = (logits,) + outputs[<span class="hljs-number">1</span>:]<br>  <span class="hljs-keyword">return</span> (loss,) + output <span class="hljs-keyword">if</span> loss <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> output<br><span class="hljs-keyword">return</span> CausalLMOutputWithPast(<br>    loss=loss,<br>    logits=logits,<br>    past_key_values=outputs.past_key_values,<br>    hidden_states=outputs.hidden_states,<br>    attentions=outputs.attentions,<br>)<br></code></pre></td></tr></table></figure>
<p>使用 huggingface 框架实现的 <code>LlamaForCausalLM</code> 进行推理的示例如下，从该示例中我们可以更好地理解大模型的推理过程：比如说，上面的输入张量<code>input_ids</code> 是用户输入的处理后的“文本”。而最初用户输入的字符串<code>prompt</code> 先进入 tokenizer 进行分词，随后编码、嵌入技术变为张量。这里最重要的函数莫属于 <code>model.generate</code>，但它只能在推理时使用。它除了在背后默默调用了上面的 <code>forward</code>，还做了很多：用于多种解码策略，例如 beam search、top-k 采样等……详细的文章可以在<a target="_blank" rel="noopener" href="https://huggingface.co/blog/how-to-generate">这篇博客</a>中找到。生成产生的张量人类无法直接看懂，还需要经过解码 <code>batch_decode</code> 才能呈现流利的英语。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, LlamaForCausalLM<br><br>model = LlamaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)<br>tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)<br>prompt = <span class="hljs-string">&quot;Hey, are you conscious? Can you talk to me?&quot;</span><br>inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)<br><span class="hljs-comment"># Generate</span><br>generate_ids = model.generate(inputs.input_ids, max_length=<span class="hljs-number">30</span>)<br>tokenizer.batch_decode(generate_ids, skip_special_tokens=<span class="hljs-literal">True</span>, clean_up_tokenization_spaces=<span class="hljs-literal">False</span>)[<span class="hljs-number">0</span>]<br><span class="hljs-string">&quot;Hey, are you conscious? Can you talk to me?\nI&#x27;m not conscious, but I can talk to you.&quot;</span><br></code></pre></td></tr></table></figure>
<h2 id="小结">小结</h2>
<p>本文从 llama 是什么出发，深入解读了 huggingface 框架对 llama 的代码实现。我先从 llama 论文开始解读，试图让所有未接触过大模型的外行人能理解大模型是如何被训练产生的。随后，我给出了 llama 的架构图，并简要说明了 llama1/2 和 transformer 框架的区别，以及为何要这样改进。然后，我从基础到上层逐个分析了 llama 的代码实现，由于我也是第一次如此细致地阅读大模型的代码，因此很多地方可能会比较啰嗦。但万事开头难，在研究的最初阶段尽可能搞清楚最基本的东西，步步为营方能豁然开朗。另外，本篇博文必不可能覆盖 llama 乃至大模型的方方面面，我等还需继续努力，进一步揭开大模型的神秘面纱。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/AI/" class="category-chain-item">AI</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/AI/" class="print-no-link">#AI</a>
      
        <a href="/tags/Transformer/" class="print-no-link">#Transformer</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>huggingface下llama代码细读（下）</div>
      <div>https://dingfen.github.io/2023/11/30/2023-11-30-huggingface2/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Bill Ding</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年11月30日</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>更新于</div>
          <div>2025年1月26日</div>
        </div>
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="SA - 相同方式共享">
                    <i class="iconfont icon-sa"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/11/30/2023-11-30-LLM-KVCache/" title="大模型推理优化技术之显存优化">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">大模型推理优化技术之显存优化</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/10/30/2023-10-30-huggingface1/" title="huggingface下llama代码细读（上）">
                        <span class="hidden-mobile">huggingface下llama代码细读（上）</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  








    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  var relativeDate = function() {
    var updatedTime = document.getElementById('updated-time');
    if (updatedTime) {
      var text = updatedTime.textContent;
      var reg = /\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:Z|[+-]\d{2}:\d{2})/;
      var matchs = text.match(reg);
      if (matchs) {
        var relativeTime = moment(matchs[0]).fromNow();
        updatedTime.textContent = text.replace(reg, relativeTime);
      }
      updatedTime.style.display = '';
    }
  };
  Fluid.utils.createScript('https://lib.baomitu.com/moment.js/2.29.4/moment.min.js', function() {
    if (!'zh-cn'.startsWith('en')) {
      Fluid.utils.createScript('https://lib.baomitu.com/moment.js/2.29.4/locale/zh-cn.min.js', function() {
        relativeDate();
      });
    } else {
      relativeDate();
    }
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  
<script src="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/Ribbon.min.js"></script>



<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<!-- hexo injector body_end start -->
  <link rel="stylesheet" crossorigin href="https://g.alicdn.com/aliyun-documentation/web-chatbot-ui/0.0.11/index.css" />
  <script type="module" crossorigin src="https://g.alicdn.com/aliyun-documentation/web-chatbot-ui/0.0.11/index.js"></script>
  <script>
    window.CHATBOT_CONFIG = {
      endpoint: "https://web-chatbot-syz-knthhrjfeq.cn-hangzhou.fcapp.run/chat", // 可以替换为 https://{your-fc-http-trigger-domain}/chat
      displayByDefault: false, // 默认不显示 AI 助手对话框
      aiChatOptions: { // 自定义取值参考：https://docs.nlkit.com/nlux/reference/ui/ai-chat#conversation-options
        conversationOptions: { // 自定义取值参考：https://docs.nlkit.com/nlux/reference/ui/ai-chat#conversation-options
          conversationStarters: [
            {prompt: '请问你是谁，能为我做什么？'},
            {prompt: '请介绍一下博客的主人'}
          ],
          layout: 'bubbles'
        },
        displayOptions: { // 自定义取值参考：https://docs.nlkit.com/nlux/reference/ui/ai-chat#display-options
          height: 550,
          // width: 400,
        },
        personaOptions: { // 自定义取值参考：https://docs.nlkit.com/nlux/reference/ui/ai-chat#chat-personas
          assistant: {
            name: '你好，我是本网站的 AI 助手',
            // AI 助手的图标
            avatar: 'https://img.alicdn.com/imgextra/i2/O1CN01Pda9nq1YDV0mnZ31H_!!6000000003025-54-tps-120-120.apng',
            tagline: '输入您的问题，我会尽力帮你解答！',
          }
        }
      }
    };
  </script>
  <style>
    :root {
      /* webchat 工具栏的颜色 */
      --webchat-toolbar-background-color: #1464E4;
      /* webchat 工具栏文字和按钮的颜色 */
      --webchat-toolbar-text-color: #FFF;
    }
    /* webchat 对话框如果被遮挡，可以尝试通过 z-index、bottom、right 等设置 来调整*/
    .webchat-container {
      z-index: 100;
      bottom: 10px;
      right: 10px;
    }
    /* webchat 的唤起按钮如果被遮挡，可以尝试通过 z-index、bottom、right 等设置 来调整。也可以通过 CSS 进一步定制唤起按钮的形状、大小等。 */
    .webchat-bubble-tip {
      z-index: 99;
      bottom: 20px;
      right: 20px;
    }
  </style>
  <!-- hexo injector body_end end --></body>
</html>
