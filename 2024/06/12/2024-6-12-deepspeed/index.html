

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Bill Ding">
  <meta name="keywords" content="">
  
    <meta name="description" content="探究 dynamic splitfuse 的奥秘">
<meta property="og:type" content="article">
<meta property="og:title" content="深入探索 DeepSpeed（三）">
<meta property="og:url" content="https://dingfen.github.io/2024/06/12/2024-6-12-deepspeed/index.html">
<meta property="og:site_name" content="峰子的乐园">
<meta property="og:description" content="探究 dynamic splitfuse 的奥秘">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://dingfen.github.io/img/LLM/deepspeed_logo.png">
<meta property="article:published_time" content="2024-06-12T14:03:00.000Z">
<meta property="article:modified_time" content="2025-01-26T11:49:09.209Z">
<meta property="article:author" content="Bill Ding">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="DeepSpeed">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://dingfen.github.io/img/LLM/deepspeed_logo.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>深入探索 DeepSpeed（三） - 峰子的乐园</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/KaTeX/0.16.2/katex.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"dingfen.github.io","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"|","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":null,"onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.1.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>峰子的乐园</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/LLM/deepspeed-four.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="深入探索 DeepSpeed（三）"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-06-12 22:03" pubdate>
          2024年6月12日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          6.2k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          52 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">深入探索 DeepSpeed（三）</h1>
            
              <p id="updated-time" class="note note-info" style="display: none">
                
                  
                    <!-- compatible with older versions-->
                    更新于：2025-01-26T19:49:09+08:00
                  
                  

                
              </p>
            
            
              <div class="markdown-body">
                
                <h1>DeepSpeed 高性能算子实现</h1>
<p><a href="https://dingfen.github.io/2024/05/15/2024-5-15-DeepSpeed/">上篇博客</a>的最后，<s>我发现 DeepSpeed inference v1 版本的算子代码似乎有精度问题。笔者在 A100 上用 DeepSpeed 推理模型，当打开 <code>replace_with_kernel_inject</code> 后，模型使用上面介绍的算子做推理后，其回答会变成一些乱码，或者是胡言乱语。本人怀疑是 v1 高性能算子代码实现有精度问题，进而导致模型回答混乱。</s></p>
<p>因此，这里不再介绍 DeepSpeed inference v1 版本的算子代码，我们来看看 inference V2 代码。</p>
<p>注：本篇博文的源码分析基于 DeepSpeed-0.14.2。</p>
<p>也许是因为 DeepSpeed inference v1 的实现不是很理想，微软很快又把 DeepSpeed inference v2 端了上来，这篇博客我们重点来关注一下 DeepSpeed inference v2 的内部实现，包括它的引擎，以及<a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepSpeed/tree/master/blogs/DeepSpeed-fastgen">它博客</a>中介绍的最大特点——动态分割融合（Dynamic SplitFuse）到底是如何实现的。</p>
<h2 id="DeepSpeed-inference-v2-示例">DeepSpeed inference v2 示例</h2>
<p>老规矩，开始之前，我们总要拿一个例子来跑一跑：</p>
<p>首先，按照 DeepSpeed 和 DeepSpeed-mii 的要求，需要安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install DeepSpeed DeepSpeed-mii<br></code></pre></td></tr></table></figure>
<p>然后，如果我们只是想临时跑一下模型看看它的输出，就可以：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> mii<br>pipe = mii.pipeline(<span class="hljs-string">&quot;mistralai/Mistral-7B-v0.1&quot;</span>)<br>response = pipe([<span class="hljs-string">&quot;DeepSpeed is&quot;</span>, <span class="hljs-string">&quot;Seattle is&quot;</span>], max_new_tokens=<span class="hljs-number">128</span>)<br><span class="hljs-built_in">print</span>(response)<br></code></pre></td></tr></table></figure>
<p>然后就可以直接运行该程序：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Run on a single GPU</span><br>DeepSpeed --num_gpus 1 mii-example.py<br><br><span class="hljs-comment"># Run on multiple GPUs</span><br>DeepSpeed --num_gpus 2 mii-example.py<br></code></pre></td></tr></table></figure>
<p>如果要比较久地部署到服务器上，并方便客户端经常访问调用，那么代码会稍微复杂一点：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> mii<br>client = mii.serve(<span class="hljs-string">&quot;mistralai/Mistral-7B-v0.1&quot;</span>)<br>response = client.generate([<span class="hljs-string">&quot;Deepspeed is&quot;</span>, <span class="hljs-string">&quot;Seattle is&quot;</span>], max_new_tokens=<span class="hljs-number">128</span>)<br><span class="hljs-built_in">print</span>(response)<br><br><span class="hljs-comment"># or in another process</span><br>client = mii.client(<span class="hljs-string">&quot;mistralai/Mistral-7B-v0.1&quot;</span>)<br>response = client.generate(<span class="hljs-string">&quot;Deepspeed is&quot;</span>, max_new_tokens=<span class="hljs-number">128</span>)<br></code></pre></td></tr></table></figure>
<p>当我们把程序跑起来后，自然要想，它究竟是如何跑起来的。</p>
<h2 id="DeepSpeed-mii">DeepSpeed-mii</h2>
<h3 id="动态分割融合">动态分割融合</h3>
<p>为了充分释放 GPU 的性能，大模型会将若干个用户输入语句（prompt）合并，一起做批处理。然而，用户输入的 prompt 到大模型内的请求语句必然是长短不一的，此时常规做法是，模型通过填充（padding）的手段将请求中的输入 token 张量填充为相同的形状，再做进一步处理。但这样做还是浪费了不少 GPU 的性能，于是就有了连续批处理技术（Continuous batching）——将每个用户输入语句尽可能地“拼接”起来，补齐空白。</p>
<p><img src="/img/LLM/vllm_dynamic_batch_1.png" srcset="/img/loading.gif" lazyload alt=""><br>
<img src="/img/LLM/vllm_dynamic_batch_2.png" srcset="/img/loading.gif" lazyload alt="vllm 的动态批处理"></p>
<p>另一个大模型推理的性能瓶颈是，KV cache 占用空间过大。为此，vllm 提出了 KV cache 分块技术，借鉴操作系统中分页（paging）的概念，将本应该连续存放的 KV cache 做分块处理。具体操作是，vllm 将 KV cache 分成若干分固定长度的块，并通过映射表建立了逻辑 KV 块和物理 KV 块间的关联。处理时，逻辑块连续，但其映射到物理内存上的物理 KV 块可以分开存放，这样既可以减少内存碎片，也可以增加 KV block 共享的可能。</p>
<p><img src="/img/LLM/vllm_page_attn.webp" srcset="/img/loading.gif" lazyload alt=""><br>
<img src="/img/LLM/vllm_page_attn_2.webp" srcset="/img/loading.gif" lazyload alt=""></p>
<p>而 DeepSpeed 工程师进一步发现，prompt 阶段往往因处理的 token 过多而出现计算瓶颈，generation 阶段往往因处理的 token 过少（仅有 1）而陷入内存瓶颈。因此，它将上述两者技术结合，提出了动态分割融合（dynamic splitfuse）的优化技术。这是一种用于 prompt 阶段和 generation 阶段的 token 组合策略。<strong>具体操作是，通过从 prompt 阶段中取出部分 token 与 generation 阶段的 token 计算结合，使得模型可以保持一致的前向推理大小（forward size）</strong>。如此保持一定量的前推大小，可以使得 GPU 的计算性能和访存带宽都得到较好的发挥👇。</p>
<p><img src="/img/LLM/observation-prompt-v-flops.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p>为此，动态分割融合需要：</p>
<ul>
<li>将长 prompt 分解成一些小 token 块，并在多个 forward（迭代）中进行调度，只有在最后一次 forward 完成后才能执行生成。</li>
<li>将一些短 prompt 组合起来，以精确地填满目标 token 块。当然，有时一些短的 prompt 也可能被分解，以确保每个小块的长度需求被精确满足，保证前向大小（forward sizes）对齐。</li>
</ul>
<p>从实验中看，动态分割融合技术提升了以下性能指标：</p>
<ul>
<li><strong>更低的延迟</strong>： 由于长 prompt 不再需要极长的前向传递来处理，模型将提供更低的客户端延迟，因为该技术能充分发挥 GPU 性能，让它在同一时间内执行更多的前向传递。</li>
<li><strong>更高的吞吐量</strong>： 对短 prompt 的融合能使模型持续运行在高吞吐状态，其实逻辑和第一点一致。</li>
<li><strong>更低的波动和更好的一致性</strong>： 由于前向传递的大小一致，且前向传递大小是性能的主要决定因素，每个前向传递的延迟比其他系统更加一致。生成频率也是如此，因为DeepSpeed-FastGen不需要像其他先前的系统那样抢占或长时间处理 prompt ，因此延迟会更低。</li>
</ul>
<p>因此，与现有最先进的大模型推理系统相比，DeepSpeed-FastGen 既可以迅速、持续地处理 prompt 的 token，还能同时完成 token 的 generation。该技术不仅提高了系统利用率，也获得了更低的延迟和更高的吞吐量。</p>
<p><img src="/img/LLM/fastgen-overview-light.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p>图: 连续批处理策略的示意图。每个块显示一个前向推理的执行。箭头表示前向推理有一个或多个生成的 token 序列。vLLM 在一个前向推理中要么是 token generation 阶段，只产生一个 token，要么处理 prompt 阶段，处理一堆 token；这里 token 的生成抢占了 prompt 的处理。Orca 则以完整长度处理 prompt。DeepSpeed-FastGen 的动态分割融合则执行固定大小 token 块，对它们做前向推理。</p>
<h3 id="代码实现">代码实现</h3>
<p>我们从上面例子中的 <code>mii.pipeline</code> 出发：下面的介绍包括了 <code>mii.pipeline</code> 和基类 <code>RaggedBatchBase</code>  在拿到 batch 个 request 后的工作流程。</p>
<p><img src="/img/LLM/miipipeline.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>我们将 pipeline 的调用代码分成三部分（红框标明），接下来分成三部分对代码做说明。</p>
<h4 id="第一部分">第一部分</h4>
<p>首先看第一个红框，这部分主要完成对用户输入 prompt 的切分、调度。它将输入的 prompts 全都导入到分词器 tokenizer 中，经过 <code>_put_request</code> 的编码后，包装产生 request。这个 request 包含了一切大模型推理需要的用户输入 tokens 和生成参数(generation config)，然后这些 requet 就会被放入队列中等待调度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_put_request</span>(<span class="hljs-params">self, uid: <span class="hljs-built_in">int</span>, <span class="hljs-built_in">input</span>: <span class="hljs-built_in">str</span>, kwargs: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]</span>) -&gt; <span class="hljs-literal">None</span>:<br>    self.result_queues[self.tid] = queue.Queue()<br>    input_tokens = self.tokenizer.encode(<span class="hljs-built_in">input</span>)<br>    request = self.make_request(self.tid, uid, input_tokens, kwargs)<br>    self.request_queue.put(request)<br></code></pre></td></tr></table></figure>
<p>那么这些 request 是如何被调度的？</p>
<p>很简单，对于那些已经将输入 token 解析完成的，处于生成阶段（generation 阶段）的 request，放入 <code>_schedule_token_gen</code> 中处理，对于还在解析输入（prompt阶段），放到 <code>_schedule_prompts</code> 中处理。prompt 的模型总入口是 <code>self.scheduled_requests</code>，要暂未被调度到的 request 会被放到 <code>self.buffer</code> 中缓存。</p>
<p><img src="/img/LLM/miischedulereq.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>系统会优先处理生成阶段的 request，当出现 kv block 空缺位后，就会加入新的 request。而若 GPU 还有闲余空间（语句（sequence）空间）处理 prompt，那么会继续处理 prompt 阶段的 request。它会依照下面的函数严格限制每个 request 的长度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_kv_requirements</span>(<span class="hljs-params">self, sequence: DSSequenceDescriptor, max_new_tokens: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">        max_new_blocks: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>]:<br>    total_tokens = sequence.seen_tokens + max_new_tokens<br>    req_blocks = ceil_div(total_tokens, self.attn.kv_block_size)<br>    block_lim = req_blocks - sequence.cur_allocated_blocks<br>    <span class="hljs-keyword">if</span> block_lim &lt;= max_new_blocks:<br>        <span class="hljs-keyword">return</span> max_new_tokens, block_lim<br>    token_capacity = (max_new_blocks +<br>        sequence.cur_allocated_blocks) * self.attn.kv_block_size - sequence.seen_tokens<br></code></pre></td></tr></table></figure>
<p><img src="/img/LLM/miischedulepro.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<h4 id="第二部分">第二部分</h4>
<p>第二个红框是模型的执行部分，Rank 1-n 的节点仅负责 generate，而 Rank 0 要考虑的就多了：他不仅需要运行 generate，还需要在最后管理释放 requset，输出 response，在最后让 Rank 1-n 节点退出释放。</p>
<p>先来看一下 <code>generate()</code> 函数，它负责 token 的生成，因此其中的实现对于我们了解 deepspeed inference 如何运作相当重要。</p>
<p><img src="/img/LLM/miigenerate.png" srcset="/img/loading.gif" lazyload alt=""></p>
<ol>
<li><code>_bcast_requests</code> 将 req 广播。<br>
a. <code>RaggedBatchBase</code> 内部将多个 GPU 节点用 <a target="_blank" rel="noopener" href="https://zeromq.org/languages/python/">ZeroMQ</a> 的高性能异步消息库连接起来。<br>
b. 在 <code>_bcast_requests</code> 函数中，Rank 0 节点负责将所有的 <code>self.scheduled_requests</code> 中的 req 转成 json 数据格式，再发给其他 GPU 节点。其他节点则负责接收这些 req。<br>
c. 此外，<code>_bcast_requests(Force=True)</code> 函数在所有 req 处理完毕后会再次被 Rank 0 节点调用，此时发送的 req 其实是空的。但这操作并非多余，这是通知其他 Rank 节点要及时退出释放资源。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_bcast_requests</span>(<span class="hljs-params">self, force=<span class="hljs-literal">False</span></span>) -&gt; RequestBatch:<br>    <span class="hljs-keyword">if</span> self.is_rank_0:<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.scheduled_requests <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> force:<br>            <span class="hljs-keyword">return</span> self.scheduled_requests<br>        <span class="hljs-comment"># Rank 0 gets batch of requests and broadcasts to other ranks</span><br>        data_dicts = self.scheduled_requests.to_msg_dicts()<br>        json_data = ujson.dumps(data_dicts)<br>        self.socket.send_string(json_data)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">try</span>:<br>            json_data = self.socket.recv_string()<br>            data_dicts = ujson.loads(json_data)<br>            self.scheduled_requests = RequestBatch.from_msg_dicts(data_dicts)<br>        <span class="hljs-keyword">except</span> zmq.Again:<br>            self.scheduled_requests = RequestBatch()<br>    <span class="hljs-keyword">return</span> self.scheduled_requests<br></code></pre></td></tr></table></figure>
<ol start="2">
<li><code>flush</code> 函数将 <code>self.scheduled_requests</code> 中要重刷的（已经完成生成的）语句资源都释放掉，包括他的 KV Block Cache 等。这方面的代码涉及到 DeepSpeed 内。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">flush_sequence</span>(<span class="hljs-params">self, uid: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-comment"># Free all resources associated with the given sequence id.</span><br>    seq = self._seqs[uid]<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.n_kv_cache_groups):<br>        self._kv_cache.free(seq.all_block_ids(cache_group=i), cache_group=i)<br><br>    self._tracking_allocator.free(seq.tracking_id)<br>    <span class="hljs-keyword">del</span> self._seqs[uid]<br></code></pre></td></tr></table></figure>
<ol start="3">
<li><code>put</code> 函数将未处理完的 token 加入到推理引擎中，然后执行 forward 前推动作。具体的运作步骤参考下文的推理引擎内容。</li>
<li>启动 logits 处理。<br>
a. 到第三步，其余 Rank 的工作就已经完成了，但对于 Rank 0，还需要做剩下的 4 5 6 7 步<br>
b. Rank 0 在这里会调用 logit_processor 和 sampler 来选择具体的 token 值。相关代码在 deepspeed.mii.batching.postprocess 中可以找到</li>
<li><code>reset_scheduler_bookkeeping</code> 对 <code>self.scheduled_requests</code> 清空！大家可能会奇怪，之前我们几乎所有的操作都是为了或者说基于这个 <code>scheduled_requests</code> 而去做的，现在说清空就清空是不是有点前功尽弃的感觉了。emm 确实，但所谓不破不立吧，有时也许做好的办法就是打破重来。</li>
<li>Rank 0 需要检查是否完成生成。对于已经完成了的 req，就会从运行的 <code>scheduled_requests.requests_to_run</code> 列表中删去</li>
<li>调用 <code>schedule_request()</code> （参考第一部分），以目前正在运行的 <code>scheduled_requests.requests_to_run</code> 为基础，建立一个新的调度 req 列。</li>
</ol>
<h4 id="第三部分">第三部分</h4>
<p>当程序执行完所有的 batch 的生成词后，需要将最终的结果返回，如果需要的话，还要将 output 广播到所有的 Rank 节点上。</p>
<p>这其中，<code>get_response</code> 函数负责调用 tokenizer 解码，生成字符串。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_response</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, Response]:<br>    result = self.result_queues[self.tid].get()<br>    uid = result[<span class="hljs-number">0</span>]<br>    generated_tokens = self.tokenizer.decode(result[<span class="hljs-number">1</span>])<br>    response = self.make_response(generated_tokens, result[<span class="hljs-number">2</span>], result[<span class="hljs-number">3</span>], result[<span class="hljs-number">4</span>])<br>    <span class="hljs-keyword">return</span> uid, response<br></code></pre></td></tr></table></figure>
<p><code>bcast_responses</code> 则负责将回答广播到所有节点上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_bcast_responses</span>(<span class="hljs-params">self, responses: <span class="hljs-type">List</span>[Response]</span>) -&gt; <span class="hljs-type">List</span>[Response]:<br>    <span class="hljs-keyword">if</span> self.is_rank_0:<br>        data_dicts = [r.to_msg_dict() <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> responses]<br>        json_data = ujson.dumps(data_dicts)<br>        self.socket.send_string(json_data)<br>    <span class="hljs-keyword">else</span>:<br>        json_data = self.socket.recv_string()<br>        data_dicts = ujson.loads(json_data)<br>        responses = [Response.from_msg_dict(msg) <span class="hljs-keyword">for</span> msg <span class="hljs-keyword">in</span> data_dicts]<br>    <span class="hljs-keyword">return</span> responses<br></code></pre></td></tr></table></figure>
<h2 id="推理引擎v2">推理引擎v2</h2>
<p>与第一版推理引擎相比，第二版推理引擎的代码要简单易懂的多（这就是代码重构带来的后发优势）。<code>InferenceEngineV2</code> 有三个成员：推理config、模型本身和推理状态管理器，可谓简单清楚直观。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">InferenceEngineV2</span>:<br>    _config: RaggedInferenceEngineConfig<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Configuration of the inference engine.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    _model: DSInferenceModelBase<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Inference model supporting ragged inference.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    _state_manager: DSStateManager<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Persistent state manager for sequences and KV-cache.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<p>而且，<code>InferenceEngineV2</code> 支持类似 vllm 中的动态批处理功能（dynamic batching）（有时也称为 continuous batching）。为了方便介绍下文的嵌入操作，这里先提一下模型是如何将长短不一的各个输入query统一处理的，并实现 continuous batching 的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">self._batch = RaggedBatchWrapper(self._config.state_manager)<br></code></pre></td></tr></table></figure>
<p>首先，在构建时，它使用了 <code>RaggedBatchWrapper</code> 包装了必要的数据结构，包括 input ids，tokens 和 seqs 的相关信息。然后，<code>put()</code> 将输入的批处理 sequence 逐个插入到 <code>_batch</code> 内：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python">self._batch.clear()<br><span class="hljs-comment"># 做推理前的准备工作，创造或拿到对应 seq，分配 KV Block，插入 token 等</span><br><span class="hljs-keyword">for</span> uid, tokens <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(batch_uids, batch_tokens):<br>    host_seq_desc = self._state_manager.get_or_create_sequence(uid)<br>    self._model.maybe_allocate_kv(host_seq_desc, tokens.numel())<br>    host_seq_desc.pre_forward(tokens.numel())<br>    self._batch.insert_sequence(host_seq_desc, tokens, do_checks=do_checks)<br><br><span class="hljs-comment"># 做 raggedBatch 相关数据的更新</span><br>self._batch.finalize()x<br><br><span class="hljs-comment"># Prep all data structures for the actual forward (in anticipation of CG in the future)</span><br><span class="hljs-comment"># and also to amortize some of the costs in a more straightforward way.</span><br>self._model.prepare_batch(self._batch)<br><br><span class="hljs-comment"># 做推理，计算出 logits 张量</span><br>logits = self._model.forward(self._batch)<br><br><span class="hljs-comment"># We return one set of logits per sequence in the batch (saves cost on unembedding)</span><br><span class="hljs-keyword">assert</span> logits.shape[<span class="hljs-number">0</span>] == self._batch.current_sequences<br></code></pre></td></tr></table></figure>
<p>在设计使用 <code>insert_sequence</code> 函数时，DeepSpeed 的工程师发现，将句子逐个插入到列表中会导致内存访问因碎片过多而过慢，因此他们的做法是先插入所有的句子到一个临时列表中，然后再使用 <code>finalize()</code> 函数将全体数据更新到 <code>RaggedBatchWrapper</code> 内的最终目前数据结构中。然后，<code>prepare_batch</code> 准备所有的用于前推数据结构，并做摊还分析。</p>
<hr>
<p>现在，我们来看看 DeepSpeed 推理 v2 版本有哪些新花样。还是从 llama2 模型出发，先看看整个模型架构中，高性能 kernel 的使用情况：</p>
<p><img src="/img/LLM/llama2inferencemodel.png" srcset="/img/loading.gif" lazyload alt=""></p>
<h2 id="参差不齐的嵌入操作">参差不齐的嵌入操作</h2>
<p>为了充分释放 GPU 的性能，模型会将若干个用户输入的请求做批处理。然而，用户输入到大模型内的请求语句必然是长短不一的，此时常规做法是，模型通过填充（padding）的手段将请求中的输入张量填充为相同的形状，再做进一步处理。但这样做还是浪费了不少 GPU 的性能，为此，DeepSpeed 提供了类似 vllm 中的动态批处理功能（dynamic batching）。它将同一模型执行的多个请求组合在一起，并且不需要填充短请求，就可直接进行批处理，从而获得更大的吞吐量。</p>
<p><img src="/img/LLM/vllm_dynamic_batch_1.png" srcset="/img/loading.gif" lazyload alt=""><br>
<img src="/img/LLM/vllm_dynamic_batch_2.png" srcset="/img/loading.gif" lazyload alt="vllm 的动态批处理"></p>
<p>因为缺少了填充对齐，因此取名“参差不齐的嵌入操作”，英文名 RaggedEmbedding，这恰好对应了类名 <code>DSRaggedEmbedding</code>。而要想充分理解 <code>DSRaggedEmbedding</code> 的实现，我们必须从与它密切相关的类 <code>RaggedBatchWrapper</code> 和 <code>RaggedEmbeddingKernel</code> 开始聊起。</p>
<h3 id="RaggedBatchWrapper">RaggedBatchWrapper</h3>
<p>虽然这类只是一个 wrapper，仅封装了相关的值而已。但我觉得这内部的值可以很好地帮助我们理解动态批处理的实现细节。因此在这里展开讲讲。</p>
<p>为了让更清楚明白，举一个简单的例子：假设现在模型正在处理 8 个 token，这 8 个 token 分别位于 3 个 sequence 中：</p>
<p><img src="/img/LLM/raggedEmbedding.png" srcset="/img/loading.gif" lazyload alt="RaggedBatch Embedding所需要的数据结构"></p>
<p>其中，sequence 指的是每个请求读入的句子，seq_lens 表示一个 batch 中所有的句子。<code>RaggedBatchWrapper</code> 的成员还有：</p>
<ul>
<li><code>input_ids</code> 用户输入的句子，被切分、数字化后，成为一列整数列</li>
<li><code>_batch_metadata_storage</code> 存放正在处理的句子数量和 tokens 数量，在本例中其值为 <code>(sum(seq_lens), len(seq_lens))</code></li>
<li><code>_token_to_seq_storage</code> 存放了 token_id 映射到 seq_id 的表，通过它每个 token 可以找到自己的 seq。</li>
<li><code>_inflight_seq_descriptor</code> 存放了每个正在处理的句子的详细信息，包括开始 token 位置，总token 数，目前看见的 token 等。</li>
<li><code>_kv_ptrs</code>  指向 GPU 缓存中的 KV-blocks 指针列表。</li>
</ul>
<h3 id="RaggedEmbeddingKernel">RaggedEmbeddingKernel</h3>
<p>下面就是 <code>DSRaggedEmbedding</code> 的前推函数：可见它直接调用了 <code>RaggedEmbeddingKernel</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">self._ragged_embed = RaggedEmbeddingKernel(self._config.residual_dtype, torch.int32,<br>                                           self._config.embedding_dim)<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, ragged_batch: RaggedBatchWrapper, word_embeddings: torch.Tensor,</span><br><span class="hljs-params">            position_embeddings: <span class="hljs-type">Optional</span>[torch.Tensor] = <span class="hljs-literal">None</span></span>) -&gt; torch.Tensor:<br>    output = empty_from(self._output, (ragged_batch.tensor_toks, self._config.embedding_dim))<br>    self._ragged_embed(output, ragged_batch, word_embeddings,<br>                       position_embed_weight=position_embeddings,<br>                       position_embed_offset=self.embed_offset)<br>    <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure>
<p>而 <code>RaggedEmbeddingKernel</code> 是对 CUDA 实现的算子的一个 python 包装，封装了 <code>ragged_embed</code> C++ 函数的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">RaggedEmbeddingKernel</span>(<span class="hljs-title class_ inherited__">DSKernelBase</span>):<br>    inf_module = RaggedOpsBuilder().load()<br>    self.kernel = inf_module.ragged_embed<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, embedded_tokens: torch.Tensor, ragged_wrapper: RaggedBatchWrapper,</span><br><span class="hljs-params">             embedding_weight: torch.Tensor, position_embed_weight: <span class="hljs-type">Optional</span>[torch.Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">             position_embed_offset: <span class="hljs-built_in">int</span> = <span class="hljs-number">0</span></span>) -&gt; torch.Tensor:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        embedded_tokens (torch.Tensor): Output tensor of shape [num_tokens, embed_dim]</span><br><span class="hljs-string">        ragged_wrapper (RaggedBatchWrapper): Wrapper for the ragged batch.</span><br><span class="hljs-string">        embedding_weight (torch.Tensor): Embedding table of shape [vocab_size, embed_dim]</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    self.kernel(embedded_tokens, ragged_wrapper.input_ids(),<br>                embedding_weight, position_embed_weight, position_embed_offset,<br>                ragged_wrapper.batch_metadata_buffer(), ragged_wrapper.inflight_seq_descriptors(),<br>                ragged_wrapper.tokens_to_seq(), ragged_wrapper.kv_ptrs())<br>    <span class="hljs-keyword">return</span> embedded_tokens<br></code></pre></td></tr></table></figure>
<hr>
<p>在开始看 C++ 实现之前，我想带大家先回顾一下 embedding 的计算方式，避免被代码绕晕。</p>
<p>NLP 中的 Embedding 原理就是将文本编码为紧凑的高维向量。而为了将原先文本中 token 的 one-hot 编码转换成更紧凑稠密的高维向量，往往需要做一次矩阵乘法：如下图所示例子，利用稠密的浮点向量来表示每个 token，再利用向量间的余弦值来表示词义关系。</p>
<p><img src="/img/LLM/tokenEmbeddingExample.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p>因为注意力机制无法发现词序关系，LLM 中还更多地加入了位置编码，比如 transformer 论文中经典的 sin-cos 位置函数表示词序位置，他们计算后会直接加到 embedding 向量中。</p>
<h3 id="C-实现">C++ 实现</h3>
<p>前面提到，对于 <code>DSRaggedEmbedding</code> 中维护的 embedding 相关的张量，他们在主机上也相应地维护了一个 shadow 张量。这些 shadow 张量是在构造 ragged embedding 时直接填充的。在条件允许的前提下，应分配 shadow 张量，以方便张量快速复制到 GPU 上。因此，我们需要尽可能地降低主机到 GPU 的数据传输延迟，来看看 DeepSpeed 是如何实现的：</p>
<h4 id="快速分配主机内存">快速分配主机内存</h4>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span>* <span class="hljs-title">get_cuda_fast_buffer</span><span class="hljs-params">(<span class="hljs-type">int64_t</span> size)</span> </span>&#123;<br>    <span class="hljs-type">void</span>* buffer_ptr;<br>    <span class="hljs-comment">// Host allocation flags that should minimize the host -&gt; accelerator copy latency</span><br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> alloc_flags =<br>        cudaHostAllocPortable | cudaHostAllocMapped | cudaHostAllocWriteCombined;<br>    <span class="hljs-built_in">cudaHostAlloc</span>(&amp;buffer_ptr, size, alloc_flags);<br>    <span class="hljs-keyword">return</span> buffer_ptr;<br>&#125;<br></code></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li><code>cudaHostAllocPortable</code> 该 flag 要求返回的主机内存将被<strong>所有CUDA context</strong>视为 pinned memory，而不仅仅执行了分配内存的那个 context。这里与 <code>cudaMallocHost()</code> 有点区别，<a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/cudamallochost-vs-cudahostalloc-cudahostallocportable/30274">参看 NVIDIA 社区解答</a></li>
<li><code>cudaHostAllocMapped</code> 该 flag 要求分配的空间必须映射到 CUDA 地址空间中，并且设备内存指针可以被 <code>cudaHostGetDevicePointer()</code> 来调用得到。</li>
<li><code>cudaHostAllocWriteCombined</code> 该 flag 要求分配的内存必须执行组合写(Write-Combined)策略。在某些系统配置上，WC 内存可以通过 PCI Express 总线更快地传输，但大多数 CPU 无法有效地读取。对于那些 CPU 写入，设备通过 pinned memory 或主机-&gt;设备传输读出的操作来说，WC 内存是一个很好的选择。</li>
</ul>
<h4 id="ragged-embed">ragged_embed</h4>
<p>接下来我们来看被封装的 <code>ragged_embed</code> C++ 代码。参考上文给出的 Embedding 原理示例，可以很清楚地明白各个变量的含义。在使用了 <code>DISPATCH_FOR_FLOAT</code> 和 <code>DISPATCH_FOR_INT</code> 这两个宏做变量类型的定义后，CUDA 代码被正式启动了。</p>
<p><img src="/img/LLM/ragged_embed_code.png" srcset="/img/loading.gif" lazyload alt="ragged_embed 代码"></p>
<h4 id="ragged-embed-CUDA-kernel">ragged embed CUDA kernel</h4>
<p>使用 CUDA 加速程序计算，首先要明确如何安排我们手中的大量线程。从之前的 Embedding 示例中，敏感的读者已经注意到，只需要按照 <code>input_ids</code> 内值读取 <code>embedding_weight</code> 的那一行就行了，不需要真的去计算矩阵乘。因此事实上 CUDA 程序需要注意的根本不是计算，而是读取和写入。</p>
<p>DeepSpeed 是这样安排他们的线程的。首先，让每个线程负责 load <code>embedding_weight</code> 矩阵中一行的一小块颗粒（红圈部分），这一颗粒大小被定义为 <code>embed::granularity = 16</code> 个字节。而一个线程块内有 512 个线程，因此一个线程块可以 load 一共 512 * 16 个字节的数据。对应的，一个 token 的向量维度有 <code>embed_dim</code>，所以需要 <code>parallel_blocks</code> 个线程块 load。这组成了线程块排布的 x 方向，一行 x 方向的线程块完成一个 token 的向量读取，而 y 方向的线程块数量对应着 token 的总数。</p>
<p><img src="/img/LLM/launch_ragged_embed_kernel.png" srcset="/img/loading.gif" lazyload alt="launch_ragged_embed_kernel"></p>
<p><img src="/img/LLM/ragged_embed_kernel.png" srcset="/img/loading.gif" lazyload alt="ragged_embed_kernel"></p>
<p>用之前的 Embedding 举例，要处理 “I have four tokens” 这句话，首先找到了这句话对应 token 的行号，填入 <code>input_ids</code> 为 [3, 1, 4, 2]。CUDA 程序执行时，会产生四行 x 方向的线程块，其中第一行的所有线程块负责从 <code>embedding_weight</code> 中装载第 3 行（对应 <code>token_value</code>）的所有数据，读入到输出的 <code>embedded_tokens</code> 矩阵中的第 1 行（对应 <code>token_idx</code>）。</p>
<h4 id="数据读取与写入">数据读取与写入</h4>
<p>现在我们来看一下该 kernel 的性能情况。由于整个 kernel 几乎没有运算，因此主要的瓶颈必然发生在读写数据上，为了尽可能快地将数据读入和写出，DeepSpeed 安排 warp 内的线程读取了连续的 32 * 16 = 512 个字节的数据。整数倍的内存读取容易实现 cacheline 的对齐。</p>
<p>而对于下面这个 API 则大有学问。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// usage</span><br>mem_access::<span class="hljs-built_in">load_global</span>&lt;embed::granularity&gt;(reg_buf, embedding_row + channel_offset)<br><br><span class="hljs-comment">// declaration</span><br><span class="hljs-keyword">template</span> &lt;<span class="hljs-type">int</span> AccessSize, LoadPolicy policy = LoadPolicy::CacheAll&gt;<br>__device__ __forceinline__ <span class="hljs-type">void</span> <span class="hljs-built_in">load_global</span>(<span class="hljs-type">void</span>* dst, <span class="hljs-type">const</span> <span class="hljs-type">void</span>* src);<br><br><span class="hljs-comment">// definition</span><br><span class="hljs-keyword">template</span> &lt;&gt;<br>__device__ __forceinline__ <span class="hljs-type">void</span> <span class="hljs-built_in">load_global</span>&lt;<span class="hljs-number">16</span>&gt;(<span class="hljs-type">void</span>* dst, <span class="hljs-type">const</span> <span class="hljs-type">void</span>* src)<br>&#123;<br>    uint4* data = <span class="hljs-built_in">reinterpret_cast</span>&lt;uint4*&gt;(dst);<br><span class="hljs-meta">#<span class="hljs-keyword">ifdef</span> PTX_AVAILABLE</span><br>    <span class="hljs-function"><span class="hljs-keyword">asm</span> <span class="hljs-title">volatile</span><span class="hljs-params">(<span class="hljs-string">&quot;ld.global.ca.v4.u32 &#123;%0, %1, %2, %3&#125;, [%4];\n&quot;</span></span></span><br><span class="hljs-params"><span class="hljs-function">                 : <span class="hljs-string">&quot;=r&quot;</span>(data[<span class="hljs-number">0</span>].x), <span class="hljs-string">&quot;=r&quot;</span>(data[<span class="hljs-number">0</span>].y), <span class="hljs-string">&quot;=r&quot;</span>(data[<span class="hljs-number">0</span>].z), <span class="hljs-string">&quot;=r&quot;</span>(data[<span class="hljs-number">0</span>].w)</span></span><br><span class="hljs-params"><span class="hljs-function">                 : <span class="hljs-string">&quot;l&quot;</span>(src))</span></span>;<br><span class="hljs-meta">#<span class="hljs-keyword">else</span></span><br>    <span class="hljs-type">const</span> uint4* src_cast = <span class="hljs-built_in">reinterpret_cast</span>&lt;<span class="hljs-type">const</span> uint4*&gt;(src);<br>    data[<span class="hljs-number">0</span>] = src_cast[<span class="hljs-number">0</span>];<br><span class="hljs-meta">#<span class="hljs-keyword">endif</span></span><br>&#125;<br><br><span class="hljs-comment">// the uint4 definition</span><br><span class="hljs-keyword">struct</span> <span class="hljs-title class_">__device_builtin__</span> __builtin_align__(<span class="hljs-number">16</span>) uint4<br>&#123;<br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> x, y, z, w;<br>&#125;;<br></code></pre></td></tr></table></figure>
<p><code>uint4</code> 表示 4 个 <code>unsigned int</code> 变量按照 16 个字节对齐的方式排布在内存中，总共有 16 个字节大小，正好对应了之前 embed 移动的颗粒度，因此 <code>load_global</code> 在实现时直接使用了 <code>uint4</code> 进行数据搬运。而对应到 PTX 汇编代码，有若干不同的模板实例化来实现：</p>
<ul>
<li>ld.global.ca.v4.u32 Cache at all levels 所有缓存上都过一遍</li>
<li>ld.global.cg.v4.u32 CacheGlobal  Cache at L2 only 仅在 L2 缓存上存放</li>
<li>ld.global.cs.v4.u32 CacheStreaming Cache with evict first policy 使用优先驱逐的方式缓存。即先后读入的数据入流水一般从缓存走过，但片叶不沾身，尽量不影响 cache 其他的数据</li>
</ul>
<h2 id="线性层计算">线性层计算</h2>
<p>看完 embedding 层的计算后，我们顺着数据流动的方向，来看一下 QKV 需要经过的线性层计算。说到底，线性层计算其实就是简单的矩阵相乘，那么在 CUDA 平台上，调用 cublas 接口完成矩阵乘无疑是最方便也是最高效的选择了 😃。</p>
<h3 id="BlasFPLinear">BlasFPLinear</h3>
<p>DeepSpeed 中选择使用 <code>BlasFPLinear</code> 类完成 cublas 接口的调用和封装。该类涉及到其他的三个类：<code>CUDAGatedActivation</code> <code>CUDABiasActivation</code> 和 <code>BlasLibLinear</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BlasFPLinear</span>(<span class="hljs-title class_ inherited__">DSLinearBase</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Linear DSModule based on BLAS library and standalone bias + activation kernel implementation.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config: DSLinearConfig, implementation_config: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">super</span>().__init__(config, implementation_config)<br>        self._linear_impl = BlasLibLinear(self._config.input_dtype)<br>        <span class="hljs-keyword">if</span> is_gated(config.activation):<br>            self._is_gated = <span class="hljs-literal">True</span><br>            self._act_fn = CUDAGatedActivation(config.out_channels, config.output_dtype, config.activation)<br>            self._double_buffer = torch.empty((config.max_tokens, config.out_channels * <span class="hljs-number">2</span>),<br>                                              dtype=config.output_dtype,<br>                                              device=get_accelerator().current_device())<br>        <span class="hljs-keyword">else</span>:<br>            self._is_gated = <span class="hljs-literal">False</span><br>            self._act_fn = CUDABiasActivation(config.out_channels, config.output_dtype, config.activation)<br>        self._output = torch.empty((config.max_tokens, config.out_channels),<br>                                   dtype=config.output_dtype,<br>                                   device=get_accelerator().current_device())<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, hidden_states: torch.Tensor, w: torch.Tensor, b: <span class="hljs-type">Optional</span>[torch.Tensor] = <span class="hljs-literal">None</span></span>) -&gt; torch.Tensor:<br>        output = empty_from(self._output, (hidden_states.shape[<span class="hljs-number">0</span>], self._config.out_channels))<br>        <span class="hljs-keyword">if</span> self._is_gated:<br>            staging_output = empty_from(self._double_buffer, (hidden_states.shape[<span class="hljs-number">0</span>], self._config.out_channels * <span class="hljs-number">2</span>))<br>            self._linear_impl(staging_output, hidden_states, w)<br>            self._act_fn(output, staging_output, b)<br>        <span class="hljs-keyword">else</span>:<br>            self._linear_impl(output, hidden_states, w)<br>            self._act_fn(output, b)<br>        <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure>
<p>先从最简单的 <code>BlasLibLinear</code> 开始吧，它调用了 C++ 函数 <code>void blas_linear(at::Tensor&amp; output, at::Tensor&amp; hidden_states, at::Tensor&amp; weights)</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BlasLibLinear</span>(<span class="hljs-title class_ inherited__">DSKernelBase</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, fp_dtype: DtypeEnum</span>):<br>        self.inf_module = InferenceCoreBuilder().load()<br>        self.inf_module.create_handle()<br>        self.kernel = self.inf_module.blas_linear<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, output: torch.Tensor, hidden_states: torch.Tensor, weights: torch.Tensor</span>) -&gt; torch.Tensor:<br>        self.kernel(output, hidden_states, weights)<br>        <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure>
<p>然后是<code>CUDABiasActivation</code>，它调用了 C++ 函数 <code>bias_activation</code>；<code>CUDAGatedActivation</code> 调用了 <code>ds_gated_activation</code>。</p>
<h4 id="C-实现-2">C++ 实现</h4>
<p>因为 cublas 中做矩阵运算时，通常会采用列主序（column-major）的方式，而 python C++ 中我们已经习惯使用了行主序的方式排布矩阵，因此在运算前需要对矩阵做一定的转换，因此矩阵的维度在这里会有点混乱，让我们好好捋一捋。</p>
<p>首先输入的 hidden_states 的矩阵 H 维度是 (num_tokens, hidden_size)，那么 weights 矩阵的维度就应该是 (head_dims * head_size, hidden_size)，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>×</mo><msup><mi>W</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">H\times W^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span> 就是结果。<strong>但由于 cublas 采用了列主序，因此我们传入的矩阵给 cublas 使用时，实际上输入的都是这个矩阵的转置！当然，得到的结果也是这个矩阵的转置</strong>。</p>
<blockquote>
<p>以下是推导过程，设 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">H_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0813em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是 cublas 看到的矩阵H，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">W_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是 cublas 看到的矩阵weights，那么有</p>
<p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>H</mi><mi>c</mi></msub><mo>=</mo><msup><mi>H</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">H_c=H^T
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0813em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8913em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span></p>
<p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>W</mi><mi>c</mi></msub><mo>=</mo><msup><mi>W</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">W_c=W^T
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8913em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>但因为得到的结果也是要转置的。于是</p>
<p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>H</mi><mo>×</mo><msup><mi>W</mi><mi>T</mi></msup><mo>=</mo><mo stretchy="false">(</mo><msubsup><mi>H</mi><mi>c</mi><mi>T</mi></msubsup><mo>×</mo><msub><mi>W</mi><mi>c</mi></msub><msup><mo stretchy="false">)</mo><mi>T</mi></msup><mo>=</mo><msubsup><mi>W</mi><mi>c</mi><mi>T</mi></msubsup><mo>×</mo><msub><mi>H</mi><mi>c</mi></msub><mo>=</mo><mi>W</mi><mo>×</mo><msup><mi>H</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">H\times W^T=(H_c^T\times W_c)^T=W_c^T\times H_c=W\times H^T
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8913em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1413em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:-0.0813em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1413em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1383em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0813em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8913em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span></p>
</blockquote>
<p>于是，输入到 cublas 计算 gemm 的公式应当变成 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>×</mo><msup><mi>H</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">W\times H^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>。这就解释了下图中的红框部分计算 m n 和 k 的代码。又因为实际 cublas 是要计算 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>W</mi><mi>c</mi><mi>T</mi></msubsup><mo>×</mo><msub><mi>H</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">W_c^T\times H_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0883em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0813em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，因此 <code>trans_a</code> 为 TRUE，而 <code>trans_b</code> 为 FALSE。随后针对 cublas API 调用就比较平凡了，不说了。</p>
<p><img src="/img/LLM/blas_linear.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p>再来看看激活函数的实现。BlasActivation 支持 GELU、RELU、SILU 和 identity 类型的激活函数。同样地，使用 CUDA 编程首要解决的就是如何安排 CUDA 线程组。这里，DeepSpeed 工程师为了避免一个线程做的活太少，让每个线程 unroll 4 次，一次 load 16 个字节，而一个线程块 512 个线程，因此一共可计算 512 * 4 * 16 / sizeof(T) 个元素。顺带我不得不吐槽一下 DeepSpeed 的代码，这么写应该是怕自己和别人读懂吧。</p>
<p><img src="/img/LLM/blas_activation.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p>然后我们来看看 CUDA 核函数本身的实现。代码读到这里，其实已经有一点规律可循了。</p>
<p>当核函数内部的计算较少甚至没有计算时，通常瓶颈都会在访存这。此时最好将内部的元素做条纹打包，让每个线程一次循环处理一个条纹的数据，而在循环内，需要将数据缓存在寄存器中，减少访存次数；</p>
<p>在对待线程相关的偏移量需要特别小心，多声明几个变量理清思路才是关键，将偏移量的颗粒度从大到小地考量会比较容易。</p>
<p>比如需要先考虑到一个线程块会计算多少元素，再考虑一次 unroll 循环有多少元素，再考虑 512 个线程中，每个线程占了多少元素，三者相加就是真正的偏移量。</p>
<p><img src="/img/LLM/blas_activation_kernel.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p>至于 GatedActivation 的实现，其实与 BlasActivation 差别不大，唯一的区别在于，他会算出一个 2 倍大的 output_feature，以便存放 gate 值和 activation 值，然后在 CUDA 核函数中，再使用 gate 函数计算，并写回最终结果即可。</p>
<h2 id="注意力计算">注意力计算</h2>
<p>关于 DeepSpeed Inference V2 注意力计算的部分，微软将其实现另开了一个库：<a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepSpeed-Kernels/">https://github.com/microsoft/DeepSpeed-Kernels/</a>，并在我们编译 DeepSpeed-MII 时将 .a 文件静态链接进来，考虑到本篇博客篇幅已经很长，因此我选择下篇再来讲讲 DeepSpeed-MII 的注意力机制实现！</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/AI/" class="category-chain-item">AI</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/AI/" class="print-no-link">#AI</a>
      
        <a href="/tags/DeepSpeed/" class="print-no-link">#DeepSpeed</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>深入探索 DeepSpeed（三）</div>
      <div>https://dingfen.github.io/2024/06/12/2024-6-12-deepspeed/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Bill Ding</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年6月12日</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>更新于</div>
          <div>2025年1月26日</div>
        </div>
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="SA - 相同方式共享">
                    <i class="iconfont icon-sa"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/07/21/2024-7-21-ai_assist/" title="在基于 hexo 框架的博客上部署定制化 AI 聊天应用">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">在基于 hexo 框架的博客上部署定制化 AI 聊天应用</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/05/29/2024-5-29-LLM-perf/" title="大模型性能优化的总结和分享">
                        <span class="hidden-mobile">大模型性能优化的总结和分享</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  








    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  var relativeDate = function() {
    var updatedTime = document.getElementById('updated-time');
    if (updatedTime) {
      var text = updatedTime.textContent;
      var reg = /\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:Z|[+-]\d{2}:\d{2})/;
      var matchs = text.match(reg);
      if (matchs) {
        var relativeTime = moment(matchs[0]).fromNow();
        updatedTime.textContent = text.replace(reg, relativeTime);
      }
      updatedTime.style.display = '';
    }
  };
  Fluid.utils.createScript('https://lib.baomitu.com/moment.js/2.29.4/moment.min.js', function() {
    if (!'zh-cn'.startsWith('en')) {
      Fluid.utils.createScript('https://lib.baomitu.com/moment.js/2.29.4/locale/zh-cn.min.js', function() {
        relativeDate();
      });
    } else {
      relativeDate();
    }
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  
<script src="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/Ribbon.min.js"></script>



<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<!-- hexo injector body_end start -->
  <link rel="stylesheet" crossorigin href="https://g.alicdn.com/aliyun-documentation/web-chatbot-ui/0.0.11/index.css" />
  <script type="module" crossorigin src="https://g.alicdn.com/aliyun-documentation/web-chatbot-ui/0.0.11/index.js"></script>
  <script>
    window.CHATBOT_CONFIG = {
      endpoint: "https://web-chatbot-syz-knthhrjfeq.cn-hangzhou.fcapp.run/chat", // 可以替换为 https://{your-fc-http-trigger-domain}/chat
      displayByDefault: false, // 默认不显示 AI 助手对话框
      aiChatOptions: { // 自定义取值参考：https://docs.nlkit.com/nlux/reference/ui/ai-chat#conversation-options
        conversationOptions: { // 自定义取值参考：https://docs.nlkit.com/nlux/reference/ui/ai-chat#conversation-options
          conversationStarters: [
            {prompt: '请问你是谁，能为我做什么？'},
            {prompt: '请介绍一下博客的主人'}
          ],
          layout: 'bubbles'
        },
        displayOptions: { // 自定义取值参考：https://docs.nlkit.com/nlux/reference/ui/ai-chat#display-options
          height: 550,
          // width: 400,
        },
        personaOptions: { // 自定义取值参考：https://docs.nlkit.com/nlux/reference/ui/ai-chat#chat-personas
          assistant: {
            name: '你好，我是本网站的 AI 助手',
            // AI 助手的图标
            avatar: 'https://img.alicdn.com/imgextra/i2/O1CN01Pda9nq1YDV0mnZ31H_!!6000000003025-54-tps-120-120.apng',
            tagline: '输入您的问题，我会尽力帮你解答！',
          }
        }
      }
    };
  </script>
  <style>
    :root {
      /* webchat 工具栏的颜色 */
      --webchat-toolbar-background-color: #1464E4;
      /* webchat 工具栏文字和按钮的颜色 */
      --webchat-toolbar-text-color: #FFF;
    }
    /* webchat 对话框如果被遮挡，可以尝试通过 z-index、bottom、right 等设置 来调整*/
    .webchat-container {
      z-index: 100;
      bottom: 10px;
      right: 10px;
    }
    /* webchat 的唤起按钮如果被遮挡，可以尝试通过 z-index、bottom、right 等设置 来调整。也可以通过 CSS 进一步定制唤起按钮的形状、大小等。 */
    .webchat-bubble-tip {
      z-index: 99;
      bottom: 20px;
      right: 20px;
    }
  </style>
  <!-- hexo injector body_end end --></body>
</html>
