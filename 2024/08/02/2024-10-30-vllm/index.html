

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Bill Ding">
  <meta name="keywords" content="">
  
    <meta name="description" content="vllm custom allreduce å®ç° åŠ¨æœº ç”¨è¿‡ vllm æ‰§è¡Œå¤§æ¨¡å‹çš„è¯»è€…åº”è¯¥å¾ˆæ¸…æ¥šï¼Œ vllm ä½¿ç”¨å¼ é‡å¹¶è¡Œï¼ˆtensor parallelï¼‰çš„æ–¹å¼æ‰§è¡Œå¤šå¡æ¨ç†ã€‚åœ¨ Self-Attention å’Œ MLP å±‚ä¼šå°†å¼ é‡åˆ†å¸ƒåˆ°å„ä¸ª GPU worker ä¸Šè®¡ç®—ï¼Œå› æ­¤å„ä¸ª GPU ä¸Šè®¡ç®—çš„åªæ˜¯ä¸€éƒ¨åˆ†çŸ©é˜µä¹˜çš„æ•°æ®ï¼Œæ•…å®Œæˆè®¡ç®—åæ‰€æœ‰çš„ GPU worker éœ€è¦å°†ç»“æœâ€œæ±‡æ€»â€èµ·æ¥ï¼Œå³æ‰§è¡Œ">
<meta property="og:type" content="article">
<meta property="og:title" content="VLLM custom allreduce å®ç°">
<meta property="og:url" content="https://dingfen.github.io/2024/08/02/2024-10-30-vllm/index.html">
<meta property="og:site_name" content="å³°å­çš„ä¹å›­">
<meta property="og:description" content="vllm custom allreduce å®ç° åŠ¨æœº ç”¨è¿‡ vllm æ‰§è¡Œå¤§æ¨¡å‹çš„è¯»è€…åº”è¯¥å¾ˆæ¸…æ¥šï¼Œ vllm ä½¿ç”¨å¼ é‡å¹¶è¡Œï¼ˆtensor parallelï¼‰çš„æ–¹å¼æ‰§è¡Œå¤šå¡æ¨ç†ã€‚åœ¨ Self-Attention å’Œ MLP å±‚ä¼šå°†å¼ é‡åˆ†å¸ƒåˆ°å„ä¸ª GPU worker ä¸Šè®¡ç®—ï¼Œå› æ­¤å„ä¸ª GPU ä¸Šè®¡ç®—çš„åªæ˜¯ä¸€éƒ¨åˆ†çŸ©é˜µä¹˜çš„æ•°æ®ï¼Œæ•…å®Œæˆè®¡ç®—åæ‰€æœ‰çš„ GPU worker éœ€è¦å°†ç»“æœâ€œæ±‡æ€»â€èµ·æ¥ï¼Œå³æ‰§è¡Œ">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://docs.vllm.ai/en/latest/_images/vllm-logo-text-light.png">
<meta property="article:published_time" content="2024-08-02T13:10:00.000Z">
<meta property="article:modified_time" content="2025-01-26T11:49:09.209Z">
<meta property="article:author" content="Bill Ding">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://docs.vllm.ai/en/latest/_images/vllm-logo-text-light.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>VLLM custom allreduce å®ç° - å³°å­çš„ä¹å›­</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/KaTeX/0.16.2/katex.min.css" />



<!-- ä¸»é¢˜ä¾èµ–çš„å›¾æ ‡åº“ï¼Œä¸è¦è‡ªè¡Œä¿®æ”¹ -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"dingfen.github.io","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"|","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":null,"onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.1.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>å³°å­çš„ä¹å›­</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>é¦–é¡µ</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>å½’æ¡£</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>åˆ†ç±»</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>æ ‡ç­¾</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>å…³äº</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-light.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="VLLM custom allreduce å®ç°"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-08-02 21:10" pubdate>
          2024å¹´8æœˆ2æ—¥ æ™šä¸Š
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          4.1k å­—
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          35 åˆ†é’Ÿ
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">VLLM custom allreduce å®ç°</h1>
            
              <p id="updated-time" class="note note-info" style="display: none">
                
                  
                    <!-- compatible with older versions-->
                    æ›´æ–°äºï¼š2025-01-26T19:49:09+08:00
                  
                  

                
              </p>
            
            
              <div class="markdown-body">
                
                <h1>vllm custom allreduce å®ç°</h1>
<h2 id="åŠ¨æœº">åŠ¨æœº</h2>
<p>ç”¨è¿‡ vllm æ‰§è¡Œå¤§æ¨¡å‹çš„è¯»è€…åº”è¯¥å¾ˆæ¸…æ¥šï¼Œ vllm ä½¿ç”¨å¼ é‡å¹¶è¡Œï¼ˆtensor parallelï¼‰çš„æ–¹å¼æ‰§è¡Œå¤šå¡æ¨ç†ã€‚åœ¨ Self-Attention å’Œ MLP å±‚ä¼šå°†å¼ é‡åˆ†å¸ƒåˆ°å„ä¸ª GPU worker ä¸Šè®¡ç®—ï¼Œå› æ­¤å„ä¸ª GPU ä¸Šè®¡ç®—çš„åªæ˜¯ä¸€éƒ¨åˆ†çŸ©é˜µä¹˜çš„æ•°æ®ï¼Œæ•…å®Œæˆè®¡ç®—åæ‰€æœ‰çš„ GPU worker éœ€è¦å°†ç»“æœâ€œæ±‡æ€»â€èµ·æ¥ï¼Œå³æ‰§è¡Œ allreduceï¼Œæ‰èƒ½è·å¾—æ‰€æœ‰çš„ç»“æœï¼Œè¿›è€Œå¼€å§‹åç»­çš„æ“ä½œï¼ˆæ¯”å¦‚ dropout æˆ–è€… layernormï¼‰ã€‚</p>
<p><img src="https://qiankunli.github.io/public/upload/machine/gpu_all_reduce.png" srcset="/img/loading.gif" lazyload alt="allreduce ç¤ºä¾‹"></p>
<p>ä¹Ÿå°±æ˜¯è¯´ï¼Œæ‰§è¡Œä¸€æ¬¡æœ‰ N å±‚ LLM çš„æ¨ç†ï¼Œå¤šä¸ª GPU worker é—´å°±éœ€è¦æ‰§è¡Œè‡³å°‘ <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>Ã—</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">2\times N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> æ¬¡ allreduceã€‚è€ƒè™‘åˆ° N é€šå¸¸ä¸º 32-128 ä¹‹é—´ï¼Œä¸”æ‰§è¡Œ allreduce ä¼šé˜»å¡åç»­æ¨¡å‹çš„æ¨ç†è¿›åº¦ï¼Œå› æ­¤ allreduce æ€§èƒ½ä¼šç›´æ¥å½±å“ vllm å¤šå¡æ¨ç†çš„æ•ˆç‡ã€‚</p>
<p>è€Œåœ¨ decoding é˜¶æ®µï¼Œå¯¹äºä¸€ä¸ª sequence æ¥è¯´ï¼Œvllm çš„ä¸€æ¬¡æ¨ç†åªä¼šæ¨å‡ºä¸€ä¸ª tokenï¼Œå› æ­¤ decoding é˜¶æ®µçš„ allreduce çš„é€šä¿¡çš„æ•°æ®é‡éå¸¸å°ã€‚æˆ‘ä»¬ä»¥ llama2-70b æ¨¡å‹åœ¨æ¨ç† batch size ä¸º 32 æ—¶çš„åœºæ™¯ä¸ºä¾‹ï¼Œå…¶decoding é˜¶æ®µéœ€è¦çš„ allreduce é€šä¿¡æ•°æ®é‡ä»…ä¸º <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>32</mn><mo>Ã—</mo><mn>8192</mn><mo>Ã—</mo><mn>2</mn><mo>=</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">32\times 8192 \times 2=512</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">32</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">8192</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">512</span></span></span></span> KBã€‚å¯è§å³ä½¿åœ¨ 70B å¤§æ¨¡å‹ä¸Šè¿è¡Œè¾ƒå¤§çš„ batch size æ‰€å¸¦æ¥çš„é€šä¿¡é‡ä¹Ÿæ˜¯éå¸¸å°‘çš„ã€‚</p>
<p>åœ¨æ²¡æœ‰ vllm custom allreduce æ—¶ï¼Œæˆ‘ä»¬ä¼šç›´æ¥ä½¿ç”¨ nvidia GPU çš„ NCCL é€šä¿¡åº“æ¥å®Œæˆ allreduceã€‚ä½†æ˜¯ï¼Œå¯¹äºä¸Šè¿°å° size çš„ allreduce åœºæ™¯ï¼ŒNCCL å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š</p>
<ol>
<li>å¤š stageï¼Œä¸æ˜¯å»¶è¿Ÿæœ€ä¼˜çš„ã€‚NCCL å®ç°çš„å¸¦å®½æœ€ä¼˜çš„æ ‘æˆ–ç¯çŠ¶ allreduceï¼ˆå…·ä½“å®ç°å¯ä»¥å‚è€ƒ <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/K8l7H2zCUr9sGzYehizFMA">Ring Allreduce</a>ï¼Œå®ƒä»¬åˆ†åˆ«å…·æœ‰ O(logn) å’Œ O(n) ä¸ªé˜¶æ®µä¼ è¾“è¿‡ç¨‹ï¼ˆn ä¸º GPU æ•°ï¼‰ã€‚è€ƒè™‘åˆ°ç°ä»£ nvidia GPU é—´çš„å·¨å¤§å¸¦å®½ï¼ˆA100 çš„æœ‰æ•ˆåŒå‘å¸¦å®½æœ‰ 480 GB/s ï¼ï¼‰ï¼Œ NCCL å®ç°çš„ allreduce æ˜¾ç„¶æ›´é€‚åˆå¤§æ•°æ®ä¼ è¾“çš„åœºæ™¯ï¼Œå¯¹äºå° size åœºæ™¯ï¼Œæˆ‘ä»¬æ›´å¸Œæœ›å…¶å»¶è¿Ÿä¸­çš„å¯åŠ¨ä¼ è¾“ï¼ˆæˆ–åŒæ­¥ï¼‰æ—¶é—´æ›´å°‘ï¼Œè€Œä¸å¿…æ‹…å¿ƒæ•°æ®çœŸæ­£çš„ä¼ è¾“æ—¶é—´å¤ªé•¿ã€‚</li>
<li>ä¸åˆ©äºå†…æ ¸èåˆã€‚NCCL å¯¹äº vllm å¼€å‘äººå‘˜æ¥è¯´æ˜¯é»‘ç›’ï¼Œå¾ˆéš¾è¢«è¿›ä¸€æ­¥èåˆä¼˜åŒ–ã€‚è€Œå¦‚æœ vllm ä½¿ç”¨è‡ªå·±çš„å†…æ ¸ï¼Œé‚£ä¹ˆå°±èƒ½æ›´è½»æ¾åœ°åšç®—å­èåˆæ“ä½œ</li>
<li>CUDA graph ä¸å‹å¥½ã€‚NCCL çš„ cuda graph éœ€è¦æ’å…¥åŒæ­¥ä¸»æœºçš„èŠ‚ç‚¹ï¼Œè¿™ä¼šé˜»å¡ GPUï¼Œå¯¼è‡´ GPU æµå‡ºç°é—´éš™ï¼š</li>
</ol>
<p><img src="/img/LLM/nccl_cuda.png" srcset="/img/loading.gif" lazyload alt=""></p>
<h2 id="æœºåˆ¶">æœºåˆ¶</h2>
<h3 id="ç¼“å†²åŒºæ³¨å†Œ">ç¼“å†²åŒºæ³¨å†Œ</h3>
<p>CUDA IPC (Interprocess Communication) æ”¯æŒæ¯ä¸ª GPU èŠ‚ç‚¹æ‹¥æœ‰ä¸€ä¸ªæŒ‡å‘å…¶ä»– GPU èŠ‚ç‚¹å†…å­˜çš„æŒ‡é’ˆã€‚æ˜¾ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™äº›æŒ‡é’ˆæ¥å®Œæˆ allreduce æ“ä½œã€‚å…·ä½“æ­¥éª¤æ˜¯ï¼Œé¦–å…ˆåœ¨åˆå§‹åŒ–çš„è¿‡ç¨‹ä¸­å°±å…ˆå°†æ¯ä¸ªèŠ‚ç‚¹çš„ä¸€ä¸ª buffer æš´éœ²ç»™å…¶ä»–èŠ‚ç‚¹ï¼Œç»„æˆä¸€ä¸ª IPC handleï¼Œç„¶ååœ¨åš allreduce æ—¶ï¼ŒèŠ‚ç‚¹åªéœ€è¦ä»å…¶ä»–æ‰€æœ‰èŠ‚ç‚¹çš„ buffer ä¸­è¯»å–æ•°æ®å³å¯ã€‚</p>
<h3 id="one-shot-allreduce">one-shot allreduce</h3>
<p>allreduce æœ‰éå¸¸å¤šç®—æ³•ã€‚åœ¨å° size åœºæ™¯ä¸‹ï¼Œä¸ºå°½å¯èƒ½å‡å°‘ GPU åŒæ­¥å’Œæ”¶å‘æ—¶é—´ï¼Œæˆ‘ä»¬å½“ç„¶å¸Œæœ›ç›´æ¥äº†å½“ä¸€äº›ï¼šç›´æ¥è®©æ‰€æœ‰èŠ‚ç‚¹çš„æ•°æ®åŒæ—¶å¹¿æ’­ç»™å…¶ä»–èŠ‚ç‚¹ã€‚è¿™å°±æ˜¯ one-shot allreduce</p>
<p><img src="/img/LLM/one_shot_allreduce.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p>one-shot allreduce çš„æ€§èƒ½å…³é”®è®¾è®¡ä¸€ä¸ªè‡ªå®šä¹‰å¯¹é½æ•°æ®ç±»å‹ï¼Œæ–¹ä¾¿è®©æ¯ä¸ªèŠ‚ç‚¹èƒ½éƒ½å¿«é€Ÿåœ°è¯»å– allreduce æ•°æ®ã€‚æœ€å¥½æ˜¯ 128 bits å¯¹é½ï¼Œå› ä¸ºæ¯ä¸ª CUDA çº¿ç¨‹ä¸€æ¬¡ä¼šè¯»å– 16 å­—èŠ‚ï¼Œå¥½è®©ç¼–è¯‘å™¨ç”Ÿæˆ <code>LD.128</code> å’Œ <code>ST.128</code> æŒ‡ä»¤ã€‚</p>
<h3 id="two-hop-allreduce">two-hop allreduce</h3>
<p>åœ¨ç¨å¾®å¤§ä¸€äº›çš„ size æˆ–èŠ‚ç‚¹ç¨å¤šçš„åœºæ™¯ä¸‹ï¼Œç›´æ¥è®©æ‰€æœ‰èŠ‚ç‚¹å¹¿æ’­å°±ä¸å¤ªåˆé€‚äº†ã€‚two-shot allreduce å…ˆæ‰§è¡Œ reduce scatterï¼Œè®©æ¯ä¸ªèŠ‚ç‚¹ä»æ‰€æœ‰èŠ‚ç‚¹é‚£è¯»å–å¯¹åº”çš„ 1/N çš„æ•°æ®ï¼Œç„¶ååŠ èµ·æ¥ã€‚ç„¶åï¼Œåœ¨åšä¸€ä¸ª allgatherï¼Œå°†æ‰€æœ‰èŠ‚ç‚¹çš„æ•°æ®å‘é€ç»™å…¶ä»–èŠ‚ç‚¹ã€‚</p>
<p><img src="/img/LLM/two_shot_allreduce.png" srcset="/img/loading.gif" lazyload alt=""></p>
<h1>ä»£ç è§£è¯»</h1>
<p>æœ¬åšæ–‡æ¶‰åŠçš„ vllm ä»£ç ä¸º 0.6.3ï¼Œè¯·æ³¨æ„æ—¶æ•ˆæ€§ã€‚</p>
<h2 id="python-å¼€å§‹">python å¼€å§‹</h2>
<h3 id="Linear-å±‚åˆ°-allreduce">Linear å±‚åˆ° allreduce</h3>
<p>é¦–å…ˆï¼Œè®©æˆ‘ä»¬å›åˆ°æœ€åˆçš„åœ°æ–¹ï¼Œå½“å¤šå¡ TP æ‰§è¡Œæ¨ç†æ—¶ï¼Œè®¡ç®— MLP down_proj ç­‰ä¼šæ¶‰åŠåˆ°äº†æˆ‘ä»¬ç›®å‰ç ”ç©¶çš„ custom allreduceï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">RowParallelLinear</span>(<span class="hljs-title class_ inherited__">LinearBase</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, input_</span>):<br>        <span class="hljs-comment"># ...</span><br>        bias_ = <span class="hljs-literal">None</span> <span class="hljs-keyword">if</span> (self.tp_rank &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> self.skip_bias_add) <span class="hljs-keyword">else</span> self.bias<br>        output_parallel = self.quant_method.apply(self,<br>                                                  input_parallel,<br>                                                  bias=bias_)<br>        <span class="hljs-keyword">if</span> self.reduce_results <span class="hljs-keyword">and</span> self.tp_size &gt; <span class="hljs-number">1</span>:<br>            output = tensor_model_parallel_all_reduce(output_parallel)<br>        <span class="hljs-keyword">else</span>:<br>            output = output_parallel<br>        <span class="hljs-comment"># ...</span><br>        <span class="hljs-keyword">return</span> output, output_bias<br></code></pre></td></tr></table></figure>
<p>è¯¥å‡½æ•°çš„å®ç°åœ¨ vllm/distributed å†…</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">tensor_model_parallel_all_reduce</span>(<span class="hljs-params">input_: torch.Tensor</span>) -&gt; torch.Tensor:<br>    <span class="hljs-string">&quot;&quot;&quot;All-reduce the input tensor across model parallel group.&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> get_tp_group().all_reduce(input_)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_tp_group</span>() -&gt; GroupCoordinator:<br>    <span class="hljs-keyword">assert</span> _TP <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>, (<span class="hljs-string">&quot;tensor model parallel group is not initialized&quot;</span>)<br>    <span class="hljs-keyword">return</span> _TP<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">all_reduce</span>(<span class="hljs-params">self, input_: torch.Tensor</span>) -&gt; torch.Tensor:<br>    <span class="hljs-comment"># ...</span><br>    <span class="hljs-comment"># use custom allreduce</span><br>    <span class="hljs-keyword">return</span> torch.ops.vllm.outplace_all_reduce(input_, group_name=self.unique_name)<br></code></pre></td></tr></table></figure>
<p>æœ€ç»ˆï¼Œç»è¿‡å‡ æ¬¡è¾—è½¬è°ƒç”¨åï¼Œpython ä»£ç æœ€ç»ˆæ¥å…¥åˆ° <code>CustomAllreduce</code> ç±»çš„åœ°æ–¹å°±æ˜¯ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_all_reduce_out_place</span>(<span class="hljs-params">self, input_: torch.Tensor</span>) -&gt; torch.Tensor:<br>    ca_comm = self.ca_comm<br>    out = ca_comm.custom_all_reduce(input_)<br>    <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure>
<h3 id="allreduce-ä½¿ç”¨å‰æ">allreduce ä½¿ç”¨å‰æ</h3>
<p>ç°åœ¨ï¼Œæˆ‘ä»¬å†æ¥ä»”ç»†çœ‹ <a target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm/blob/v0.6.3/vllm/distributed/device_communicators/custom_all_reduce.py#L46"><code>CustomAllreduce</code></a>ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">CustomAllreduce</span>:<br>    _SUPPORTED_WORLD_SIZES = [<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">8</span>]<br><br>    <span class="hljs-comment"># max_size: max supported allreduce size</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,</span><br><span class="hljs-params">                 group: ProcessGroup,</span><br><span class="hljs-params">                 device: <span class="hljs-type">Union</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">str</span>, torch.device],</span><br><span class="hljs-params">                 max_size=<span class="hljs-number">8192</span> * <span class="hljs-number">1024</span></span>) -&gt; <span class="hljs-literal">None</span>:<br></code></pre></td></tr></table></figure>
<p>å¯ä»¥ç¡®å®šï¼Œ Custom Allreduce ç‰¹æ€§ä»…æ”¯æŒåœ¨ 2ï¼Œ4ï¼Œ6ï¼Œ8 å¡ä¸Šæ¨ç†æ—¶æ‰èƒ½æ‰“å¼€ï¼Œå¹¶ä¸”æœ€å¤§æ”¯æŒçš„ allreduce size ä¸º 8 MBã€‚è¿™é‡Œçš„ allreduce size æŒ‡çš„æ˜¯ MLP åœ¨å„ä¸ª GPU ä¸Šè®¡ç®—å‡ºæ¥çš„å¼ é‡å¤§å°ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">CustomAllreduce</span>:<br>    <span class="hljs-comment"># ...</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">custom_all_reduce</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span>: torch.Tensor</span>) -&gt; <span class="hljs-type">Optional</span>[torch.Tensor]:<br>        <span class="hljs-keyword">if</span> self.disabled <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> self.should_custom_ar(<span class="hljs-built_in">input</span>):<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> self._IS_CAPTURING:<br>            <span class="hljs-keyword">if</span> torch.cuda.is_current_stream_capturing():<br>                <span class="hljs-keyword">return</span> self.all_reduce_reg(<span class="hljs-built_in">input</span>)<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-comment"># if warm up, mimic the allocation pattern</span><br>                <span class="hljs-comment"># since custom allreduce is out-of-place</span><br>                <span class="hljs-keyword">return</span> torch.empty_like(<span class="hljs-built_in">input</span>)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># note: outside of cuda graph context,</span><br>            <span class="hljs-comment"># custom allreduce incurs a cost of cudaMemcpy, which should</span><br>            <span class="hljs-comment"># be small(&lt;=1% of overall latency) compared to the performance</span><br>            <span class="hljs-comment"># gains of using custom kernels</span><br>            <span class="hljs-keyword">return</span> self.all_reduce_unreg(<span class="hljs-built_in">input</span>)<br><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure>
<p>è‹¥æˆ‘ä»¬æ‰“å¼€ custom allreduce ç‰¹æ€§ï¼Œ<code>custom_all_reduce</code> ä¼šå…ˆæ‰§è¡Œ <code>should_custom_ar</code> ï¼Œä¹‹åçš„é€»è¾‘å¯ä»¥åˆ†ä¸ºä¸‰æ¡</p>
<ul>
<li>ä¸€æ¡æ˜¯ <code>self._IS_CAPTURING</code> ä¸º Trueï¼Œä½¿ç”¨ <code>all_reduce_reg</code>ï¼Œåœ¨ CUDA graph capture è¯¥ stream æµè°ƒç”¨</li>
<li>ä¸€æ¡æ˜¯ <code>self._IS_CAPTURING</code> ä¸º Falseï¼Œä½¿ç”¨ <code>all_reduce_unreg</code>ï¼Œåœ¨ä¸æ˜¯ CUDA graph æˆ–è€… CUDA graph æœª capture stream æµè°ƒç”¨</li>
<li>æœ€åæ˜¯ warm up æ—¶çš„è®¡ç®—ï¼Œå¯ä»¥å¿½ç•¥</li>
</ul>
<p>å…ˆæ¥çœ‹ <code>should_custom_ar</code> æ¥ç¡®å®š allreduce çš„ä½¿ç”¨å‰æï¼š</p>
<ul>
<li>ä¸€æ¡æ˜¯ <code>self._IS_CAPTURING</code> ä¸º Falseï¼Œä½¿ç”¨ <code>all_reduce_unreg</code></li>
<li>æœ€åæ˜¯ warm up æ—¶çš„è®¡ç®—ï¼Œå¯ä»¥å¿½ç•¥ã€‚</li>
</ul>
<p>è¿™ä¸¤ä¸ªå‡½æ•° <code>all_reduce</code> å‡½æ•°æˆ‘ä»¬å…ˆæŒ‰ä¸‹ä¸è¡¨ï¼Œæˆ‘ä»¬å…ˆæ¥çœ‹ <code>should_custom_ar</code> æ¥ç¡®å®š allreduce çš„ä½¿ç”¨å‰æï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">should_custom_ar</span>(<span class="hljs-params">self, inp: torch.Tensor</span>):<br>    <span class="hljs-keyword">if</span> self.disabled:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>    inp_size = inp.numel() * inp.element_size()<br>    <span class="hljs-comment"># custom allreduce requires input byte size to be multiples of 16</span><br>    <span class="hljs-keyword">if</span> inp_size % <span class="hljs-number">16</span> != <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> is_weak_contiguous(inp):<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>    <span class="hljs-comment"># for 4 or more non NVLink-capable GPUs, custom allreduce provides</span><br>    <span class="hljs-comment"># little performance improvement over NCCL.</span><br>    <span class="hljs-keyword">if</span> self.world_size == <span class="hljs-number">2</span> <span class="hljs-keyword">or</span> self.full_nvlink:<br>        <span class="hljs-keyword">return</span> inp_size &lt; self.max_size<br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br></code></pre></td></tr></table></figure>
<p>ä¸éš¾å‘ç°ï¼Œ<code>should_custom_ar</code> åœ¨å¦‚ä¸‹æ¡ä»¶ä¼šè¿”å› Falseï¼Œallreduce ä¼ è¾“çš„ tensor å¤§å°ä¸æ˜¯ 16 å¯¹é½çš„ï¼Œæˆ–è€…ä¸æ˜¯å¼±è¿ç»­ï¼ˆè¿ç»­æˆ–æœ‰å‰ç½®åç§»çš„è¿ç»­ï¼‰ï¼Œæˆ–è€…è¿è¡Œç¯å¢ƒä¸­æœ‰å››å¼ ä»¥ä¸Šçš„é NVLink çš„ GPU å¡ã€‚</p>
<p>æ€»ç»“ä¸€ä¸‹ï¼ŒCustom allreduce ç‰¹æ€§åœ¨å¦‚ä¸‹æ¡ä»¶å…¨éƒ¨æ»¡è¶³æ–¹å¯ä½¿ç”¨ï¼š</p>
<ul>
<li>ç”¨æˆ·æ²¡æœ‰æ‰‹åŠ¨ disableï¼Œå³æœªä¼ å…¥ <code>disable_custom_all_reduce=True</code></li>
<li>æœºå™¨ä¸Šæœ‰ 2ï¼Œ4ï¼Œ6ï¼Œ8 GPU å¡ï¼Œä¸”å½“æœ‰å››å¼ åŠä»¥ä¸Šçš„å¡æ—¶ï¼Œä»–ä»¬å¿…é¡»ä½¿ç”¨ NVLink è¿æ¥</li>
<li>allreduce çš„å¼ é‡å¤§å°ä¸è¶…è¿‡ 8 MBï¼Œå¿…é¡» 16 Byte å¯¹é½ï¼Œå¿…é¡»æ»¡è¶³è¿ç»­æ¡ä»¶</li>
</ul>
<h3 id="Python-åˆ°-C">Python åˆ° C++</h3>
<p>ä¹‹å‰æˆ‘ä»¬æåˆ°ï¼Œvllm kernel å†…çš„é€šä¿¡æ•°æ®ï¼Œæ˜¯é€šè¿‡æ¯ä¸ªèŠ‚ç‚¹ä¸Šçš„ CUDA IPC buffer æ¥äº¤æµå®ç°çš„ã€‚æœ¬ç€æ€€ç–‘ä¸»ä¹‰çš„ç²¾ç¥ï¼Œæˆ‘ä»¬æ¥æ·±å…¥è¿½è¸ªä¸€ä¸‹ï¼Œå½“å‰èŠ‚ç‚¹ä¸‹çš„ CUDA è¿›ç¨‹å¦‚ä½•è·å¾—å…¶ä»–èŠ‚ç‚¹çš„ IPC buffer çš„æŒ‡é’ˆçš„ã€‚</p>
<p>å…¶å…³é”®å°±è—åŒ¿äºä¸‹é¢çš„<a target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm/blob/v0.6.3/vllm/distributed/device_communicators/custom_all_reduce.py#L148-L175">ä»£ç </a>ä¸­ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># buffers memory are owned by this Python class and passed to C++</span><br><span class="hljs-comment"># meta data composes of two parts: meta data for synchronization</span><br><span class="hljs-comment"># (256 bytes) and a temporary buffer for storing intermediate</span><br><span class="hljs-comment"># allreduce results.</span><br>self.meta = torch.zeros(ops.meta_size() + max_size,<br>                        dtype=torch.uint8,<br>                        device=self.device)<br><span class="hljs-comment"># This is a pre-registered IPC buffer. In eager mode, input tensors</span><br><span class="hljs-comment"># are first copied into this buffer before allreduce is performed</span><br>self.buffer = torch.empty(max_size,<br>                            dtype=torch.uint8,<br>                            device=self.device)<br><span class="hljs-comment"># This is a buffer for storing the tuples of pointers pointing to</span><br><span class="hljs-comment"># IPC buffers from all ranks. Each registered tuple has size of</span><br><span class="hljs-comment"># 8*world_size bytes where world_size is at most 8. Allocating 8MB</span><br><span class="hljs-comment"># is enough for 131072 such tuples. The largest model I&#x27;ve seen only</span><br><span class="hljs-comment"># needs less than 10000 of registered tuples.</span><br>self.rank_data = torch.empty(<span class="hljs-number">8</span> * <span class="hljs-number">1024</span> * <span class="hljs-number">1024</span>,<br>                                dtype=torch.uint8,<br>                                device=self.device)<br>self.max_size = max_size<br>self.rank = rank<br>self.world_size = world_size<br></code></pre></td></tr></table></figure>
<p>é€ä¸€åœ°è§£é‡Šä¸€ä¸‹è¿™äº›å˜é‡ï¼Œå› ä¸ºå®ƒä¼šåœ¨ä¸‹é¢çš„ç¯‡å¹…ä¸­åå¤å‡ºç°ï¼Œå‡†ç¡®åœ°ç†è§£å®ƒå¯¹æˆ‘ä»¬è¯»æ‡‚ vllm custom allreduce å®ç°è‡³å…³é‡è¦ã€‚</p>
<ul>
<li><code>meta</code> Python ç±» owned çš„ buffersï¼Œå¯ä»¥ç†è§£ä¸ºæ•´ä¸ª Python ç±»çš„æ‰€æœ‰å­—èŠ‚ï¼ŒåŒ…æ‹¬ä¸¤éƒ¨åˆ†ï¼Œç”¨äº GPU é—´æ•°æ®é€šä¿¡çš„ 256 å­—èŠ‚å’Œç”¨äºæš‚å­˜ allreduce æ•°æ®çš„ buffer</li>
<li><code>buffer</code> ç”¨äºåœ¨è¯¥èŠ‚ç‚¹ä¸Šçš„ CUDA IPC çš„æš‚å­˜æ•°æ®çš„ buffer</li>
<li><code>rank_data</code> ç”¨äºæ¥å—æ¥è‡ªå…¶ä»–èŠ‚ç‚¹ CUDA IPC æ•°æ®çš„ buffer</li>
<li><code>rank</code> å½“å‰èŠ‚ç‚¹å·</li>
<li><code>world_size</code> ç›®å‰æœºå™¨ä¸‹çš„ GPU æ€»æ•°</li>
</ul>
<h2 id="C-CustomAllreduce">C++ CustomAllreduce</h2>
<p>åç»­çš„æŒ‡ä»¤åšäº†éå¸¸é‡è¦çš„å‡ ä»¶äº‹ï¼Œæˆ‘ä»¬ä¸€ä¸ªä¸€ä¸ªæ¥çœ‹ï¼š</p>
<h3 id="é€šè¿‡-CPU-å¹¿æ’­æ‰€æœ‰èŠ‚ç‚¹çš„-IPC-handles">é€šè¿‡ CPU å¹¿æ’­æ‰€æœ‰èŠ‚ç‚¹çš„ IPC handles</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">handles, offsets = self._get_ipc_meta(self.meta)<br>self.full_nvlink = full_nvlink<br>self._ptr = ops.init_custom_ar(self.meta, self.rank_data, handles,<br>                                offsets, rank, self.full_nvlink)<br>self.register_buffer(self.buffer)<br></code></pre></td></tr></table></figure>
<p><code>_get_ipc_meta</code> é€šè¿‡åœ¨ CPU ä¸Šè°ƒç”¨ <code>torch.distributed.broadcast_object_list</code> çš„å¹¿æ’­æ‰‹æ®µï¼Œä½¿å¾—</p>
<ul>
<li><code>handles</code> è·å¾—äº†<strong>æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹çš„ ipc handler æŒ‡é’ˆ</strong></li>
<li><code>offsets</code> è·å¾—äº†<strong>ipc buffer ä¸‹æ¥å—æ¥è‡ªå…¶ä»–èŠ‚ç‚¹æ•°æ®çš„ç›®æ ‡ä½ç½®åç§»é‡</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_ipc_meta</span>(<span class="hljs-params">self, inp: torch.Tensor</span>):<br>    data = inp.untyped_storage()._share_cuda_()<br>    shard_data = (<br>        data[<span class="hljs-number">1</span>],  <span class="hljs-comment"># ipc handle to base ptr</span><br>        data[<span class="hljs-number">3</span>],  <span class="hljs-comment"># offset of base ptr</span><br>    )<br>    <span class="hljs-keyword">return</span> self._gather_ipc_meta(shard_data)<br></code></pre></td></tr></table></figure>
<p>ä¸€å¼€å§‹æˆ‘å¾ˆæ˜¯ä¸èƒ½ç†è§£ä¸ºä»€ä¹ˆ data[1] å’Œ data[3] åˆ†åˆ«å¯¹åº” ipc handle å’Œ offset ï¼Œåæ¥æˆ‘å‚è€ƒäº† <a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/blob/main/torch/csrc/StorageSharing.cpp#L353">torch/csrc<br>
/StorageSharing.cpp çš„æºç </a>æ‰æ˜ç™½è¿™å…¶ä¸­çš„å®‰æ’ğŸ¤­ã€‚å› ä¸ºå…¶ä¸­çš„ tuple 1 å°±æ˜¯ <code>cudaIpcMemHandle_t</code> handleï¼Œ3 å°±æ˜¯çœŸæ­£ storage çš„åç§»é‡çš„å­—èŠ‚æ•°ã€‚</p>
<h3 id="åˆ›å»ºä¸åˆå§‹åŒ–-C-CustomAllreduce">åˆ›å»ºä¸åˆå§‹åŒ– C++ CustomAllreduce</h3>
<p><code>ops.init_custom_ar</code> åˆ›å»ºå¹¶åˆå§‹åŒ–äº† C++ <code>CustomAllreduce</code>ã€‚</p>
<p><code>ops.init_custom_ar</code> é€šè¿‡ pytorch çš„ <code>TORCH_LIBRARY_EXPAND</code>  C++ æ‰©å±•ï¼ˆè¿™æ˜¯ç”¨äºè‡ªå®šä¹‰ç®—å­çš„ä¸€ä¸ªå®ï¼‰æ¥è°ƒç”¨ backend çš„ C++ ä»£ç ï¼Œä¹Ÿå°±æ˜¯å¯¹åº”åˆ°äº†ä¸‹é¢çš„ C++ å‡½æ•°ï¼š</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">fptr_t</span> <span class="hljs-title">init_custom_ar</span><span class="hljs-params">(torch::Tensor&amp; meta, torch::Tensor&amp; rank_data,</span></span><br><span class="hljs-params"><span class="hljs-function">                      <span class="hljs-type">const</span> std::vector&lt;std::string&gt;&amp; handles,</span></span><br><span class="hljs-params"><span class="hljs-function">                      <span class="hljs-type">const</span> std::vector&lt;<span class="hljs-type">int64_t</span>&gt;&amp; offsets, <span class="hljs-type">int64_t</span> rank,</span></span><br><span class="hljs-params"><span class="hljs-function">                      <span class="hljs-type">bool</span> full_nvlink)</span> </span>&#123;<br>  <span class="hljs-type">int</span> world_size = offsets.<span class="hljs-built_in">size</span>();<br>  cudaIpcMemHandle_t ipc_handles[<span class="hljs-number">8</span>];<br>  <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; world_size; i++) &#123;<br>    std::<span class="hljs-built_in">memcpy</span>(&amp;ipc_handles[i], handles[i].<span class="hljs-built_in">data</span>(), <span class="hljs-built_in">sizeof</span>(cudaIpcMemHandle_t));<br>  &#125;<br>  <span class="hljs-keyword">return</span> (<span class="hljs-type">fptr_t</span>) <span class="hljs-keyword">new</span> vllm::<span class="hljs-built_in">CustomAllreduce</span>(<br>      <span class="hljs-built_in">reinterpret_cast</span>&lt;vllm::Signal*&gt;(meta.<span class="hljs-built_in">data_ptr</span>()), rank_data.<span class="hljs-built_in">data_ptr</span>(),<br>      rank_data.<span class="hljs-built_in">numel</span>(), ipc_handles, offsets, rank, full_nvlink);<br>&#125;<br></code></pre></td></tr></table></figure>
<p>ä¸Šé¢çš„å®ç°æ„æ€å¾ˆç®€å•ï¼ŒæŠŠä¹‹å‰ä» CPU å¹¿æ’­æ‹¿åˆ°çš„ <code>handles</code> æ”¾å…¥åˆ° <code>CustomAllreduce</code> ç±»å†…ç®¡ç†ã€‚</p>
<p>éšåå°±æ˜¯åˆå§‹åŒ– <code>CustomAllreduce</code>ã€‚è¯¥ç±»å†…éƒ¨çš„æ•°ç»„ <code>sg_</code> æœ€å¤šæœ‰ 8 ä¸ª <code>vllm::Signal</code> æŒ‡é’ˆï¼Œä»–ä»¬åˆ†åˆ«æŒ‡å‘æ‰€æœ‰ GPU èŠ‚ç‚¹ä¸Š <code>CustomAllreduce</code> çš„ <code>meta</code> å†…å­˜ï¼ˆé€šè¿‡ CUDA IPC handles å’Œè‡ªå·±å†…éƒ¨æŒ‡é’ˆï¼‰ã€‚</p>
<p>è€Œä¸ºäº†ä¿è¯æ”¶å‘ä¸ä¼šäº’ç›¸å½±å“æˆ–äº§ç”Ÿæ­»é”ï¼Œ<code>vllm::Signal</code> å°†æ”¶å‘æ•°ç»„åˆ†å¼€äº†ï¼š</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">constexpr</span> <span class="hljs-type">int</span> kMaxBlocks = <span class="hljs-number">36</span>;<br><span class="hljs-keyword">struct</span> <span class="hljs-title class_">Signal</span> &#123;<br>  <span class="hljs-built_in">alignas</span>(<span class="hljs-number">128</span>) FlagType self_counter[kMaxBlocks][<span class="hljs-number">8</span>];<br>  <span class="hljs-built_in">alignas</span>(<span class="hljs-number">128</span>) FlagType peer_counter[<span class="hljs-number">2</span>][kMaxBlocks][<span class="hljs-number">8</span>];<br>&#125;;<br></code></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">CustomAllreduce</span>(Signal* meta, <span class="hljs-type">void</span>* rank_data, <span class="hljs-type">size_t</span> rank_data_sz,<br>                <span class="hljs-type">const</span> cudaIpcMemHandle_t* handles,<br>                <span class="hljs-type">const</span> std::vector&lt;<span class="hljs-type">int64_t</span>&gt;&amp; offsets, <span class="hljs-type">int</span> rank,<br>                <span class="hljs-type">bool</span> full_nvlink = <span class="hljs-literal">true</span>)<br>    : <span class="hljs-built_in">rank_</span>(rank), <span class="hljs-built_in">world_size_</span>(offsets.<span class="hljs-built_in">size</span>()),<br>    <span class="hljs-built_in">full_nvlink_</span>(full_nvlink), <span class="hljs-built_in">self_sg_</span>(meta),<br>    <span class="hljs-built_in">d_rank_data_base_</span>(<span class="hljs-built_in">reinterpret_cast</span>&lt;RankData*&gt;(rank_data)),<br>    <span class="hljs-built_in">d_rank_data_end_</span>(d_rank_data_base_ + rank_data_sz / <span class="hljs-built_in">sizeof</span>(RankData)) &#123;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; world_size_; i++) &#123;<br>        Signal* rank_sg;<br>        <span class="hljs-keyword">if</span> (i != rank_) &#123;<br>            <span class="hljs-type">char</span>* handle = <span class="hljs-built_in">open_ipc_handle</span>(&amp;handles[i]);<br>            handle += offsets[i];<br>            rank_sg = (Signal*)handle;<br>        &#125; <span class="hljs-keyword">else</span> &#123;<br>            rank_sg = self_sg_;<br>        &#125;<br>        sg_.signals[i] = rank_sg;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<p>å‡½æ•° <code>open_ipc_handle</code> çš„å®ç°å¯ä»¥å‚è€ƒ CUDA IPC APIçš„<a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g01050a29fefde385b1042081ada4cde9">ä½¿ç”¨è¯´æ˜</a>ã€‚æœ€åï¼Œè¿”å›çš„ <code>char* handle</code> å†åŠ ä¸Šä¹‹å‰è·å¾—çš„åç§»é‡ï¼Œå°±å¯ä»¥ç›´æ¥æŒ‡å‘çœŸæ­£å­˜æ”¾ storage æ•°æ®çš„å†…å­˜ä½ç½®äº†ã€‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">char</span>* <span class="hljs-title">open_ipc_handle</span><span class="hljs-params">(<span class="hljs-type">const</span> <span class="hljs-type">void</span>* ipc_handle)</span> </span>&#123;<br>  <span class="hljs-keyword">auto</span> [it, new_handle] =<br>    ipc_handles_.<span class="hljs-built_in">insert</span>(&#123;*((IPC_KEY*)ipc_handle), <span class="hljs-literal">nullptr</span>&#125;);<br>  <span class="hljs-keyword">if</span> (new_handle) &#123;<br>    <span class="hljs-type">char</span>* ipc_ptr;<br>    <span class="hljs-built_in">CUDACHECK</span>(<span class="hljs-built_in">cudaIpcOpenMemHandle</span>((<span class="hljs-type">void</span>**)&amp;ipc_ptr,<br>                                    *((<span class="hljs-type">const</span> cudaIpcMemHandle_t*)ipc_handle),<br>                                    cudaIpcMemLazyEnablePeerAccess));<br>    it-&gt;second = ipc_ptr;<br>  &#125;<br>  <span class="hljs-keyword">return</span> it-&gt;second;<br>&#125;<br></code></pre></td></tr></table></figure>
<h3 id="æ³¨å†Œ-IPC-buffer">æ³¨å†Œ IPC buffer</h3>
<p><code>register_buffer</code> å‡½æ•°å®Œæˆäº† CUDA IPC buffer çš„æ³¨å†Œã€‚</p>
<p>ä¸‹é¢çš„ C++ ä»£ç ä¼šè¢«æ‰§è¡Œã€‚æ³¨æ„åˆ°åœ¨ç¨‹åºçš„æœ€åæ˜¯ buffer å®Œæˆæ³¨å†Œæœ€å…³é”®çš„ä¸€æ­¥ï¼šç¨‹åºä¼šå°† <code>handles</code> + <code>offset</code> å…¨éƒ¨éƒ½ copy åˆ° <code>d_rank_data_base_</code> æŒ‡å‘çš„å†…å­˜ï¼Œç»“åˆ <code>CustomAllreduce</code> çš„åˆå§‹åŒ–è¿‡ç¨‹ï¼Œå¯ä»¥çŸ¥é“è¿™å°±æ˜¯å°†æŒ‡å‘ç”¨äº allreduce æ•°æ®äº¤æ¢çš„å†…å­˜çš„æŒ‡é’ˆç§»åŠ¨åˆ°äº† <code>rank_data</code> å†…ã€‚</p>
<p>ç¨‹åºæœ€åçš„ <code>buffers_</code> åˆ™è®°å½•äº†ä¸€å¼ è¡¨æ ¼ï¼Œå­˜æ”¾ç€æœ¬èŠ‚ç‚¹ <code>buffer</code> ä¸ <code>rank_data</code> çš„å¯¹åº”å…³ç³»ã€‚è¿™ä¸€æ­¥å®Œæˆä¹‹åï¼Œæœ¬èŠ‚ç‚¹çš„ GPU å°±å¯ä»¥é€šè¿‡ <code>buffer</code> çš„æŒ‡é’ˆï¼Œè·çŸ¥å…¶ä»– GPU çš„å¯¹åº” <code>buffer</code> çš„ IPC äº¤æ¢åœ°å€ï¼Œé‚£è¿™æ ·ä¹Ÿå°±å®Œæˆäº† registerã€‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// here Tensor t is self.buffer in python</span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">register_buffer</span><span class="hljs-params">(<span class="hljs-type">fptr_t</span> _fa, torch::Tensor&amp; t,</span></span><br><span class="hljs-params"><span class="hljs-function">                     <span class="hljs-type">const</span> std::vector&lt;std::string&gt;&amp; handles,</span></span><br><span class="hljs-params"><span class="hljs-function">                     <span class="hljs-type">const</span> std::vector&lt;<span class="hljs-type">int64_t</span>&gt;&amp; offsets)</span> </span>&#123;<br>  <span class="hljs-keyword">auto</span> fa = <span class="hljs-built_in">reinterpret_cast</span>&lt;vllm::CustomAllreduce*&gt;(_fa);<br>  fa-&gt;<span class="hljs-built_in">register_buffer</span>(handles, offsets, t.<span class="hljs-built_in">data_ptr</span>());<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">register_buffer</span><span class="hljs-params">(<span class="hljs-type">const</span> std::vector&lt;std::string&gt;&amp; handles,</span></span><br><span class="hljs-params"><span class="hljs-function">                     <span class="hljs-type">const</span> std::vector&lt;<span class="hljs-type">int64_t</span>&gt;&amp; offsets, <span class="hljs-type">void</span>* self)</span> </span>&#123;<br>    <span class="hljs-built_in">check_rank_data_capacity</span>();<br>    RankData data;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; world_size_; i++) &#123;<br>        <span class="hljs-keyword">if</span> (i != rank_) &#123;<br>            <span class="hljs-type">char</span>* handle = <span class="hljs-built_in">open_ipc_handle</span>(handles[i].<span class="hljs-built_in">data</span>());<br>            handle += offsets[i];<br>            data.ptrs[i] = handle;<br>        &#125; <span class="hljs-keyword">else</span> &#123;<br>            data.ptrs[i] = self;<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">auto</span> d_data = d_rank_data_base_++;<br>    <span class="hljs-built_in">CUDACHECK</span>(<span class="hljs-built_in">cudaMemcpy</span>(d_data, &amp;data, <span class="hljs-built_in">sizeof</span>(RankData), cudaMemcpyHostToDevice));<br>    buffers_[self] = d_data;<br>&#125;<br></code></pre></td></tr></table></figure>
<h3 id="all-reduce-reg-ä¸-all-reduce-unreg">all_reduce_reg ä¸ all_reduce_unreg</h3>
<p>æ˜ç™½äº† <code>CustomAllreduce</code> ä¸­çš„æ³¨å†Œæ„ä¹‰åï¼Œå†æ¥çœ‹ä¹‹å‰æåˆ°çš„ä¸¤ä¸ª <code>all_reduce_(un)reg</code> å‡½æ•°å°±èƒ½æ›´å¥½åœ°ç†è§£ã€‚</p>
<ul>
<li><code>all_reduce_reg</code> åœ¨ä½¿ç”¨å‰å°±å·²ç»é»˜è®¤è¾“å…¥çš„ <code>inp</code> å·²ç»å®Œæˆæ³¨å†Œï¼›</li>
<li><code>all_reduce_unreg</code> åˆ™éœ€è¦å…ˆå°† <code>inp</code> çš„æ•°æ®æ‹·è´åˆ°å®Œæˆæ³¨å†Œçš„ <code>self.buffer</code>ï¼Œå†åš allreduceã€‚</li>
</ul>
<p>æ‰€ä»¥ï¼Œ<code>all_reduce_unreg</code> å‡½æ•°ä¼šå¤šä¸€æ¬¡ <code>cudaMemcpy</code>ï¼Œå¹¸å¥½è¿™ä¸ªæ•°æ®æ‹·è´æŸå¤±çš„æ€§èƒ½ä»£ä»·ä¸å¤§ã€‚å®ƒä¼šåœ¨ CUDA graph context å¤–æ—¶è¢«è°ƒç”¨ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># all reduce, assuming inp tensor is IPC registered with register_buffer,</span><br><span class="hljs-comment"># or, in the context of cuda graphs, register_graph_buffers</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">all_reduce_reg</span>(<span class="hljs-params">self, inp: torch.Tensor, out: torch.Tensor = <span class="hljs-literal">None</span></span>):<br>    <span class="hljs-keyword">if</span> out <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        out = torch.empty_like(inp)<br>    ops.all_reduce_reg(self._ptr, inp, out)<br>    <span class="hljs-keyword">return</span> out<br><br><span class="hljs-comment"># all reduce, assuming inp tensor is NOT IPC registered</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">all_reduce_unreg</span>(<span class="hljs-params">self, inp: torch.Tensor, out: torch.Tensor = <span class="hljs-literal">None</span></span>):<br>    <span class="hljs-keyword">if</span> out <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        out = torch.empty_like(inp)<br>    ops.all_reduce_unreg(self._ptr, inp, self.buffer, out)<br>    <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">all_reduce_unreg</span><span class="hljs-params">(<span class="hljs-type">fptr_t</span> _fa, torch::Tensor&amp; inp, torch::Tensor&amp; reg_buffer,</span></span><br><span class="hljs-params"><span class="hljs-function">                      torch::Tensor&amp; out)</span> </span>&#123;<br>  <span class="hljs-type">const</span> at::<span class="hljs-function">cuda::OptionalCUDAGuard <span class="hljs-title">device_guard</span><span class="hljs-params">(device_of(inp))</span></span>;<br>  <span class="hljs-keyword">auto</span> stream = c10::cuda::<span class="hljs-built_in">getCurrentCUDAStream</span>().<span class="hljs-built_in">stream</span>();<br>  <span class="hljs-keyword">auto</span> input_size = inp.<span class="hljs-built_in">numel</span>() * inp.<span class="hljs-built_in">element_size</span>();<br>  <span class="hljs-comment">// async copy the inp to self.buffer</span><br>  <span class="hljs-built_in">AT_CUDA_CHECK</span>(<span class="hljs-built_in">cudaMemcpyAsync</span>(reg_buffer.<span class="hljs-built_in">data_ptr</span>(), inp.<span class="hljs-built_in">data_ptr</span>(),<br>                                input_size, cudaMemcpyDeviceToDevice, stream));<br>  _all_reduce(_fa, reg_buffer, out, stream);<br>&#125;<br></code></pre></td></tr></table></figure>
<p>ç®€å•èµ·è§ï¼Œæˆ‘ä»¬é‡ç‚¹æ¥çœ‹ <code>all_reduce_unreg</code> è¿™ä¸€ä¸ªçš„å®ç°ï¼Œå®ƒå°†è¾“å…¥çš„ <code>inp</code> æ•°æ®æ‹·è´ç»™ <code>self.buffer</code> åï¼Œå°±ä¼šè°ƒç”¨ <code>_all_reduce</code> å‡½æ•°æ¥å®Œæˆ allreduceã€‚</p>
<h3 id="allreduce-å®ç°">allreduce å®ç°</h3>
<p><code>_all_reduce</code> å‡½æ•°åªæ˜¯ä¸€ä¸ªå¯åŠ¨å™¨ï¼Œå®ƒä¼šä¾ç…§è¾“å…¥è¾“å‡ºçš„æ•°æ®ç±»å‹å¯åŠ¨ <code>CustomAllreduce::allreduce</code> å‡½æ•°ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬é‡ç‚¹ç ”ç©¶ä¸€ä¸‹ <code>allreduce</code> å‡½æ•°ï¼š</p>
<p>å…ˆæ¥çœ‹å…¶ä¸­çš„ç¬¬ä¸€éƒ¨åˆ†ï¼Œè¯¥éƒ¨åˆ†ä¸ä¹‹å‰çš„æ³¨å†Œ IPC buffer å†…å®¹ç´§å¯†ç›¸å…³ã€‚å›é¡¾å‰æ–‡ï¼Œ<code>CustomAllreduce::buffers_</code> å†…å­˜æ”¾ç€æœ¬èŠ‚ç‚¹ <code>buffer</code> ä¸ <code>rank_data</code> çš„å¯¹åº”å…³ç³»ï¼Œäºæ˜¯è·å¾—çš„ <code>ptrs</code> å°±æ˜¯æŒ‡å‘äº† 8 ä¸ª <code>rank_data</code> å†…å­˜çš„æŒ‡é’ˆæ•°ç»„ã€‚å½“ç„¶ï¼Œè¿˜æœ‰ä¸€ç§æƒ…å†µæ˜¯ä¸åœ¨å½“å‰ä¸Šä¸‹æ–‡ï¼Œé‚£ä¹ˆéœ€è¦ä» <code>d_rank_data_base_</code> å–å‡ºå¯¹åº” <code>rank_data</code> çš„æŒ‡é’ˆæ•°ç»„ã€‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">template</span> &lt;<span class="hljs-keyword">typename</span> T&gt;<br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">allreduce</span><span class="hljs-params">(cudaStream_t stream, T* input, T* output, <span class="hljs-type">int</span> size,</span></span><br><span class="hljs-params"><span class="hljs-function">               <span class="hljs-type">int</span> threads = <span class="hljs-number">512</span>, <span class="hljs-type">int</span> block_limit = <span class="hljs-number">36</span>)</span> </span>&#123;<br>  <span class="hljs-keyword">auto</span> d = <span class="hljs-type">packed_t</span>&lt;T&gt;::P::size;<br>  <span class="hljs-comment">// ...</span><br>  RankData* ptrs;<br>  cudaStreamCaptureStatus status;<br>  <span class="hljs-built_in">CUDACHECK</span>(<span class="hljs-built_in">cudaStreamIsCapturing</span>(stream, &amp;status));<br>  <span class="hljs-keyword">if</span> (status == cudaStreamCaptureStatusActive) &#123;<br>    ptrs = d_rank_data_base_ + graph_unreg_buffers_.<span class="hljs-built_in">size</span>();<br>    graph_unreg_buffers_.<span class="hljs-built_in">push_back</span>(input);<br>  &#125; <span class="hljs-keyword">else</span> &#123;<br>    <span class="hljs-keyword">auto</span> it = buffers_.<span class="hljs-built_in">find</span>(input);<br>    <span class="hljs-keyword">if</span> (it == buffers_.<span class="hljs-built_in">end</span>())<br>      <span class="hljs-keyword">throw</span> std::<span class="hljs-built_in">runtime_error</span>(...)<br>    ptrs = it-&gt;second;<br>  &#125;<br></code></pre></td></tr></table></figure>
<p>å–å‡ºå¯¹åº”äº†å…¶ä»–èŠ‚ç‚¹ handle æŒ‡é’ˆåï¼Œä¸‹ä¸€æ­¥å°±æ˜¯å¼€å§‹ Allreduce æ“ä½œäº†ã€‚ä¸‹é¢çš„ä»£ç ä¼šåˆ†æƒ…å†µè°ƒç”¨ <code>cross_device_reduce_1stage</code> æˆ–è€… <code>cross_device_reduce_2stage</code>ã€‚ä»ä»£ç çœ‹ï¼Œå¯¹äºå°èŠ‚ç‚¹æ•°å° size çš„æƒ…å†µï¼Œä¼šä½¿ç”¨ä¸€é˜¶æ®µ allreduce <code>cross_device_reduce_1stage</code>ï¼Œåä¹‹é€‰æ‹©äºŒé˜¶æ®µ <code>cross_device_reduce_2stage</code>ã€‚</p>
<p>CUDA kernel å‡½æ•°çš„ blocks å’Œ threads æ˜¯ä½œè€…è¯•éªŒå‡ºæ¥çš„ï¼Œä»–åœ¨ A100ï¼ŒA10ï¼ŒA30 å’Œ T4 ä¸Šå°è¯•äº†å¤šæ¬¡ï¼Œæœ€ç»ˆé€‰æ‹© 36 ä¸ª blocks ä»¥è·å¾—æœ€å¥½çš„æ€§èƒ½ã€‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs c++">    size /= d;<br>    <span class="hljs-keyword">auto</span> bytes = size * <span class="hljs-built_in">sizeof</span>(<span class="hljs-keyword">typename</span> <span class="hljs-type">packed_t</span>&lt;T&gt;::P);<br>    <span class="hljs-type">int</span> blocks = std::<span class="hljs-built_in">min</span>(block_limit, (size + threads - <span class="hljs-number">1</span>) / threads);<br><span class="hljs-meta">#<span class="hljs-keyword">define</span> KL(ngpus, name)                                                       \</span><br><span class="hljs-meta">  name<span class="hljs-string">&lt;T, ngpus&gt;</span><span class="hljs-string">&lt;&lt;&lt;blocks, threads, 0, stream&gt;</span>&gt;&gt;(ptrs, sg_, self_sg_, output, \</span><br><span class="hljs-meta">                                                 rank_, size);</span><br>    <span class="hljs-comment">// TODO(hanzhi713): Threshold is different for A100 and H100.</span><br>    <span class="hljs-comment">// Add per device threshold.</span><br><span class="hljs-meta">#<span class="hljs-keyword">define</span> REDUCE_CASE(ngpus)                            \</span><br><span class="hljs-meta">  case ngpus: &#123;                                       \</span><br><span class="hljs-meta">    <span class="hljs-keyword">if</span> (world_size_ == 2) &#123;                           \</span><br><span class="hljs-meta">      KL(ngpus, cross_device_reduce_1stage);          \</span><br><span class="hljs-meta">    &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (full_nvlink_) &#123;                        \</span><br><span class="hljs-meta">      <span class="hljs-keyword">if</span> ((world_size_ &lt;= 4 &amp;&amp; bytes &lt; 512 * 1024) || \</span><br><span class="hljs-meta">          (world_size_ &lt;= 8 &amp;&amp; bytes &lt; 256 * 1024)) &#123; \</span><br><span class="hljs-meta">        KL(ngpus, cross_device_reduce_1stage);        \</span><br><span class="hljs-meta">      &#125; <span class="hljs-keyword">else</span> &#123;                                        \</span><br><span class="hljs-meta">        KL(ngpus, cross_device_reduce_2stage);        \</span><br><span class="hljs-meta">      &#125;                                               \</span><br><span class="hljs-meta">    &#125;                                                 \</span><br><span class="hljs-meta">    break;                                            \</span><br><span class="hljs-meta">  &#125;</span><br><br>    <span class="hljs-keyword">switch</span> (world_size_) &#123;<br>      <span class="hljs-built_in">REDUCE_CASE</span>(<span class="hljs-number">2</span>)<br>      <span class="hljs-built_in">REDUCE_CASE</span>(<span class="hljs-number">4</span>)<br>      <span class="hljs-built_in">REDUCE_CASE</span>(<span class="hljs-number">6</span>)<br>      <span class="hljs-built_in">REDUCE_CASE</span>(<span class="hljs-number">8</span>)<br>      <span class="hljs-keyword">default</span>:<br>        <span class="hljs-keyword">throw</span> std::<span class="hljs-built_in">runtime_error</span>(...)<br>    &#125;<br><span class="hljs-meta">#<span class="hljs-keyword">undef</span> REDUCE_CASE</span><br><span class="hljs-meta">#<span class="hljs-keyword">undef</span> KL</span><br>  &#125;<br></code></pre></td></tr></table></figure>
<h3 id="cross-device-reduce-å®ç°">cross_device_reduce å®ç°</h3>
<p>cross_device_reduce_1stage ä»£ç å®ç°å¦‚ä¸‹ï¼šé¦–å…ˆè¦ä¿è¯æ‰€æœ‰çš„èŠ‚ç‚¹éƒ½åœ¨æ‰§è¡Œ allreduce å‰åŒæ­¥ï¼Œ<code>multi_gpu_barrier&lt;ngpus, true&gt;()</code> ä¼šé¦–å…ˆå°† <code>vll::Signal</code> æ•°ç»„æ‰§è¡Œä¿¡å·åŒæ­¥ï¼ŒéšååŒæ­¥ block å†…çº¿ç¨‹ã€‚ä¹‹æ‰€ä»¥ç”¨ <code>vll::Signal</code> åšä¿¡å·åŒæ­¥ï¼Œæ˜¯å› ä¸ºä»£ç ä¸­ä¼šå‡ºç°ä¸¤æ¬¡ <code>multi_gpu_barrier</code>ï¼Œä¸ºäº†é˜²æ­¢èŠ‚ç‚¹é—´é€Ÿåº¦ä¸ä¸€è‡´å¯¼è‡´çš„åœ¨ä¸åŒ <code>multi_gpu_barrier</code> å‡½æ•°ä¸ŠåŒæ­¥ã€‚æ¯”å¦‚è‹¥æ²¡æœ‰  <code>vll::Signal</code>ï¼ŒèŠ‚ç‚¹ 1 åœ¨ç¬¬äºŒä¸ª <code>multi_gpu_barrier</code> åŒæ­¥ï¼Œè€ŒèŠ‚ç‚¹ 2 è¿˜æœªè¾¾åˆ°ç¬¬ä¸€ä¸ª <code>multi_gpu_barrier</code>ï¼Œç„¶åä»–ä»¬åŒæ­¥åæ¥ç€å¾€ä¸‹èµ°ï¼Œå°±ä¼šå‡ºç°ç¨‹åºæ­»é”æˆ–è€… bugã€‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">template</span> &lt;<span class="hljs-keyword">typename</span> T, <span class="hljs-type">int</span> ngpus&gt;<br>__global__ <span class="hljs-type">void</span> __launch_bounds__(<span class="hljs-number">512</span>, <span class="hljs-number">1</span>)<br>    <span class="hljs-built_in">cross_device_reduce_1stage</span>(RankData* _dp, RankSignals sg, Signal* self_sg,<br>                               T* __restrict__ result, <span class="hljs-type">int</span> rank, <span class="hljs-type">int</span> size) &#123;<br>  <span class="hljs-keyword">using</span> P = <span class="hljs-keyword">typename</span> <span class="hljs-type">packed_t</span>&lt;T&gt;::P;<br>  <span class="hljs-keyword">using</span> A = <span class="hljs-keyword">typename</span> <span class="hljs-type">packed_t</span>&lt;T&gt;::A;<br>  <span class="hljs-comment">// note: we don&#x27;t reorder the address so the accumulation order is the same</span><br>  <span class="hljs-comment">// for all ranks, ensuring bitwise identical results</span><br>  <span class="hljs-keyword">auto</span> dp = *_dp;<br>  <span class="hljs-built_in">multi_gpu_barrier</span>&lt;ngpus, <span class="hljs-literal">true</span>&gt;(sg, self_sg, rank);<br>  <span class="hljs-comment">// do the actual reduction</span><br>  <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> idx = blockIdx.x * blockDim.x + threadIdx.x; idx &lt; size;<br>       idx += gridDim.x * blockDim.x) &#123;<br>    ((P*)result)[idx] = <span class="hljs-built_in">packed_reduce</span>&lt;P, ngpus, A&gt;((<span class="hljs-type">const</span> P**)&amp;dp.ptrs[<span class="hljs-number">0</span>], idx);<br>  &#125;<br>  <span class="hljs-built_in">multi_gpu_barrier</span>&lt;ngpus, <span class="hljs-literal">false</span>&gt;(sg, self_sg, rank);<br>&#125;<br></code></pre></td></tr></table></figure>
<p>å¯¹äº two-shot allreduceï¼Œæ‰§è¡Œæ­¥éª¤ä¼šç¨æ˜¾å¤æ‚ã€‚é¦–å…ˆï¼Œç¨‹åºå°†æ¯ä¸ªèŠ‚ç‚¹çš„æ•°æ®åˆ†æˆäº† <code>ngpus</code> ä»½ï¼Œç„¶åæŒ‰ç…§èŠ‚ç‚¹ <code>rank</code> å·åˆ†é…å¥½æŒ‡é’ˆä½ç½®ï¼Œå†å°†æ•°æ®é€šè¿‡ reduce çš„æ–¹å¼æ±‚å’Œå­˜å…¥åˆ° <code>tmp</code> æ•°ç»„ä¸­ã€‚ç»è¿‡ç¬¬äºŒä¸ª <code>multi_gpu_barrier</code> åï¼Œæ‰§è¡Œ allgatherï¼Œç¬¬ i part éƒ¨åˆ†çš„æ•°æ®ä¼šä»ç¬¬ i ä¸ªèŠ‚ç‚¹è¿‡æ¥ï¼Œæ‰€ä»¥ i å·èŠ‚ç‚¹éœ€è¦éå† <code>ngpus</code> éï¼Œå°†å…¶ä»–èŠ‚ç‚¹çš„æ•°æ®éƒ½ gather èµ·æ¥ã€‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">template</span> &lt;<span class="hljs-keyword">typename</span> T, <span class="hljs-type">int</span> ngpus&gt;<br>__global__ <span class="hljs-type">void</span> __launch_bounds__(<span class="hljs-number">512</span>, <span class="hljs-number">1</span>)<br>    <span class="hljs-built_in">cross_device_reduce_2stage</span>(RankData* _dp, RankSignals sg, Signal* self_sg,<br>                               T* __restrict__ result, <span class="hljs-type">int</span> rank, <span class="hljs-type">int</span> size) &#123;<br>  <span class="hljs-type">int</span> tid = blockIdx.x * blockDim.x + threadIdx.x;<br>  <span class="hljs-type">int</span> stride = gridDim.x * blockDim.x;<br>  <span class="hljs-keyword">using</span> P = <span class="hljs-keyword">typename</span> <span class="hljs-type">packed_t</span>&lt;T&gt;::P;<br>  <span class="hljs-keyword">using</span> A = <span class="hljs-keyword">typename</span> <span class="hljs-type">packed_t</span>&lt;T&gt;::A;<br>  <span class="hljs-type">int</span> part = size / ngpus;<br>  <span class="hljs-type">int</span> start = rank * part;<br>  <span class="hljs-type">int</span> end = rank == ngpus - <span class="hljs-number">1</span> ? size : start + part;<br>  <span class="hljs-type">int</span> largest_part = part + size % ngpus;<br>  <span class="hljs-type">const</span> P* ptrs[ngpus];<br>  P* tmps[ngpus];<br><span class="hljs-meta">#<span class="hljs-keyword">pragma</span> unroll</span><br>  <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; ngpus; i++) &#123;<br>    <span class="hljs-type">int</span> target = (rank + i) % ngpus;<br>    ptrs[i] = (<span class="hljs-type">const</span> P*)_dp-&gt;ptrs[target];<br>    tmps[i] = <span class="hljs-built_in">get_tmp_buf</span>&lt;P&gt;(sg.signals[target]);<br>  &#125;<br>  <span class="hljs-keyword">auto</span> tmp_out = tmps[<span class="hljs-number">0</span>];<br>  <span class="hljs-built_in">multi_gpu_barrier</span>&lt;ngpus, <span class="hljs-literal">true</span>&gt;(sg, self_sg, rank);<br>  <span class="hljs-comment">// stage 1: reduce scatter</span><br>  <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> idx = start + tid; idx &lt; end; idx += stride) &#123;<br>    tmp_out[idx - start] = <span class="hljs-built_in">packed_reduce</span>&lt;P, ngpus, A&gt;(ptrs, idx);<br>  &#125;<br>  <span class="hljs-built_in">multi_gpu_barrier</span>&lt;ngpus, <span class="hljs-literal">false</span>, <span class="hljs-literal">true</span>&gt;(sg, self_sg, rank);<br><br>  <span class="hljs-comment">// stage 2: allgather</span><br>  <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> idx = tid; idx &lt; largest_part; idx += stride) &#123;<br><span class="hljs-meta">#<span class="hljs-keyword">pragma</span> unroll</span><br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; ngpus; i++) &#123;<br>      <span class="hljs-type">int</span> gather_from_rank = ((rank + i) % ngpus);<br>      <span class="hljs-keyword">if</span> (gather_from_rank == ngpus - <span class="hljs-number">1</span> || idx &lt; part) &#123;<br>        <span class="hljs-type">int</span> dst_idx = gather_from_rank * part + idx;<br>        ((P*)result)[dst_idx] = tmps[i][idx];<br>      &#125;<br>    &#125;<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<p>å€˜è‹¥ä½ ç†è§£äº†å‰æ–‡æ‰€è¯´ one-shot allreduce å’Œ two-shot allreduce çš„å®ç°åŸç†ï¼Œé‚£ä¹ˆä¸Šé¢éƒ¨åˆ†çš„ CUDA ä»£ç å®ç°å…¶å®éå¸¸ç®€å•ï¼Œä½†éš¾åœ¨å¦‚ä½•å†™å‡ºé«˜æ€§èƒ½çš„ä»£ç ã€‚æœ¬ç€å­¦ä¹ çš„æ€åº¦ï¼Œæˆ‘ä»”ç»†ç ”ç©¶å¹¶æ€»ç»“äº†ä»¥ä¸‹ä¼˜åŒ–ç»†èŠ‚ï¼š</p>
<ul>
<li>
<p><code>packed_t</code> ä¸­å¯¹é½ 128 bits çš„å®ç°ã€‚æœ‰åˆ©äºçº¿ç¨‹æ›´å¿« load æ•°æ®</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">template</span> &lt;<span class="hljs-keyword">typename</span> T, <span class="hljs-type">int</span> sz&gt;<br><span class="hljs-keyword">struct</span> <span class="hljs-title class_">__align__</span>(<span class="hljs-built_in">alignof</span>(T) * sz) <span class="hljs-type">array_t</span> &#123;<br>  T data[sz];<br>  <span class="hljs-keyword">using</span> type = T;<br>  <span class="hljs-type">static</span> <span class="hljs-keyword">constexpr</span> <span class="hljs-type">int</span> size = sz;<br>&#125;;<br><br><span class="hljs-keyword">template</span> &lt;<span class="hljs-keyword">typename</span> T&gt;<br><span class="hljs-keyword">struct</span> <span class="hljs-title class_">packed_t</span> &#123;<br>  <span class="hljs-comment">// the (P)acked type for load/store</span><br>  <span class="hljs-keyword">using</span> P = <span class="hljs-type">array_t</span>&lt;T, <span class="hljs-number">16</span> / <span class="hljs-built_in">sizeof</span>(T)&gt;;<br>  <span class="hljs-comment">// the (A)ccumulator type for reduction</span><br>  <span class="hljs-keyword">using</span> A = <span class="hljs-type">array_t</span>&lt;<span class="hljs-type">float</span>, <span class="hljs-number">16</span> / <span class="hljs-built_in">sizeof</span>(T)&gt;;<br>&#125;;<br></code></pre></td></tr></table></figure>
</li>
<li>
<p>åµŒå…¥ cuda ptx ä»£ç ï¼ŒåŒæ ·åœ°ï¼Œä¸ºäº†ä¿è¯ 128 bits å¯¹é½ï¼Œä½¿ç”¨ u32 æŒ‡ä»¤</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">static</span> DINLINE <span class="hljs-type">void</span> <span class="hljs-title">st_flag_volatile</span><span class="hljs-params">(FlagType* flag_addr, FlagType flag)</span> </span>&#123;<br>  <span class="hljs-function"><span class="hljs-keyword">asm</span> <span class="hljs-title">volatile</span><span class="hljs-params">(<span class="hljs-string">&quot;st.volatile.global.u32 [%1], %0;&quot;</span> ::<span class="hljs-string">&quot;r&quot;</span>(flag), <span class="hljs-string">&quot;l&quot;</span>(flag_addr))</span></span>;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">static</span> DINLINE FlagType <span class="hljs-title">ld_flag_volatile</span><span class="hljs-params">(FlagType* flag_addr)</span> </span>&#123;<br>  FlagType flag;<br>  <span class="hljs-function"><span class="hljs-keyword">asm</span> <span class="hljs-title">volatile</span><span class="hljs-params">(<span class="hljs-string">&quot;ld.volatile.global.u32 %0, [%1];&quot;</span></span></span><br><span class="hljs-params"><span class="hljs-function">              : <span class="hljs-string">&quot;=r&quot;</span>(flag)</span></span><br><span class="hljs-params"><span class="hljs-function">              : <span class="hljs-string">&quot;l&quot;</span>(flag_addr))</span></span>;<br>  <span class="hljs-keyword">return</span> flag;<br>&#125;<br></code></pre></td></tr></table></figure>
</li>
<li>
<p>ä½¿ç”¨ä½ç²¾åº¦åšè®¡ç®—ï¼Œåè½¬åˆ°é«˜ç²¾åº¦ float</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">template</span> &lt;<span class="hljs-keyword">typename</span> P, <span class="hljs-type">int</span> ngpus, <span class="hljs-keyword">typename</span> A&gt;<br><span class="hljs-function">DINLINE P <span class="hljs-title">packed_reduce</span><span class="hljs-params">(<span class="hljs-type">const</span> P* ptrs[], <span class="hljs-type">int</span> idx)</span> </span>&#123;<br>  A tmp = <span class="hljs-built_in">upcast</span>(ptrs[<span class="hljs-number">0</span>][idx]);<br><span class="hljs-meta">#<span class="hljs-keyword">pragma</span> unroll</span><br>  <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>; i &lt; ngpus; i++) &#123;<br>    <span class="hljs-built_in">packed_assign_add</span>(tmp, <span class="hljs-built_in">upcast</span>(ptrs[i][idx]));<br>  &#125;<br>  <span class="hljs-keyword">return</span> <span class="hljs-built_in">downcast</span>&lt;P&gt;(tmp);<br>&#125;<br></code></pre></td></tr></table></figure>
</li>
<li>
<p>å¾ªç¯å……åˆ†å±•å¼€ï¼Œæ¯”å¦‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">pragma</span> unroll</span><br>  <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; ngpus; i++) &#123;<br>    <span class="hljs-type">int</span> gather_from_rank = ((rank + i) % ngpus);<br>    <span class="hljs-keyword">if</span> (gather_from_rank == ngpus - <span class="hljs-number">1</span> || idx &lt; part) &#123;<br>      <span class="hljs-type">int</span> dst_idx = gather_from_rank * part + idx;<br>      ((P*)result)[dst_idx] = tmps[i][idx];<br>    &#125;<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure></li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/AI/" class="category-chain-item">AI</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/AI/" class="print-no-link">#AI</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>VLLM custom allreduce å®ç°</div>
      <div>https://dingfen.github.io/2024/08/02/2024-10-30-vllm/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>ä½œè€…</div>
          <div>Bill Ding</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>å‘å¸ƒäº</div>
          <div>2024å¹´8æœˆ2æ—¥</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>æ›´æ–°äº</div>
          <div>2025å¹´1æœˆ26æ—¥</div>
        </div>
      
      
        <div class="license-meta-item">
          <div>è®¸å¯åè®®</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - ç½²å">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="SA - ç›¸åŒæ–¹å¼å…±äº«">
                    <i class="iconfont icon-sa"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/08/18/2024-8-18-cute/" title="æ·±å…¥ CUTLASS ä¹‹ CuTe è¯¦è§£">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">æ·±å…¥ CUTLASS ä¹‹ CuTe è¯¦è§£</span>
                        <span class="visible-mobile">ä¸Šä¸€ç¯‡</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/08/02/2024-5-9-vllm/" title="VLLM Paged Attention å®ç°">
                        <span class="hidden-mobile">VLLM Paged Attention å®ç°</span>
                        <span class="visible-mobile">ä¸‹ä¸€ç¯‡</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>ç›®å½•</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  








    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">æœç´¢</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">å…³é”®è¯</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        æ€»è®¿é—®é‡ 
        <span id="busuanzi_value_site_pv"></span>
         æ¬¡
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        æ€»è®¿å®¢æ•° 
        <span id="busuanzi_value_site_uv"></span>
         äºº
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  var relativeDate = function() {
    var updatedTime = document.getElementById('updated-time');
    if (updatedTime) {
      var text = updatedTime.textContent;
      var reg = /\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:Z|[+-]\d{2}:\d{2})/;
      var matchs = text.match(reg);
      if (matchs) {
        var relativeTime = moment(matchs[0]).fromNow();
        updatedTime.textContent = text.replace(reg, relativeTime);
      }
      updatedTime.style.display = '';
    }
  };
  Fluid.utils.createScript('https://lib.baomitu.com/moment.js/2.29.4/moment.min.js', function() {
    if (!'zh-cn'.startsWith('en')) {
      Fluid.utils.createScript('https://lib.baomitu.com/moment.js/2.29.4/locale/zh-cn.min.js', function() {
        relativeDate();
      });
    } else {
      relativeDate();
    }
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  
<script src="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/Ribbon.min.js"></script>



<!-- ä¸»é¢˜çš„å¯åŠ¨é¡¹ï¼Œå°†å®ƒä¿æŒåœ¨æœ€åº•éƒ¨ -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">åšå®¢åœ¨å…è®¸ JavaScript è¿è¡Œçš„ç¯å¢ƒä¸‹æµè§ˆæ•ˆæœæ›´ä½³</div>
  </noscript>
<!-- hexo injector body_end start -->
  <link rel="stylesheet" crossorigin href="https://g.alicdn.com/aliyun-documentation/web-chatbot-ui/0.0.11/index.css" />
  <script type="module" crossorigin src="https://g.alicdn.com/aliyun-documentation/web-chatbot-ui/0.0.11/index.js"></script>
  <script>
    window.CHATBOT_CONFIG = {
      endpoint: "https://web-chatbot-syz-knthhrjfeq.cn-hangzhou.fcapp.run/chat", // å¯ä»¥æ›¿æ¢ä¸º https://{your-fc-http-trigger-domain}/chat
      displayByDefault: false, // é»˜è®¤ä¸æ˜¾ç¤º AI åŠ©æ‰‹å¯¹è¯æ¡†
      aiChatOptions: { // è‡ªå®šä¹‰å–å€¼å‚è€ƒï¼šhttps://docs.nlkit.com/nlux/reference/ui/ai-chat#conversation-options
        conversationOptions: { // è‡ªå®šä¹‰å–å€¼å‚è€ƒï¼šhttps://docs.nlkit.com/nlux/reference/ui/ai-chat#conversation-options
          conversationStarters: [
            {prompt: 'è¯·é—®ä½ æ˜¯è°ï¼Œèƒ½ä¸ºæˆ‘åšä»€ä¹ˆï¼Ÿ'},
            {prompt: 'è¯·ä»‹ç»ä¸€ä¸‹åšå®¢çš„ä¸»äºº'}
          ],
          layout: 'bubbles'
        },
        displayOptions: { // è‡ªå®šä¹‰å–å€¼å‚è€ƒï¼šhttps://docs.nlkit.com/nlux/reference/ui/ai-chat#display-options
          height: 550,
          // width: 400,
        },
        personaOptions: { // è‡ªå®šä¹‰å–å€¼å‚è€ƒï¼šhttps://docs.nlkit.com/nlux/reference/ui/ai-chat#chat-personas
          assistant: {
            name: 'ä½ å¥½ï¼Œæˆ‘æ˜¯æœ¬ç½‘ç«™çš„ AI åŠ©æ‰‹',
            // AI åŠ©æ‰‹çš„å›¾æ ‡
            avatar: 'https://img.alicdn.com/imgextra/i2/O1CN01Pda9nq1YDV0mnZ31H_!!6000000003025-54-tps-120-120.apng',
            tagline: 'è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œæˆ‘ä¼šå°½åŠ›å¸®ä½ è§£ç­”ï¼',
          }
        }
      }
    };
  </script>
  <style>
    :root {
      /* webchat å·¥å…·æ çš„é¢œè‰² */
      --webchat-toolbar-background-color: #1464E4;
      /* webchat å·¥å…·æ æ–‡å­—å’ŒæŒ‰é’®çš„é¢œè‰² */
      --webchat-toolbar-text-color: #FFF;
    }
    /* webchat å¯¹è¯æ¡†å¦‚æœè¢«é®æŒ¡ï¼Œå¯ä»¥å°è¯•é€šè¿‡ z-indexã€bottomã€right ç­‰è®¾ç½® æ¥è°ƒæ•´*/
    .webchat-container {
      z-index: 100;
      bottom: 10px;
      right: 10px;
    }
    /* webchat çš„å”¤èµ·æŒ‰é’®å¦‚æœè¢«é®æŒ¡ï¼Œå¯ä»¥å°è¯•é€šè¿‡ z-indexã€bottomã€right ç­‰è®¾ç½® æ¥è°ƒæ•´ã€‚ä¹Ÿå¯ä»¥é€šè¿‡ CSS è¿›ä¸€æ­¥å®šåˆ¶å”¤èµ·æŒ‰é’®çš„å½¢çŠ¶ã€å¤§å°ç­‰ã€‚ */
    .webchat-bubble-tip {
      z-index: 99;
      bottom: 20px;
      right: 20px;
    }
  </style>
  <!-- hexo injector body_end end --></body>
</html>
