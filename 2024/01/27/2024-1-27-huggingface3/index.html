

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Bill Ding">
  <meta name="keywords" content="">
  
    <meta name="description" content="æ™ºè°±æ¸…è¨€ã®å¤§æ¨¡å‹">
<meta property="og:type" content="article">
<meta property="og:title" content="THUDM&#x2F;chatglmä»£ç ç»†è¯»">
<meta property="og:url" content="https://dingfen.github.io/2024/01/27/2024-1-27-huggingface3/index.html">
<meta property="og:site_name" content="å³°å­çš„ä¹å›­">
<meta property="og:description" content="æ™ºè°±æ¸…è¨€ã®å¤§æ¨¡å‹">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://dingfen.github.io/img/LLM/ChatGLM2.png">
<meta property="og:image" content="https://dingfen.github.io/img/LLM/RoPE3.png">
<meta property="og:image" content="https://dingfen.github.io/img/LLM/ChatGLM2MLP.png">
<meta property="article:published_time" content="2024-01-27T04:00:00.000Z">
<meta property="article:modified_time" content="2025-01-26T11:49:09.209Z">
<meta property="article:author" content="Bill Ding">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://dingfen.github.io/img/LLM/ChatGLM2.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>THUDM/chatglmä»£ç ç»†è¯» - å³°å­çš„ä¹å›­</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/KaTeX/0.16.2/katex.min.css" />



<!-- ä¸»é¢˜ä¾èµ–çš„å›¾æ ‡åº“ï¼Œä¸è¦è‡ªè¡Œä¿®æ”¹ -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"dingfen.github.io","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"|","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":null,"onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.1.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>å³°å­çš„ä¹å›­</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>é¦–é¡µ</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>å½’æ¡£</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>åˆ†ç±»</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>æ ‡ç­¾</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>å…³äº</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="THUDM/chatglmä»£ç ç»†è¯»"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-01-27 12:00" pubdate>
          2024å¹´1æœˆ27æ—¥ ä¸­åˆ
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          4.1k å­—
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          34 åˆ†é’Ÿ
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">THUDM/chatglmä»£ç ç»†è¯»</h1>
            
              <p id="updated-time" class="note note-info" style="display: none">
                
                  
                    <!-- compatible with older versions-->
                    æ›´æ–°äºï¼š2025-01-26T19:49:09+08:00
                  
                  

                
              </p>
            
            
              <div class="markdown-body">
                
                <h1>THUDM/ChatGLMä»£ç ç»†è¯»</h1>
<h2 id="ChatGLM-æ˜¯ä»€ä¹ˆ">ChatGLM æ˜¯ä»€ä¹ˆ</h2>
<p>ChatGLM-6B æ˜¯ä¸€ä¸ªåŸºäº General Language Model (GLM) æ¶æ„çš„å¼€æºã€æ”¯æŒä¸­è‹±åŒè¯­çš„å¯¹è¯è¯­è¨€æ¨¡å‹ã€‚<br>
ç”±æ¸…åå¤§å­¦ç ”å‘ï¼Œæˆªè‡³ç¬”è€…æ›´æ–°æ—¶å·²ç»å‘å¸ƒäº†ç¬¬ä¸‰ä»£ ChatGLM3ã€‚ChatGLM æ¨¡å‹ä½¿ç”¨äº†å’Œ ChatGPT ç›¸ä¼¼çš„æŠ€æœ¯ï¼Œä½¿ç”¨çº¦ 1T æ ‡è¯†ç¬¦çš„ä¸­è‹±åŒè¯­è®­ç»ƒï¼Œå†è¾…ä»¥ç›‘ç£å¾®è°ƒã€åé¦ˆè‡ªåŠ©ã€äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ç­‰æŠ€æœ¯ç‚¼æˆã€‚</p>
<p>æ¸…åå¤§å­¦å›¢é˜Ÿå’Œæ™ºè°±AIå¯ä»¥è¯´æµ‘èº«æ˜¯è‚ï¼Œå‘å¸ƒäº†è®¸å¤šå¤§æ¨¡å‹ï¼š</p>
<ul>
<li>
<p><a target="_blank" rel="noopener" href="https://github.com/thudm/chatglm2-6b">ChatGLM2-6B</a></p>
<ul>
<li>æ›´å¼ºå¤§çš„æ€§èƒ½ï¼šå‡çº§äº† ChatGLM2-6B çš„åŸºåº§æ¨¡å‹ã€‚ChatGLM2-6B ä½¿ç”¨äº† GLM çš„æ··åˆç›®æ ‡å‡½æ•°ï¼Œç»è¿‡äº† 1.4T ä¸­è‹±æ ‡è¯†ç¬¦çš„é¢„è®­ç»ƒä¸äººç±»åå¥½å¯¹é½è®­ç»ƒï¼Œè¯„æµ‹ç»“æœæ˜¾ç¤ºï¼Œç›¸æ¯”äºåˆä»£æ¨¡å‹ï¼ŒChatGLM2-6B åœ¨ MMLUï¼ˆ+23%ï¼‰ã€CEvalï¼ˆ+33%ï¼‰ã€GSM8Kï¼ˆ+571%ï¼‰ ã€BBHï¼ˆ+60%ï¼‰ç­‰æ•°æ®é›†ä¸Šçš„æ€§èƒ½å–å¾—äº†å¤§å¹…åº¦çš„æå‡ï¼Œåœ¨åŒå°ºå¯¸å¼€æºæ¨¡å‹ä¸­å…·æœ‰è¾ƒå¼ºçš„ç«äº‰åŠ›ã€‚</li>
<li>æ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼šä½¿ç”¨ FlashAttention æŠ€æœ¯å°†åŸºåº§æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆContext Lengthï¼‰ç”± ChatGLM-6B çš„ 2K æ‰©å±•åˆ°äº† 32Kï¼Œå¹¶åœ¨å¯¹è¯é˜¶æ®µä½¿ç”¨ 8K çš„ä¸Šä¸‹æ–‡é•¿åº¦è®­ç»ƒã€‚å¯¹äºæ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼Œæˆ‘ä»¬å‘å¸ƒäº† ChatGLM2-6B-32K æ¨¡å‹ã€‚LongBench çš„æµ‹è¯„ç»“æœè¡¨æ˜ï¼Œåœ¨ç­‰é‡çº§çš„å¼€æºæ¨¡å‹ä¸­ï¼ŒChatGLM2-6B-32K æœ‰ç€è¾ƒä¸ºæ˜æ˜¾çš„ç«äº‰ä¼˜åŠ¿ã€‚</li>
<li>æ›´é«˜æ•ˆçš„æ¨ç†ï¼šä½¿ç”¨ Multi-Query Attention æŠ€æœ¯è·å¾—äº†æ›´å¿«çš„æ¨ç†é€Ÿåº¦å’Œæ›´ä½çš„æ˜¾å­˜å ç”¨ï¼šåœ¨å®˜æ–¹çš„æ¨¡å‹å®ç°ä¸‹ï¼Œæ¨ç†é€Ÿåº¦ç›¸æ¯”åˆä»£æå‡äº† 42%ï¼ŒINT4 é‡åŒ–ä¸‹ï¼Œ6G æ˜¾å­˜æ”¯æŒçš„å¯¹è¯é•¿åº¦ç”± 1K æå‡åˆ°äº† 8Kã€‚</li>
</ul>
</li>
<li>
<p><a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM3">ChatGLM3</a></p>
<ul>
<li>æ›´å¼ºå¤§çš„åŸºç¡€æ¨¡å‹ï¼š ChatGLM3-6B çš„åŸºç¡€æ¨¡å‹ ChatGLM3-6B-Base é‡‡ç”¨äº†æ›´å¤šæ ·çš„è®­ç»ƒæ•°æ®ã€æ›´å……åˆ†çš„è®­ç»ƒæ­¥æ•°å’Œæ›´åˆç†çš„è®­ç»ƒç­–ç•¥ã€‚åœ¨è¯­ä¹‰ã€æ•°å­¦ã€æ¨ç†ã€ä»£ç ã€çŸ¥è¯†ç­‰ä¸åŒè§’åº¦çš„æ•°æ®é›†ä¸Šæµ‹è¯„æ˜¾ç¤ºï¼Œ<em>ChatGLM3-6B-Base å…·æœ‰åœ¨ 10B ä»¥ä¸‹çš„åŸºç¡€æ¨¡å‹ä¸­æœ€å¼ºçš„æ€§èƒ½</em>ã€‚</li>
<li>æ›´å®Œæ•´çš„åŠŸèƒ½æ”¯æŒï¼š ChatGLM3-6B é‡‡ç”¨äº†å…¨æ–°è®¾è®¡çš„ Prompt æ ¼å¼ ï¼Œé™¤æ­£å¸¸çš„å¤šè½®å¯¹è¯å¤–ã€‚åŒæ—¶åŸç”Ÿæ”¯æŒå·¥å…·è°ƒç”¨ï¼ˆFunction Callï¼‰ã€ä»£ç æ‰§è¡Œï¼ˆCode Interpreterï¼‰å’Œ Agent ä»»åŠ¡ç­‰å¤æ‚åœºæ™¯ã€‚</li>
<li>æ›´å…¨é¢çš„å¼€æºåºåˆ—ï¼š é™¤äº†å¯¹è¯æ¨¡å‹ ChatGLM3-6B å¤–ï¼Œè¿˜å¼€æºäº†åŸºç¡€æ¨¡å‹ ChatGLM3-6B-Base ã€é•¿æ–‡æœ¬å¯¹è¯æ¨¡å‹ ChatGLM3-6B-32Kã€‚</li>
</ul>
</li>
</ul>
<p>æ­¤å¤–ï¼Œè¿˜æœ‰ä»¥ä¸‹æ¨¡å‹ï¼Œæˆ‘ä¸å†è¯¦ç»†ä»‹ç»ï¼š</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/THUDM/CodeGeeX2">CodeGeeX2</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/THUDM/WebGLM">WebGLM</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/THUDM/VisualGLM-6B">VisualGLM-6B</a></li>
</ul>
<h2 id="ChatGLM-æ¶æ„">ChatGLM æ¶æ„</h2>
<p>ChatGLM2 çš„æ¶æ„å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å¯ä»¥çœ‹å‡ºæ¥ï¼Œä¸ llama ç­‰å…¶ä»–åŸºäº transformer çš„æ¨¡å‹æ²¡ä»€ä¹ˆä¸åŒï¼š</p>
<p><img src="/img/LLM/ChatGLM2.png" srcset="/img/loading.gif" lazyload alt=""></p>
<h2 id="ChatGLM2-ä»£ç è¯¦è§£">ChatGLM2 ä»£ç è¯¦è§£</h2>
<p>å› å·¥ä½œåŸå› ï¼Œå…ˆåš ChatGLM2 ä»£ç çš„åˆ†æï¼Œå…¶æ¨¡å‹çš„ config æ•°æ®å¦‚ä¸‹ï¼š</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>  <span class="hljs-attr">&quot;_name_or_path&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;THUDM/chatglm2-6b&quot;</span><span class="hljs-punctuation">,</span><br>  # ....<br>  <span class="hljs-attr">&quot;add_bias_linear&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">false</span></span><span class="hljs-punctuation">,</span>                             # æ˜¯å¦åœ¨çº¿æ€§å±‚æ·»åŠ åç§»é¡¹ï¼Œå¦<br>  <span class="hljs-attr">&quot;add_qkv_bias&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">true</span></span><span class="hljs-punctuation">,</span>                                 # æ˜¯å¦æ·»åŠ  qkv å±‚çš„åç§»é¡¹ï¼Œæ˜¯<br>  <span class="hljs-attr">&quot;apply_query_key_layer_scaling&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">true</span></span><span class="hljs-punctuation">,</span>                # æ˜¯å¦å¯¹ qkv å±‚è¿›è¡Œç¼©æ”¾ï¼Œæ˜¯<br>  <span class="hljs-attr">&quot;apply_residual_connection_post_layernorm&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">false</span></span><span class="hljs-punctuation">,</span>    # æ˜¯å¦åœ¨æ®‹å·®è¿æ¥åè¿›è¡Œå±‚å½’ä¸€åŒ–ï¼Œå¦<br>  <span class="hljs-attr">&quot;attention_dropout&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">0.0</span><span class="hljs-punctuation">,</span>                             # attention dropout å€¼ï¼Œ<span class="hljs-number">0</span><br>  <span class="hljs-attr">&quot;attention_softmax_in_fp32&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">true</span></span><span class="hljs-punctuation">,</span>                    # æ˜¯å¦åœ¨ fp32 ä¸‹è®¡ç®— softmaxï¼Œæ˜¯<br>  <span class="hljs-attr">&quot;bias_dropout_fusion&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">true</span></span><span class="hljs-punctuation">,</span>                          # æ˜¯å¦èåˆ bias dropoutï¼Œæ˜¯<br>  <span class="hljs-attr">&quot;ffn_hidden_size&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">13696</span><span class="hljs-punctuation">,</span>                             # ffn å±‚çš„éšè—å±‚å¤§å°ï¼Œ<span class="hljs-number">13696</span><br>  <span class="hljs-attr">&quot;fp32_residual_connection&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">false</span></span><span class="hljs-punctuation">,</span>                    # æ˜¯å¦åœ¨ fp32 ä¸‹è¿›è¡Œæ®‹å·®è¿æ¥ï¼Œå¦<br>  <span class="hljs-attr">&quot;hidden_dropout&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">0.0</span><span class="hljs-punctuation">,</span>                                # hidden dropout å€¼ï¼Œ<span class="hljs-number">0</span><br>  <span class="hljs-attr">&quot;hidden_size&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">4096</span><span class="hljs-punctuation">,</span>                                  # éšè—å±‚å¤§å°ï¼Œ<span class="hljs-number">4096</span><br>  <span class="hljs-attr">&quot;kv_channels&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">128</span><span class="hljs-punctuation">,</span>                                   # kv å¼ é‡çš„é€šé“æ•°ï¼Œ<span class="hljs-number">128</span><br>  <span class="hljs-attr">&quot;layernorm_epsilon&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">1e-05</span><span class="hljs-punctuation">,</span>                           # layernorm å±‚çš„æœ€å°å€¼ï¼Œ<span class="hljs-number">1e-05</span><br>  <span class="hljs-attr">&quot;multi_query_attention&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">true</span></span><span class="hljs-punctuation">,</span>                        # æ˜¯å¦ä½¿ç”¨å¤šæŸ¥è¯¢æ³¨æ„åŠ›ï¼Œæ˜¯<br>  <span class="hljs-attr">&quot;multi_query_group_num&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">2</span><span class="hljs-punctuation">,</span>                           # å¤šæŸ¥è¯¢æ³¨æ„åŠ›çš„ç»„æ•°ï¼Œ<span class="hljs-number">2</span><br>  <span class="hljs-attr">&quot;num_attention_heads&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">32</span><span class="hljs-punctuation">,</span>                            # æ³¨æ„åŠ›å¤´æ•°ï¼Œ<span class="hljs-number">32</span><br>  <span class="hljs-attr">&quot;num_layers&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">28</span><span class="hljs-punctuation">,</span>                                     # transformerå±‚æ•°ï¼Œ<span class="hljs-number">28</span><br>  <span class="hljs-attr">&quot;original_rope&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">true</span></span><span class="hljs-punctuation">,</span>                                # æ˜¯å¦ä½¿ç”¨åŸç”Ÿ RoPEï¼Œæ˜¯<br>  <span class="hljs-attr">&quot;padded_vocab_size&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">65024</span><span class="hljs-punctuation">,</span>                           # å¡«å……åçš„è¯è¡¨å¤§å°ï¼Œ<span class="hljs-number">65024</span><br>  <span class="hljs-attr">&quot;post_layer_norm&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">true</span></span><span class="hljs-punctuation">,</span>                              # æ˜¯å¦åœ¨ transformer å±‚åè¿›è¡Œå±‚å½’ä¸€åŒ–ï¼Œæ˜¯<br>  <span class="hljs-attr">&quot;rmsnorm&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">true</span></span><span class="hljs-punctuation">,</span>                                      # æ˜¯å¦ä½¿ç”¨ RMSNormï¼Œæ˜¯<br>  <span class="hljs-attr">&quot;seq_length&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">32768</span><span class="hljs-punctuation">,</span>                                  # æ”¯æŒçš„æœ€é•¿åºåˆ—é•¿åº¦ï¼Œ<span class="hljs-number">32768</span><br>  <span class="hljs-attr">&quot;use_cache&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">true</span></span><span class="hljs-punctuation">,</span>                                    # æ˜¯å¦ä½¿ç”¨ KV cacheï¼Œæ˜¯<br>  <span class="hljs-attr">&quot;torch_dtype&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;float16&quot;</span><span class="hljs-punctuation">,</span>                             # æ¨¡å‹çš„ç²¾åº¦ï¼Œfloat16<br>  <span class="hljs-attr">&quot;transformers_version&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;4.27.1&quot;</span><span class="hljs-punctuation">,</span>                     # transformers ç‰ˆæœ¬ï¼Œ<span class="hljs-number">4.27</span><span class="hljs-number">.1</span><br>  <span class="hljs-attr">&quot;tie_word_embeddings&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">false</span></span><span class="hljs-punctuation">,</span>                         # æ˜¯å¦å¯¹è¯åµŒå…¥è¿›è¡Œç»‘å®šï¼Œå¦<br>  <span class="hljs-attr">&quot;eos_token_id&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">2</span><span class="hljs-punctuation">,</span>                                    # å¥å­ç»“æŸç¬¦ idï¼Œ<span class="hljs-number">2</span><br>  <span class="hljs-attr">&quot;pad_token_id&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">0</span>                                     # å¡«å……ç¬¦ idï¼Œ<span class="hljs-number">0</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure>
<h3 id="ChatGLM-åŸºä»¶">ChatGLM åŸºä»¶</h3>
<p>ä¸<a href="https://dingfen.github.io/ai/2023/10/30/huggingface1.html">å…ˆå‰çš„llamaåšå®¢</a>å¥—è·¯ä¸€æ ·ï¼Œæˆ‘ä»¬ä»åº•åˆ°é¡¶ï¼Œå…ˆä»æœ€åŸºç¡€çš„åŸºä»¶éƒ¨åˆ†ä»£ç å¼€å§‹ï¼Œä¸€æ­¥æ­¥å¾€ä¸Šèµ°ï¼Œä»è€Œé€æ­¥ç†è§£æ•´ä¸ªæ¨¡å‹çš„æ¶æ„ã€‚</p>
<h4 id="PrefixEncoder">PrefixEncoder</h4>
<p>PrefixEncoder æ˜¯ä¸€ä¸ªå‰ç¼€ç¼–ç å™¨ï¼ŒChatGLM æ¨¡å‹ä½¿ç”¨å®ƒå¯¹å†å²è¾“å…¥çš„æ–‡æœ¬åºåˆ—è¿›è¡ŒåµŒå…¥ç¼–ç ã€‚ä»æºç ä¸­å¯çŸ¥ï¼Œè¯¥ç±»ç»“æ„éšç”¨æˆ·é…ç½®çš„ <code>config.prefix_projection</code> è€Œå˜åŒ–ï¼Œå¦‚æœä¸º Trueï¼Œåˆ™ä½¿ç”¨ä¸€ä¸ªä¸¤å±‚çš„ MLP è¿›è¡Œç¼–ç ï¼Œå¦åˆ™ç›´æ¥ä½¿ç”¨ Embedding å±‚è¿›è¡Œç¼–ç ã€‚Embedding å±‚çš„è§„æ¨¡æ˜¯ <code>config.pre_seq_len</code>ï¼Œä¹‹å‰çš„æ–‡æœ¬é•¿åº¦ï¼Œæ¯ä¸ªè¯çš„åµŒå…¥å‘é‡ç»´åº¦æ˜¯ <code>config.num_layers * config.kv_channels * config.multi_query_group_num * 2 = 28 * 128 * 2 * 2 = 14336</code>ã€‚è€ŒåŠ çš„ä¸¤å±‚ MLP åˆ™æ˜¯å°†å…¶é™ç»´åˆ° <code>config.hidden_size</code>ï¼Œå…¶ MLP å†…éƒ¨çš„å‚æ•°å¯ä»¥å­˜å‚¨æ›´å¤šçš„æƒé‡ä¿¡æ¯ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PrefixEncoder</span>(torch.nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    The torch.nn model to encode the prefix</span><br><span class="hljs-string">    Input shape: (batch-size, prefix-length)</span><br><span class="hljs-string">    Output shape: (batch-size, prefix-length, 2*layers*hidden)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config: ChatGLMConfig</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.prefix_projection = config.prefix_projection<br>        <span class="hljs-keyword">if</span> self.prefix_projection:<br>            <span class="hljs-comment"># Use a two-layer MLP to encode the prefix</span><br>            kv_size = config.num_layers * config.kv_channels * config.multi_query_group_num * <span class="hljs-number">2</span><br>            self.embedding = torch.nn.Embedding(config.pre_seq_len, kv_size)<br>            self.trans = torch.nn.Sequential(<br>                torch.nn.Linear(kv_size, config.hidden_size),<br>                torch.nn.Tanh(),<br>                torch.nn.Linear(config.hidden_size, kv_size)<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            self.embedding = torch.nn.Embedding(config.pre_seq_len,<br>                                                config.num_layers * config.kv_channels * config.multi_query_group_num * <span class="hljs-number">2</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, prefix: torch.Tensor</span>):<br>        <span class="hljs-keyword">if</span> self.prefix_projection:<br>            prefix_tokens = self.embedding(prefix)<br>            past_key_values = self.trans(prefix_tokens)<br>        <span class="hljs-keyword">else</span>:<br>            past_key_values = self.embedding(prefix)<br>        <span class="hljs-keyword">return</span> past_key_values<br></code></pre></td></tr></table></figure>
<h4 id="RotaryEmbedding">RotaryEmbedding</h4>
<p>æ—‹è½¬ä½ç½®ç¼–ç çš„åŸç†åœ¨<a href="https://dingfen.github.io/ai/2023/10/30/huggingface1.html">å…ˆå‰çš„llamaåšå®¢</a>ä¸­å·²ç»ä»‹ç»è¿‡ã€‚ä½† <code>apply_rotary_pos_emb</code> å®ç°æœ‰æ‰€ä¸åŒã€‚å…ˆæ”¾å‡ è¡Œå…¬å¼ä¾›è¯»è€…å‚è€ƒå›å¿†ï¼š</p>
<p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>Î¸</mi><mi>i</mi></msub><mo>=</mo><mn>1000</mn><msup><mn>0</mn><mrow><mo>âˆ’</mo><mn>2</mn><mo stretchy="false">(</mo><mi>i</mi><mo>âˆ’</mo><mn>1</mn><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mi>d</mi></mrow></msup><mo separator="true">,</mo><mi>i</mi><mo>âˆˆ</mo><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>d</mi><mi mathvariant="normal">/</mi><mn>2</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\theta_i = 10000^{-2(i-1)/d}, i \in [1,2,...,d/2] 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">Î¸</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1324em;vertical-align:-0.1944em;"></span><span class="mord">1000</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">âˆ’</span><span class="mord mtight">2</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mbin mtight">âˆ’</span><span class="mord mtight">1</span><span class="mclose mtight">)</span><span class="mord mtight">/</span><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">âˆˆ</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">d</span><span class="mord">/2</span><span class="mclose">]</span></span></span></span></span></p>
<center>
  <img src="/img/LLM/RoPE3.png" srcset="/img/loading.gif" lazyload >
</center>
<p>ç»“åˆå‰åä»£ç ï¼Œ<code>rope_cache</code> æ˜¯ ChatGLM æ¨¡å‹çš„ <code>RotaryEmbedding</code> å­˜å‚¨çš„ cos sin cacheã€‚å…¶æœ€ä½ç»´ä¸­ï¼Œ0 å­˜å‚¨äº† cos å€¼ï¼Œ1 å­˜å‚¨äº† sin å€¼ï¼Œæ‰€ä»¥ <code>apply_rotary_pos_emb</code> çš„ 9-15 è¡Œå‡ºç°äº†ä¸€ä¸ªä»¤äººè´¹è§£çš„å¤æ‚å…¬å¼ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># ... code in __init__</span><br>rotary_dim = <span class="hljs-number">128</span><br>RotaryEmbedding(rotary_dim // <span class="hljs-number">2</span>, original_impl=config.original_rope, device=device,<br>                dtype=config.torch_dtype)<br>    theta = <span class="hljs-number">1.0</span> / (base ** (torch.arange(<span class="hljs-number">0</span>, n_elem, <span class="hljs-number">2</span>, dtype=dtype, device=device) / n_elem))<br>    seq_idx = torch.arange(seq_len, dtype=dtype, device=device)<br>    idx_theta = torch.outer(seq_idx, theta).<span class="hljs-built_in">float</span>()<br>    <span class="hljs-comment"># compute cos &amp; sin  shape[seq_len, dim//4, 2]</span><br>    cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>
<p><code>apply_rotary_pos_emb</code> å°† query tensor æœ€ä½ç»´æ‹†æˆä¸¤åŠï¼Œå‰åŠéƒ¨åˆ†å‚ä¸è¿ç®—ï¼Œå¼ é‡æ˜¯ <code>x: Shape[seq_len, bsz, num_heads, dim//2, 2]</code> å’Œ <code>rope cache: Shape[seq_len, bsz, 1, dim//2, 2]</code>ï¼Œå¯è§ cache çš„ç¬¬äºŒç»´ä¼šè¿›è¡Œ broadcastï¼Œéšåå°†ç»“æœçš„ä¸‰å››ç»´ flattenï¼Œå˜ä¸º <code>Shape[seq_len, bsz, num_heads, dim//2]</code>ï¼Œå¹¶è¿æ¥ä¸Š query tensor çš„ååŠéƒ¨ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">apply_rotary_pos_emb</span>(<span class="hljs-params">x: torch.Tensor, rope_cache: torch.Tensor</span>) -&gt; torch.Tensor:<br>    <span class="hljs-comment"># x: [sq, b, np, hn]</span><br>    sq, b, np, hn = x.size(<span class="hljs-number">0</span>), x.size(<span class="hljs-number">1</span>), x.size(<span class="hljs-number">2</span>), x.size(<span class="hljs-number">3</span>)<br>    rot_dim = rope_cache.shape[-<span class="hljs-number">2</span>] * <span class="hljs-number">2</span><br>    x, x_pass = x[..., :rot_dim], x[..., rot_dim:]<br>    <span class="hljs-comment"># truncate to support variable sizes</span><br>    rope_cache = rope_cache[:sq]<br>    xshaped = x.reshape(sq, -<span class="hljs-number">1</span>, np, rot_dim // <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>    rope_cache = rope_cache.view(sq, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, xshaped.size(<span class="hljs-number">3</span>), <span class="hljs-number">2</span>)<br>    x_out2 = torch.stack(<br>        [<br>            xshaped[..., <span class="hljs-number">0</span>] * rope_cache[..., <span class="hljs-number">0</span>] - xshaped[..., <span class="hljs-number">1</span>] * rope_cache[..., <span class="hljs-number">1</span>],<br>            xshaped[..., <span class="hljs-number">1</span>] * rope_cache[..., <span class="hljs-number">0</span>] + xshaped[..., <span class="hljs-number">0</span>] * rope_cache[..., <span class="hljs-number">1</span>],<br>        ],<br>        -<span class="hljs-number">1</span>,<br>    )<br>    x_out2 = x_out2.flatten(<span class="hljs-number">3</span>)<br>    <span class="hljs-keyword">return</span> torch.cat((x_out2, x_pass), dim=-<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>
<p>è¯»è€…å¯èƒ½è§‰å¾—å¥‡æ€ªï¼Œä¸ºä½•å‰åŠéƒ¨åˆ†çš„ query å‚ä¸äº†è¿ç®—ï¼ŒååŠéƒ¨åˆ†çš„ query ç›´æ¥å°±è¿æ¥åˆ°åé¢äº†ï¼Ÿäº‹å®ä¸Šæˆ‘ä¸€å¼€å§‹åœ¨è¯»è¿™ä¸€ä»£ç æ—¶è®¤ä¸ºè¿™æ˜¯ä¸ªbugï¼Œ<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/647085112">çŸ¥ä¹ä¸Šä¹Ÿæœ‰äººå‘ç°äº†è¿™ä¸€ç‚¹</a>ï¼Œä½†ç»ä»–äººæé†’ï¼Œè®¤ä¸ºè¿™æ˜¯ä¸€ä¸ª trick ğŸ¤¯ã€‚ä½†å…¶åº•å±‚åŸç†æ˜¯å•¥æˆ‘ä¸å¤ªæ¸…æ¥šäº†ã€‚</p>
<h4 id="RMSNorm">RMSNorm</h4>
<p>RMSNorm å½’ä¸€å±‚çš„ä½œç”¨å’ŒåŸç†ä¹Ÿåœ¨<a href="https://dingfen.github.io/ai/2023/10/30/huggingface1.html">å…ˆå‰çš„llamaåšå®¢</a>ä¸­å·²ç»ä»‹ç»è¿‡ï¼Œä»…åˆ—å‡ºç›¸åº”çš„å…¬å¼ï¼š$ RMS(a) = \sqrt{\frac{1}{n}\sum_i^n{a_i^2}} <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>ï¼Œ</mtext></mrow><annotation encoding="application/x-tex">ï¼Œ</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">ï¼Œ</span></span></span></span> \bar{a}_i = \frac{a_i}{RMS(a)}g_i$ã€‚å…¶æºä»£ç ä¸ llama çš„ä¹Ÿé«˜åº¦ç›¸ä¼¼ï¼Œä¸å†èµ˜è¿°ã€‚</p>
<h4 id="Attention">Attention</h4>
<p>ChatGLM æ¨¡å‹çš„æ ¸å¿ƒæ˜¯ Attention æœºåˆ¶ï¼Œå…¶å®ç°ä¸ llama-2 ç›¸æ¯”æœ‰å¾ˆå¤§ä¸åŒï¼Œä½†å…¶æ¶æ„æ˜¯ç›¸ä¼¼çš„ã€‚ä»ç„¶æ˜¯åœ¨å®ç°å¦‚ä¸‹å…¬å¼è®¡ç®—ï¼š</p>
<p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy="false">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex"> softmax(\frac{QK^T}{\sqrt{d_k}})V 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.4483em;vertical-align:-0.93em;"></span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183em;"><span style="top:-2.2528em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8572em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.1828em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></span></p>
<p>ä½†ä»å®ç°ä¸Šçœ‹ï¼Œæˆ‘æ€»ç»“äº†ä»¥ä¸‹å‡ ç‚¹ä¸åŒï¼š</p>
<ol>
<li>æœ‰ä¸¤ä¸ªç±»å®ç° attention æœºåˆ¶ï¼ŒSelfAttention å’Œ CoreAttention ç±»ã€‚</li>
<li>å‚ä¸è¿ç®—çš„å¼ é‡ç»´åº¦ä¸åŒã€‚ChatGLM2 å®ç°ä¸ä¸»æµçš„å¼ é‡ç»´åº¦ä¸åŒï¼Œå…¶ query å¼ é‡çš„ç»´åº¦æ˜¯ <code>(seq_len, batch_size, num_heads, head_dim)</code>ï¼Œè€Œä¸€èˆ¬è€Œè¨€å¼ é‡çš„ç»´åº¦æ˜¯ <code>(batch_size, num_heads, seq_len, head_dim)</code>ã€‚è¿™å¯¼è‡´ä»£ç æ¯”è¾ƒç¹çéš¾æ‡‚ï¼Œæˆ‘ä¹Ÿä¸ç†è§£ä¸ºä½•å®ç°è¦ä¸ä¸»æµç›¸æ‚–ã€‚</li>
</ol>
<p>ç°åœ¨ï¼Œæˆ‘ä»¬æ¥çœ‹ç±» <code>CoreAttention</code> çš„å…·ä½“å®ç°ï¼šChatGLM2 æ¨¡å‹çš„ transform å±‚ä¸­ï¼ŒMulti-Head Attention æœ‰ 32 ä¸ªå¤´ï¼Œæ¯ä¸ªå¤´çš„ç»´åº¦ä¸º 128ï¼Œå¹¶ä¸”ä½¿ç”¨äº† Grouped-Query Attention ä¼˜åŒ–æŠ€æœ¯ï¼Œå°† KV å¼ é‡åˆ†æˆäº†ä¸¤ç»„ï¼Œå‡å°‘æƒé‡çš„å†…å­˜å¼€é”€ã€‚ç‰¹åˆ«æ³¨æ„åˆ°ï¼Œä¸ llama ä¸åŒçš„æ˜¯ï¼ŒChatGLM2 æ¨¡å‹æ‰“å¼€äº† <code>query_key_layer_scaling</code>ï¼Œä»ä»£ç å®ç°ä¸Šçœ‹ï¼Œå±‚å· <code>layer_number</code> è¶Šå¤§ï¼Œ<code>coeff</code> è¶Šå¤§ï¼Œå› æ­¤ï¼Œ<code>norm_factor</code> ä¹Ÿä¼šéšä¹‹å¢å¤§ã€‚æ­¤å¤–ï¼Œè¿˜å¯ç”¨äº†æ³¨æ„åŠ›çš„ dropout å±‚ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">CoreAttention</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config: ChatGLMConfig, layer_number</span>):<br>        <span class="hljs-built_in">super</span>(CoreAttention, self).__init__()<br><br>        self.apply_query_key_layer_scaling = config.apply_query_key_layer_scaling<br>        self.attention_softmax_in_fp32 = config.attention_softmax_in_fp32<br>        <span class="hljs-keyword">if</span> self.apply_query_key_layer_scaling:<br>            self.attention_softmax_in_fp32 = <span class="hljs-literal">True</span><br>        self.layer_number = <span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>, layer_number)<br><br>        projection_size = config.kv_channels * config.num_attention_heads<br><br>        <span class="hljs-comment"># Per attention head and per partition values.</span><br>        self.hidden_size_per_partition = projection_size<br>        self.hidden_size_per_attention_head = projection_size // config.num_attention_heads<br>        self.num_attention_heads_per_partition = config.num_attention_heads<br><br>        coeff = <span class="hljs-literal">None</span><br>        self.norm_factor = math.sqrt(self.hidden_size_per_attention_head)<br>        <span class="hljs-keyword">if</span> self.apply_query_key_layer_scaling:<br>            coeff = self.layer_number<br>            self.norm_factor *= coeff<br>        self.coeff = coeff<br><br>        self.attention_dropout = torch.nn.Dropout(config.attention_dropout)<br></code></pre></td></tr></table></figure>
<p>torch&gt;=2.0 æœ‰é’ˆå¯¹ Attention æœºåˆ¶çš„ä¼˜åŒ–æ¥å£ <code>nn.functional.scaled_dot_product_attention</code>ï¼Œå› æ­¤ï¼ŒChatGLM2 æ¨¡å‹ç›´æ¥ä½¿ç”¨äº†è¯¥æ¥å£ã€‚ç”±äºä¸Šæ–‡æ‰€è¯´çš„ç¬¬ä¸€ä¸ªä¸åŒç‚¹ï¼Œåœ¨ä½¿ç”¨æ¥å£å‰ï¼Œç¬¬å››è¡Œ ä¼šå¯¹ QKV å¼ é‡è¿›è¡Œé‡æ’ã€‚è°ƒç”¨å®Œæ¥å£åï¼Œè¿˜è¦å°†ç»“æœé‡æ’å›å»ï¼Œå¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºã€‚ä½†ä»¤äººè´¹è§£çš„æ˜¯ï¼Œä¹‹å‰è¯´çš„ <code>query_key_layer_scaling</code> ä»¥åŠ dropout å±‚çš„ç‰¹æ€§å¥½åƒå¯¹ torch&gt;=2.0 ä¸èµ·ä½œç”¨ ğŸ¤”ï¼ˆå› ä¸ºä»£ç é‡Œå‹æ ¹æ²¡ç”¨ä¸Šå®ƒä»¬ï¼‰ã€‚</p>
<p>è‡³äº torch-1 çš„å¯¹æ³¨æ„åŠ›æœºåˆ¶çš„è£¸å®ç°ï¼Œemmmï¼Œä»£ç å¤ªè¿‡å¤æ‚ï¼Œå°±ç®—èŠ±æ—¶é—´å»ç†è§£ä¹Ÿæ˜¯ç™½è´¹åŠŸå¤«ã€‚æ€»è€Œè¨€ä¹‹ï¼Œ<code>CoreAttention</code> ç±»å›´ç»•ç€ torch-2 çš„ SDPA æ¥å£åŒ…è£…äº†ä¸€ä¸‹ï¼Œå°±å®Œäº‹äº†ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, query_layer, key_layer, value_layer, attention_mask</span>):<br>    pytorch_major_version = <span class="hljs-built_in">int</span>(torch.__version__.split(<span class="hljs-string">&#x27;.&#x27;</span>)[<span class="hljs-number">0</span>])<br>    <span class="hljs-keyword">if</span> pytorch_major_version &gt;= <span class="hljs-number">2</span>:<br>        query_layer, key_layer, value_layer = [k.permute(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> [query_layer, key_layer, value_layer]]<br>        <span class="hljs-keyword">if</span> attention_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> query_layer.shape[<span class="hljs-number">2</span>] == key_layer.shape[<span class="hljs-number">2</span>]:<br>            context_layer = torch.nn.functional.scaled_dot_product_attention(query_layer, key_layer, value_layer,<br>                                                                                is_causal=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> attention_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                attention_mask = ~attention_mask<br>            context_layer = torch.nn.functional.scaled_dot_product_attention(query_layer, key_layer, value_layer,<br>                                                                                attention_mask)<br>        context_layer = context_layer.permute(<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>)<br>        new_context_layer_shape = context_layer.size()[:-<span class="hljs-number">2</span>] + (self.hidden_size_per_partition,)<br>        context_layer = context_layer.reshape(*new_context_layer_shape)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">pass</span><br></code></pre></td></tr></table></figure>
<p>å†æ¥çœ‹ <code>SelfAttention</code> çš„å®ç°ï¼ŒSelfAttention ç±»åŒ…å«äº†ä¸‰ä¸ªç»„æˆæˆåˆ†ï¼šä¸€ï¼‰å°†è¾“å…¥å¼ é‡è½¬æ¢æˆ QKV å¼ é‡çš„çº¿æ€§å±‚ï¼ŒäºŒï¼‰CoreAttention ç±»ï¼Œå®ç°æ³¨æ„åŠ›è®¡ç®—ã€‚ä¸‰ï¼‰å°† CoreAttention çš„ SDPA ç»“æœè½¬æ¢æˆè¾“å‡ºå¼ é‡çš„çº¿æ€§å±‚ã€‚å…¶ä¸­ï¼Œçº¿æ€§å±‚çš„è¾“å…¥ç»´åº¦æ˜¯ <code>config.hidden_size</code>ï¼Œè¾“å‡ºç»´åº¦æ˜¯ <code>config.projection_size</code>ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SelfAttention</span>(torch.nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;Parallel self-attention layer abstract class.</span><br><span class="hljs-string">    Self-attention layer takes input with size [s, b, h]</span><br><span class="hljs-string">    and returns output of the same size.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config: ChatGLMConfig, layer_number, device=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>(SelfAttention, self).__init__()<br>        self.layer_number = <span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>, layer_number)<br>        <span class="hljs-comment"># ......</span><br>        self.multi_query_attention = config.multi_query_attention<br>        self.qkv_hidden_size = <span class="hljs-number">3</span> * self.projection_size<br>        <span class="hljs-keyword">if</span> self.multi_query_attention:<br>            self.num_multi_query_groups_per_partition = config.multi_query_group_num<br>            self.qkv_hidden_size = (<br>                    self.projection_size + <span class="hljs-number">2</span> * self.hidden_size_per_attention_head * config.multi_query_group_num<br>            )<br>        self.query_key_value = nn.Linear(config.hidden_size, self.qkv_hidden_size,<br>                                         bias=config.add_bias_linear <span class="hljs-keyword">or</span> config.add_qkv_bias,<br>                                         device=device, **_config_to_kwargs(config)<br>                                         )<br><br>        self.core_attention = CoreAttention(config, self.layer_number)<br><br>        <span class="hljs-comment"># Output.</span><br>        self.dense = nn.Linear(self.projection_size, config.hidden_size, bias=config.add_bias_linear,<br>                               device=device, **_config_to_kwargs(config)<br>                               )<br></code></pre></td></tr></table></figure>
<p>å…¶ forward å‡½æ•°çš„è¿ç®—è¿‡ç¨‹ä¹Ÿå¯ä»¥åˆ†æˆä¸‰éƒ¨åˆ†ï¼š</p>
<ul>
<li>å‡†å¤‡ QKV å¼ é‡ã€‚è¾“å…¥å¼ é‡ <code>hidden_states</code> ç»è¿‡çº¿æ€§å±‚åï¼Œå¾—åˆ° QKV æ··åˆå¼ é‡ï¼Œéšåæ‹†åˆ†ï¼š<code>mixed_x_layer</code> çš„æœ€åä¸€ç»´åº¦æ˜¯ <code>hidden_size + 2 * KV_dim</code>ï¼Œå› ä¸ºä½¿ç”¨äº† Grouped-Query Attention ä¼˜åŒ–æŠ€æœ¯åï¼ŒKey Value å¼ é‡çš„å¤´æ•°å˜å°‘äº†ã€‚å¯¹åº”åœ°ï¼Œ7-9 è¡Œä¹Ÿæ˜¯æŒ‰ç…§ <code>hidden_size + 2 * KV_dim</code> æ‹†åˆ†å‡º QKV å¼ é‡ã€‚</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self, hidden_states, attention_mask, rotary_pos_emb, kv_cache=<span class="hljs-literal">None</span>, use_cache=<span class="hljs-literal">True</span></span><br><span class="hljs-params"></span>):<br>    <span class="hljs-comment"># hidden_states: [sq, b, h]</span><br><br>    <span class="hljs-comment"># =================================================</span><br>    <span class="hljs-comment"># Pre-allocate memory for key-values for inference.</span><br>    <span class="hljs-comment"># =================================================</span><br>    <span class="hljs-comment"># =====================</span><br>    <span class="hljs-comment"># Query, Key, and Value</span><br>    <span class="hljs-comment"># =====================</span><br><br>    <span class="hljs-comment"># Attention heads [sq, b, h] --&gt; [sq, b, (np * 3 * hn)]</span><br>    mixed_x_layer = self.query_key_value(hidden_states)<br><br>    <span class="hljs-keyword">if</span> self.multi_query_attention:<br>        (query_layer, key_layer, value_layer) = mixed_x_layer.split(<br>            [<br>                self.num_attention_heads_per_partition * self.hidden_size_per_attention_head,<br>                self.num_multi_query_groups_per_partition * self.hidden_size_per_attention_head,<br>                self.num_multi_query_groups_per_partition * self.hidden_size_per_attention_head,<br>            ],<br>            dim=-<span class="hljs-number">1</span>,<br>        )<br></code></pre></td></tr></table></figure>
<ul>
<li>å°† QKV å¼ é‡è½¬å˜ä¸º <code>Shape[sq, b, np, hn]</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">    query_layer = query_layer.view(<br>        query_layer.size()[:-<span class="hljs-number">1</span>] + (self.num_attention_heads_per_partition, self.hidden_size_per_attention_head)<br>    )<br>    key_layer = key_layer.view(<br>        key_layer.size()[:-<span class="hljs-number">1</span>] + (self.num_multi_query_groups_per_partition, self.hidden_size_per_attention_head)<br>    )<br>    value_layer = value_layer.view(<br>        value_layer.size()[:-<span class="hljs-number">1</span>]<br>        + (self.num_multi_query_groups_per_partition, self.hidden_size_per_attention_head)<br>    )<br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-comment"># ... no MQA part</span><br></code></pre></td></tr></table></figure>
<ul>
<li>å‡†å¤‡ RoPEï¼Œå’Œ KV cacheã€‚QKV å…ˆåŠ å…¥æ—‹è½¬ä½ç½®ç¼–ç ï¼Œå†è¿æ¥ä¸Šä¹‹å‰çš„ KV cache å¼ é‡ã€‚</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># apply relative positional encoding (rotary embedding)</span><br><span class="hljs-keyword">if</span> rotary_pos_emb <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>    query_layer = apply_rotary_pos_emb(query_layer, rotary_pos_emb)<br>    key_layer = apply_rotary_pos_emb(key_layer, rotary_pos_emb)<br><br><span class="hljs-comment"># adjust key and value for inference</span><br><span class="hljs-keyword">if</span> kv_cache <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>    cache_k, cache_v = kv_cache<br>    key_layer = torch.cat((cache_k, key_layer), dim=<span class="hljs-number">0</span>)<br>    value_layer = torch.cat((cache_v, value_layer), dim=<span class="hljs-number">0</span>)<br><span class="hljs-keyword">if</span> use_cache:<br>    kv_cache = (key_layer, value_layer)<br><span class="hljs-keyword">else</span>:<br>    kv_cache = <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure>
<ul>
<li>Grouped-Query Attention ä½¿å¾— KV å¼ é‡çš„æ•°é‡åªæœ‰ <code>num_multi_query_groups_per_partition</code>ï¼ˆä¸¤ä¸ªï¼‰ï¼Œä½†å´è¦å¯¹åº” <code>num_attention_heads_per_partition</code> ä¸ª Queryï¼Œå› æ­¤ä»£ç æœ€åè¦å°† KV å¼ é‡ä½¿ç”¨ <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html">torch.expand</a> å°†ç¬¬ä¸‰ç»´æ‰©å±•ã€‚</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> self.multi_query_attention:<br>    key_layer = key_layer.unsqueeze(-<span class="hljs-number">2</span>)<br>    key_layer = key_layer.expand(<br>        -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, self.num_attention_heads_per_partition // self.num_multi_query_groups_per_partition, -<span class="hljs-number">1</span><br>    )<br>    key_layer = key_layer.contiguous().view(<br>        key_layer.size()[:<span class="hljs-number">2</span>] + (self.num_attention_heads_per_partition, self.hidden_size_per_attention_head)<br>    )<br>    value_layer = value_layer.unsqueeze(-<span class="hljs-number">2</span>)<br>    value_layer = value_layer.expand(<br>        -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, self.num_attention_heads_per_partition // self.num_multi_query_groups_per_partition, -<span class="hljs-number">1</span><br>    )<br>    value_layer = value_layer.contiguous().view(<br>        value_layer.size()[:<span class="hljs-number">2</span>] + (self.num_attention_heads_per_partition, self.hidden_size_per_attention_head)<br>    )<br></code></pre></td></tr></table></figure>
<ul>
<li>æ‰§è¡Œæ³¨æ„åŠ›è®¡ç®—å’Œæœ€åçš„è¾“å‡ºçº¿æ€§å±‚è®¡ç®—ï¼Œè¿™é‡Œçš„ä»£ç å°±å¾ˆç®€å•äº†ã€‚</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># ==================================</span><br><span class="hljs-comment"># core attention computation</span><br><span class="hljs-comment"># ==================================</span><br><br>context_layer = self.core_attention(query_layer, key_layer, value_layer, attention_mask)<br><br><span class="hljs-comment"># =================</span><br><span class="hljs-comment"># Output. [sq, b, h]</span><br><span class="hljs-comment"># =================</span><br><br>output = self.dense(context_layer)<br><br><span class="hljs-keyword">return</span> output, kv_cache<br></code></pre></td></tr></table></figure>
<h4 id="MLP">MLP</h4>
<p>ChatGLM2 çš„ MLP å±‚ä¸ llama çš„ç•¥æœ‰ä¸åŒã€‚ä½†éƒ½æœ‰ä¸¤å±‚å…¨è¿æ¥å±‚ï¼Œä»¥åŠä¸­é—´çš„æ¿€æ´»å±‚ swigluã€‚ä»£ç æ¯”è¾ƒå¹³å‡¡ï¼Œå°±ä¸ç»†è¯´äº†ã€‚</p>
<p><img src="/img/LLM/ChatGLM2MLP.png" srcset="/img/loading.gif" lazyload alt=""></p>
<h3 id="ChatGLM2-ä¸­é—´ä»¶">ChatGLM2 ä¸­é—´ä»¶</h3>
<p>ChatGLM2 ä¸­é—´ä»¶çš„æ ¸å¿ƒæ˜¯ <code>GLMBlock</code>ï¼Œ<code>GLMBlock</code> åŒ…å«äº†ä¸‰ä¸ªç»„æˆæˆåˆ†ï¼šä¸€ï¼‰åœ¨æ³¨æ„åŠ›å‰åçš„ LayerNorm å±‚ï¼ŒäºŒï¼‰æ³¨æ„åŠ›å±‚ï¼Œä¸‰ï¼‰åœ¨æœ€åçš„ MLP å±‚ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">GLMBlock</span>(torch.nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;A single transformer layer.</span><br><span class="hljs-string">    Transformer layer takes input with size [s, b, h] and returns an</span><br><span class="hljs-string">    output of the same size.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config: ChatGLMConfig, layer_number, device=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-comment"># ...</span><br>        LayerNormFunc = RMSNorm <span class="hljs-keyword">if</span> config.rmsnorm <span class="hljs-keyword">else</span> LayerNorm<br>        <span class="hljs-comment"># Layernorm on the input data.</span><br>        self.input_layernorm = LayerNormFunc(config.hidden_size, eps=config.layernorm_epsilon, device=device,<br>                                             dtype=config.torch_dtype)<br><br>        <span class="hljs-comment"># Self attention.</span><br>        self.self_attention = SelfAttention(config, layer_number, device=device)<br>        self.hidden_dropout = config.hidden_dropout<br><br>        <span class="hljs-comment"># Layernorm on the attention output</span><br>        self.post_attention_layernorm = LayerNormFunc(config.hidden_size, eps=config.layernorm_epsilon, device=device,<br>                                                      dtype=config.torch_dtype)<br><br>        <span class="hljs-comment"># MLP</span><br>        self.mlp = MLP(config, device=device)<br></code></pre></td></tr></table></figure>
<p>å¤šä¸ª <code>GLMBlock</code> å †å èµ·æ¥ï¼Œç»„æˆä¸€ä¸ª <code>GLMTransformer</code> å±‚ã€‚<code>GLMTransformer</code> å±‚çš„æœ€åè¿˜ä¼šé¢å¤–é™„ä¸Šä¸€å±‚ LayerNormï¼Œè¿™éƒ¨åˆ†ä»£ç å¹³å‡¡ã€‚</p>
<p><code>GLMBlock</code> çš„ forward å‡½æ•°å¾ˆå¥½åœ°æè¿°äº† <code>GLMBlock</code> æ¶æ„ä»¥åŠæ•°æ®æµå‘ï¼šå…ˆæ˜¯ layernorm å±‚ï¼Œå†æ˜¯æ³¨æ„åŠ›å±‚ï¼Œæ—è·¯ä½¿ç”¨æ®‹å·®ç»“æ„ï¼Œéšåè·Ÿä¸Š dropout å±‚å’Œ Layernorm å±‚ï¼Œæœ€åæ˜¯ MLP å±‚å’Œ dropout å±‚ï¼Œè¿™äº›æ•°æ®é€šè·¯åœ¨ä¹‹å‰çš„ ChatGLM2 æ¶æ„å›¾ä¸Šæç»˜å¾—å¾ˆæ¸…æ¥šï¼Œè¯»è€…å¯å‚è§<a target="_blank" rel="noopener" href="https://huggingface.co/THUDM/chatglm2-6b/blob/main/modeling_chatglm.py#L536">ä»£ç </a>ã€‚</p>
<p><code>GLMTransformer</code> ç±»åˆ™æ˜¯å°†è‹¥å¹²ä¸ª <code>GLMBlock</code> åšå †å å°è£…ï¼Œå¹¶åœ¨ç½‘ç»œçš„æœ€ååŠ å…¥ä¸€å±‚ layernorm å±‚ã€‚å…¶ forward å‡½æ•°ä¸­åŒ…å«äº† checkpoint ç›¸å…³çš„ä»£ç ğŸ‘‡ï¼Œä¹‹å‰åœ¨ <a href="https://dingfen.github.io/ai/2023/11/30/huggingface2.html">llama è¯¦è§£çš„åšå®¢</a>ä¸­æˆ‘ä»¬æåˆ°è¿‡ï¼Œä½¿ç”¨ gradient_checkpointing å¯ä»¥æœ‰æ•ˆèŠ‚çº¦æ˜¾å­˜ï¼Œå› ä¸ºå‰æ¨æ—¶ç¨‹åºä¸ä¿å­˜ä¸­é—´è®¡ç®—å€¼ã€‚è€Œ use_cache æ˜¯é€šè¿‡æ¶ˆè€—ç©ºé—´æ¢æ—¶é—´ã€‚å› æ­¤è‹¥ gradient_checkpointing å’Œ use_cache éƒ½æ‰“å¼€ï¼Œé‚£ä¹ˆä¸¤è€…å¯èƒ½ä¼šäº’ç›¸æŠµæ¶ˆä¼˜åŒ–å½±å“ï¼Œæ•…æ¨¡å‹ä¸æ”¯æŒä¸¤è€…åŒæ—¶ä¸º Trueã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self, hidden_states, attention_mask, rotary_pos_emb, kv_caches=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        use_cache: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">True</span>,</span><br><span class="hljs-params">        output_hidden_states: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> kv_caches:<br>        kv_caches = [<span class="hljs-literal">None</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.num_layers)]<br>    presents = () <span class="hljs-keyword">if</span> use_cache <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">if</span> self.gradient_checkpointing <span class="hljs-keyword">and</span> self.training:<br>        <span class="hljs-keyword">if</span> use_cache:<br>            logger.warning_once(<br>                <span class="hljs-string">&quot;`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...&quot;</span><br>            )<br>            use_cache = <span class="hljs-literal">False</span><br>    <span class="hljs-comment"># ...</span><br></code></pre></td></tr></table></figure>
<h3 id="ChatGLM2-Model">ChatGLM2 Model</h3>
<p><code>ChatGLMPreTrainedModel</code> ç±»æ˜¯ä¸€ä¸ªåŸºæŠ½è±¡ç±»ï¼Œè´Ÿè´£å¤„ç† mask å’Œ position_id ç­‰ã€‚<code>ChatGLMModel</code> ç»§æ‰¿äº†å®ƒï¼Œ<code>ChatGLMModel</code> èƒ½å›Šæ‹¬äº† <code>GLMTransformer</code> ç±»ï¼Œä»¥åŠè¾“å‡ºå±‚ output layerï¼›å¢åŠ äº†å¯¹è¾“å…¥ <code>input_ids</code> ä½¿ç”¨ torch.nn.Embedding åµŒå…¥ç¼–ç ï¼Œä»¥åŠå¯¹å†å²å¯¹è¯ä½¿ç”¨ <code>PrefixEncoder</code> ç¼–ç çš„ç¯èŠ‚ç­‰ã€‚</p>
<h4 id="ChatGLMPreTrainedModel">ChatGLMPreTrainedModel</h4>
<p>å¤„ç† position_idï¼šæ ¹æ®è¾“å…¥çš„ input_ids çš„å½¢çŠ¶ï¼š(batch_size, seq_len)ï¼Œæ¯ä¸ªbatchéƒ½ä»é›¶å¼€å§‹é€’å¢æ ‡å·ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_position_ids</span>(<span class="hljs-params">self, input_ids, device</span>):<br>    batch_size, seq_length = input_ids.shape<br>    position_ids = torch.arange(seq_length, dtype=torch.long, device=device).<br>    unsqueeze(<span class="hljs-number">0</span>).repeat(batch_size, <span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> position_ids<br></code></pre></td></tr></table></figure>
<p>å¤„ç† maskï¼šattention mask åº”å½“æ˜¯ä¸€ä¸ªä¸‹ä¸‰è§’çŸ©é˜µï¼Œ</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_masks</span>(<span class="hljs-params">self, input_ids, past_key_values, padding_mask=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-comment"># é¦–å…ˆåˆå§‹åŒ–å•ä½é˜µ ç»´åº¦ (bsz, seq_len, seq_len)ï¼Œå¹¶å– tril_</span><br>    batch_size, seq_length = input_ids.shape<br>    full_attention_mask = torch.ones(batch_size, seq_length, seq_length, device=input_ids.device)<br>    full_attention_mask.tril_()<br><br>    <span class="hljs-comment"># æ¥ä¸Š KV cache çš„ attention mask</span><br>    <span class="hljs-keyword">if</span> past_key_values:<br>        past_length = past_key_values[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>].shape[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">if</span> past_length:<br>        full_attention_mask = torch.cat((torch.ones(batch_size, seq_length, past_length,<br>                                    device=input_ids.device), full_attention_mask), dim=-<span class="hljs-number">1</span>)<br>    <span class="hljs-comment"># padding_mask æ˜¯ bool tensorï¼Œä¸¤è€…ç›¸ä¹˜åï¼Œå°äº0.5çš„ä¸º false</span><br>    <span class="hljs-keyword">if</span> padding_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        full_attention_mask = full_attention_mask * padding_mask.unsqueeze(<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> past_length <span class="hljs-keyword">and</span> padding_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        full_attention_mask -= padding_mask.unsqueeze(-<span class="hljs-number">1</span>) - <span class="hljs-number">1</span><br>    full_attention_mask = (full_attention_mask &lt; <span class="hljs-number">0.5</span>).<span class="hljs-built_in">bool</span>()<br>    full_attention_mask.unsqueeze_(<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> full_attention_mask<br></code></pre></td></tr></table></figure>
<hr>
<p><code>ChatGLMForConditionalGeneration</code> ç±»å’Œ <code>ChatGLMForSequenceClassification</code> ç±»æ˜¯ç”¨æˆ·ï¼ˆæŒ‡ transformeræ¡†æ¶ï¼‰çœŸæ­£è°ƒç”¨åˆ°çš„æ¨¡å‹ï¼Œä»–ä»¬ä¿©å†…éƒ¨éƒ½åŒ…å«äº† <code>ChatGLMModel</code> ç±»ã€‚</p>
<p>ä»æ¶æ„å±‚é¢çœ‹ï¼Œ<code>ChatGLMForConditionalGeneration</code> åœ¨ ChatGLM æ¨¡å‹çš„æœ€åä¸Šå¢åŠ äº†æŸå¤±å‡½æ•° <code>CrossEntropyLoss</code>ï¼Œä»¥ä¾¿è®­ç»ƒï¼›è€Œ <code>ChatGLMForSequenceClassification</code> ç±»åˆ™åœ¨æœ€åå¢åŠ äº†ä¸€ä¸ªçº¿æ€§å±‚å’Œ dropout å±‚ç”¨äº sequence classificationï¼Œå½“ç„¶è¯¥ç±»æœ€åä¹Ÿä¼šè°ƒç”¨æŸå¤±å‡½æ•° <code>CrossEntropyLoss</code> å’Œ <code>BCEWithLogitsLoss</code>ã€‚</p>
<p>åœ¨ç”¨æˆ·ä¸å¤§æ¨¡å‹å‘ç”Ÿäº¤è°ˆæ—¶ï¼ŒChatGLM æ¨¡å‹ä¼šä½¿ç”¨ transformer æ¶æ„ä¸‹æ§åˆ¶ token ç”Ÿæˆçš„å·¥å…·ï¼šlogits processorã€‚å®ƒå¯ä»¥é€šè¿‡æ§åˆ¶ logits æ¦‚ç‡ï¼Œæ”¹å˜å¤§æ¨¡å‹åå‡ºçš„ tokenã€‚ä¸€ä¸ªå¾ˆå¸¸ç”¨çš„åœºæ™¯æ˜¯æ§åˆ¶å¤§æ¨¡å‹ä¸åå‡ºâ€œæ•æ„Ÿè¯â€ï¼Œå¯ä»¥åŠ ä¸€ä¸ªæ•æ„Ÿè¯åˆ—è¡¨ï¼Œè®©å¤§æ¨¡å‹åœ¨åå‡ºè¯¥è¯å‰æ¢ä¸€ä¸ªè¯æˆ–è€…å¹²è„†ä¸äº§ç”Ÿæ•æ„Ÿè¯ã€‚<a target="_blank" rel="noopener" href="https://blog.csdn.net/yjh_SE007/article/details/132259230">è¿™ç¯‡åšå®¢</a>é‡Œä»‹ç»äº†è‡ªå®šä¹‰ LogitsProcessor çš„æ–¹æ³•ã€‚</p>
<h3 id="æ€»ç»“">æ€»ç»“</h3>
<p>è‡³æ­¤ï¼Œæˆ‘æœ‰è¯¦æœ‰ç•¥åœ°ç»™å¤§å®¶å±•ç¤ºäº† chatglm2 æ¨¡å‹çš„æ¶æ„å’Œä»£ç å®ç°ã€‚æ€»çš„æ¥è¯´ï¼Œè¯¥æ¨¡å‹æ¶æ„åŸºæœ¬ç›¸ä¼¼äºå…¶ä»–ä¸»æµçš„å¤§æ¨¡å‹ã€‚åœ¨çœ‹è¿‡ llama æ¨¡å‹çš„å…·ä½“å®ç°åï¼Œchatglm2 ç»™æˆ‘ä¸€ç§å¾ˆç†Ÿæ‚‰åˆé™Œç”Ÿçš„æ„Ÿè§‰ã€‚ç†Ÿæ‚‰åœ¨äºå…¶æ¶æ„å’Œæ•°æ®é€šè·¯å±‚é¢çš„ç›¸ä¼¼æ„Ÿï¼Œè€Œé™Œç”Ÿåœ¨äº chatglm2 æ¨¡å‹åœ¨å…·ä½“è®¡ç®—æ³¨æ„åŠ›æ—¶é‡‡ç”¨äº†ä¸åŒå¯»å¸¸çš„å¼ é‡å½¢çŠ¶ï¼Œä¸ä»…ä¸å…¶ä»–å¤§æ¨¡å‹ç›¸å¼‚ï¼Œä¹Ÿä¸ torch çš„æ³¨æ„åŠ›å‡½æ•°æ¥å£ä¸åŒã€‚</p>
<p>ä»ä½¿ç”¨ä½“éªŒä¸Šè¯´ï¼ŒChatGLM2 æ˜¯ä¸€æ¬¾éå¸¸ä¸é”™çš„ä¸­è‹±æ–‡å¯¹è¯å¤§æ¨¡å‹ï¼Œå‡ è½®å¯¹è¯ä¸‹æ¥åŸºæœ¬ç¬¦åˆé¢„æœŸã€‚å…¶6Bçš„å¤§å°å¯¹æ–°æ‰‹å’Œä¸ªäººç©å®¶å‹å¥½ï¼Œå€¼å¾—æœ‰æ¡ä»¶çš„å„ä½è¯•è¯•ğŸ˜„ã€‚</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/AI/" class="category-chain-item">AI</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/AI/" class="print-no-link">#AI</a>
      
        <a href="/tags/Transformer/" class="print-no-link">#Transformer</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>THUDM/chatglmä»£ç ç»†è¯»</div>
      <div>https://dingfen.github.io/2024/01/27/2024-1-27-huggingface3/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>ä½œè€…</div>
          <div>Bill Ding</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>å‘å¸ƒäº</div>
          <div>2024å¹´1æœˆ27æ—¥</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>æ›´æ–°äº</div>
          <div>2025å¹´1æœˆ26æ—¥</div>
        </div>
      
      
        <div class="license-meta-item">
          <div>è®¸å¯åè®®</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - ç½²å">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="SA - ç›¸åŒæ–¹å¼å…±äº«">
                    <i class="iconfont icon-sa"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/04/10/2024-4-10-hello-world/" title="å°†åšå®¢åˆ‡æ¢åˆ° hexo Fluid ä¸»é¢˜">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">å°†åšå®¢åˆ‡æ¢åˆ° hexo Fluid ä¸»é¢˜</span>
                        <span class="visible-mobile">ä¸Šä¸€ç¯‡</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/11/30/2023-11-30-LLM-KVCache/" title="å¤§æ¨¡å‹æ¨ç†ä¼˜åŒ–æŠ€æœ¯ä¹‹æ˜¾å­˜ä¼˜åŒ–">
                        <span class="hidden-mobile">å¤§æ¨¡å‹æ¨ç†ä¼˜åŒ–æŠ€æœ¯ä¹‹æ˜¾å­˜ä¼˜åŒ–</span>
                        <span class="visible-mobile">ä¸‹ä¸€ç¯‡</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>ç›®å½•</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  








    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">æœç´¢</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">å…³é”®è¯</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        æ€»è®¿é—®é‡ 
        <span id="busuanzi_value_site_pv"></span>
         æ¬¡
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        æ€»è®¿å®¢æ•° 
        <span id="busuanzi_value_site_uv"></span>
         äºº
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  var relativeDate = function() {
    var updatedTime = document.getElementById('updated-time');
    if (updatedTime) {
      var text = updatedTime.textContent;
      var reg = /\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:Z|[+-]\d{2}:\d{2})/;
      var matchs = text.match(reg);
      if (matchs) {
        var relativeTime = moment(matchs[0]).fromNow();
        updatedTime.textContent = text.replace(reg, relativeTime);
      }
      updatedTime.style.display = '';
    }
  };
  Fluid.utils.createScript('https://lib.baomitu.com/moment.js/2.29.4/moment.min.js', function() {
    if (!'zh-cn'.startsWith('en')) {
      Fluid.utils.createScript('https://lib.baomitu.com/moment.js/2.29.4/locale/zh-cn.min.js', function() {
        relativeDate();
      });
    } else {
      relativeDate();
    }
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  
<script src="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/Ribbon.min.js"></script>



<!-- ä¸»é¢˜çš„å¯åŠ¨é¡¹ï¼Œå°†å®ƒä¿æŒåœ¨æœ€åº•éƒ¨ -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">åšå®¢åœ¨å…è®¸ JavaScript è¿è¡Œçš„ç¯å¢ƒä¸‹æµè§ˆæ•ˆæœæ›´ä½³</div>
  </noscript>
<!-- hexo injector body_end start -->
  <link rel="stylesheet" crossorigin href="https://g.alicdn.com/aliyun-documentation/web-chatbot-ui/0.0.11/index.css" />
  <script type="module" crossorigin src="https://g.alicdn.com/aliyun-documentation/web-chatbot-ui/0.0.11/index.js"></script>
  <script>
    window.CHATBOT_CONFIG = {
      endpoint: "https://web-chatbot-syz-knthhrjfeq.cn-hangzhou.fcapp.run/chat", // å¯ä»¥æ›¿æ¢ä¸º https://{your-fc-http-trigger-domain}/chat
      displayByDefault: false, // é»˜è®¤ä¸æ˜¾ç¤º AI åŠ©æ‰‹å¯¹è¯æ¡†
      aiChatOptions: { // è‡ªå®šä¹‰å–å€¼å‚è€ƒï¼šhttps://docs.nlkit.com/nlux/reference/ui/ai-chat#conversation-options
        conversationOptions: { // è‡ªå®šä¹‰å–å€¼å‚è€ƒï¼šhttps://docs.nlkit.com/nlux/reference/ui/ai-chat#conversation-options
          conversationStarters: [
            {prompt: 'è¯·é—®ä½ æ˜¯è°ï¼Œèƒ½ä¸ºæˆ‘åšä»€ä¹ˆï¼Ÿ'},
            {prompt: 'è¯·ä»‹ç»ä¸€ä¸‹åšå®¢çš„ä¸»äºº'}
          ],
          layout: 'bubbles'
        },
        displayOptions: { // è‡ªå®šä¹‰å–å€¼å‚è€ƒï¼šhttps://docs.nlkit.com/nlux/reference/ui/ai-chat#display-options
          height: 550,
          // width: 400,
        },
        personaOptions: { // è‡ªå®šä¹‰å–å€¼å‚è€ƒï¼šhttps://docs.nlkit.com/nlux/reference/ui/ai-chat#chat-personas
          assistant: {
            name: 'ä½ å¥½ï¼Œæˆ‘æ˜¯æœ¬ç½‘ç«™çš„ AI åŠ©æ‰‹',
            // AI åŠ©æ‰‹çš„å›¾æ ‡
            avatar: 'https://img.alicdn.com/imgextra/i2/O1CN01Pda9nq1YDV0mnZ31H_!!6000000003025-54-tps-120-120.apng',
            tagline: 'è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œæˆ‘ä¼šå°½åŠ›å¸®ä½ è§£ç­”ï¼',
          }
        }
      }
    };
  </script>
  <style>
    :root {
      /* webchat å·¥å…·æ çš„é¢œè‰² */
      --webchat-toolbar-background-color: #1464E4;
      /* webchat å·¥å…·æ æ–‡å­—å’ŒæŒ‰é’®çš„é¢œè‰² */
      --webchat-toolbar-text-color: #FFF;
    }
    /* webchat å¯¹è¯æ¡†å¦‚æœè¢«é®æŒ¡ï¼Œå¯ä»¥å°è¯•é€šè¿‡ z-indexã€bottomã€right ç­‰è®¾ç½® æ¥è°ƒæ•´*/
    .webchat-container {
      z-index: 100;
      bottom: 10px;
      right: 10px;
    }
    /* webchat çš„å”¤èµ·æŒ‰é’®å¦‚æœè¢«é®æŒ¡ï¼Œå¯ä»¥å°è¯•é€šè¿‡ z-indexã€bottomã€right ç­‰è®¾ç½® æ¥è°ƒæ•´ã€‚ä¹Ÿå¯ä»¥é€šè¿‡ CSS è¿›ä¸€æ­¥å®šåˆ¶å”¤èµ·æŒ‰é’®çš„å½¢çŠ¶ã€å¤§å°ç­‰ã€‚ */
    .webchat-bubble-tip {
      z-index: 99;
      bottom: 20px;
      right: 20px;
    }
  </style>
  <!-- hexo injector body_end end --></body>
</html>
