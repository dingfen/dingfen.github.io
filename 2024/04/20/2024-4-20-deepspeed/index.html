

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Bill Ding">
  <meta name="keywords" content="">
  
    <meta name="description" content="加速深度学习推理和训练的新引擎">
<meta property="og:type" content="article">
<meta property="og:title" content="深入探索 deepspeed（一）">
<meta property="og:url" content="https://dingfen.github.io/2024/04/20/2024-4-20-deepspeed/index.html">
<meta property="og:site_name" content="峰子的乐园">
<meta property="og:description" content="加速深度学习推理和训练的新引擎">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://dingfen.github.io/img/LLM/deepspeed_logo.png">
<meta property="article:published_time" content="2024-04-20T02:00:00.000Z">
<meta property="article:modified_time" content="2025-01-26T11:49:09.209Z">
<meta property="article:author" content="Bill Ding">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="deepspeed">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://dingfen.github.io/img/LLM/deepspeed_logo.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>深入探索 deepspeed（一） - 峰子的乐园</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/KaTeX/0.16.2/katex.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"dingfen.github.io","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"|","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":null,"onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.1.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>峰子的乐园</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/LLM/deepspeed-four.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="深入探索 deepspeed（一）"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-04-20 10:00" pubdate>
          2024年4月20日 上午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          4.2k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          36 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">深入探索 deepspeed（一）</h1>
            
              <p id="updated-time" class="note note-info" style="display: none">
                
                  
                    <!-- compatible with older versions-->
                    更新于：2025-01-26T19:49:09+08:00
                  
                  

                
              </p>
            
            
              <div class="markdown-body">
                
                <p>推理和训练大模型通常需要巨大的计算资源和时间。微软推出 <a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepSpeed">DeepSpeed</a> 深度学习优化库，旨在解决这一问题。从本篇博客中，我们将深入了解 deepspeed，并理解微软的工程师们是如何通过对内存、并行度、通信的优化，从而极大地加速了大模型的推理和训练过程。</p>
<p>注：本篇博文的源码分析基于 deepspeed-0.14.2。</p>
<h1>DeepSpeed 简介</h1>
<p>DeepSpeed 是一个专门为深度学习模型训练设计的优化库。它实现了 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.02054">ZeRO 论文</a>中描述的所有内容。它是目前开源社区中广泛使用的训练大模型的框架。一般地，它支持以下三个阶段（stage）：</p>
<ul>
<li>ZeRO stage 1：优化器状态分区</li>
<li>ZeRO stage 2：梯度分区</li>
<li>ZeRO stage 3：参数分区</li>
</ul>
<p>上述三个阶段给了用户更多更灵活的训练选择。用户可参考<a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/deepspeed#select-a-zero-stage">官方文档</a>，选择适配自己硬件条件的训练方式。一般而言，花费的内存越少，训练时间越长；花费的内存越多，训练时间越短。</p>
<table>
<thead>
<tr>
<th>Fastest</th>
<th>Memory efficient</th>
</tr>
</thead>
<tbody>
<tr>
<td>ZeRO-1</td>
<td>ZeRO-3 + offload</td>
</tr>
<tr>
<td>ZeRO-2</td>
<td>ZeRO-3</td>
</tr>
<tr>
<td>ZeRO-2 + offload</td>
<td>ZeRO-2 + offload</td>
</tr>
<tr>
<td>ZeRO-3</td>
<td>ZeRO-2</td>
</tr>
<tr>
<td>ZeRO-3 + offload</td>
<td>ZeRO-1</td>
</tr>
</tbody>
</table>
<p>此外，它还支持了以下功能：</p>
<ul>
<li>自定义混合精度训练处理。使用类似 PyTorch AMP 的方式，也可以选择使用类似 Apex 的方式</li>
<li>基于 CUDA 扩展的快速优化器。主要优化器包括 Adam、AdamW、OneBitAdam 和 Lamb</li>
<li>将部分训练参数卸载到 CPU 主存或者 SSD 上，适合于显存空间不足的用户。详细可参考<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2101.06840">ZeRO-Offload</a> 到 CPU 和 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.07857.pdf">NVMe</a>这两篇论文。</li>
</ul>
<h1>deepspeed 之大模型推理</h1>
<p>网络上使用 deepspeed 做训练的博客汗牛充栋，但使用它做推理的博客就比较少，因此我先从推理开始探索 deepspeed 的内部机制。从<a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/blog/deepspeed-accelerating-large-scale-model-inference-and-training-via-system-optimizations-and-compression/">官网博客上</a>可以了解到，deepspeed 推理有如下几个特点：</p>
<ol>
<li>deepspeed 将推理的多个算子融合为单一的算子 kernel，从而减少 kernel 间的启动开销和访问内存的延迟。与 JIT、XLA 或其他项目的算子融合相比，deepspeed 的算子融合力度更猛，它融合了element-wise、矩阵乘、转置、归约到一个 kernel。与未融合相比，以上几个部分的加速比可分别达到 1.5x, 2.9x, 3x, 1.2x</li>
</ol>
<p><img src="https://www.microsoft.com/en-us/research/uploads/prod/2021/05/Fig1_DeepSpeed5_Blog.jpg" srcset="/img/loading.gif" lazyload alt=""></p>
<ol start="2">
<li>
<p>为推理定制化的 GeMM 。小 batch size 会导致维度更瘦小 GeMM 运算操作：即参与 Gemm 运算的矩阵都是权重参数比激活大得多的矩阵，并且每个参数的总计算量受批量大小的限制。此时，GeMM 的性能主要取决于从主内存读取参数所花费的时间，即内存瓶颈，而不是 GPU 的计算瓶颈。因此，为了达到最佳性能，deepspeed 对推理内核进行了微调，以最大限度地利用内存带宽来加载参数。这项优化使得 DeepSpeed 推理内核在批量大小为 1-10 的推理工作负载上，实现比 NVIDIA cuBLAS 库高出 20% 的性能。</p>
</li>
<li>
<p>使用通用和专用的 Transformer 内核。deepspeed 推理部分使用了两种 transformers 内核来实现前文提到的两个优化方案：</p>
</li>
</ol>
<p>Generic Transformer：使用深度融合技术，将 Transformer 中的各个 PyTorch 操作（如 LayerNorm、Softmax 和 bias-add）替换为高度优化的 DeepSpeed 版本。<br>
Specialized Transformer：进一步利用深度融合技术，创建了融合调度，不仅在PyTorch的宏操作符（如Softmax）内部融合微操作符，还在多个宏操作符（如 Softmax 和 LayerNorm，以及转置操作和甚至 GeMM）之间进行融合。Specialized Transformer 内核的融合结构上图所示。</p>
<p>但在深究这些高大上的优化原理和实现之前，我们需要先用 deepspeed 跑一个模型，以此作为一个驱动例子，带领我们一步步深入下去：</p>
<h2 id="使用-deepspeed-完成模型推理">使用 deepspeed 完成模型推理</h2>
<h3 id="具体步骤">具体步骤</h3>
<p>首先，需要进入 NGC docker，再安装 deepspeed transformers 等库：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker run --init -it --name <span class="hljs-variable">$&#123;NAME&#125;</span> nvcr.io/nvidia/pytorch:23.03-py3 /bin/bash<br>pip install deepspeed transformers sentencepiece mpi4py<br></code></pre></td></tr></table></figure>
<p>然后，准备好模型和权重数据。敲入以下推理代码，以模型 <code>t5-v1_1-small</code> 为例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># create the model</span><br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline<br><span class="hljs-keyword">from</span> transformers.models.t5.modeling_t5 <span class="hljs-keyword">import</span> T5Block<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> deepspeed<br><br>local_rank = <span class="hljs-built_in">int</span>(os.environ[<span class="hljs-string">&quot;LOCAL_RANK&quot;</span>])<br>world_size = <span class="hljs-built_in">int</span>(os.environ[<span class="hljs-string">&quot;WORLD_SIZE&quot;</span>])<br>deepspeed.init_distributed()<br>pipe = pipeline(<span class="hljs-string">&quot;text2text-generation&quot;</span>, model=<span class="hljs-string">&quot;google/t5-v1_1-small&quot;</span>, device=local_rank)<br><span class="hljs-comment"># Initialize the DeepSpeed-Inference engine</span><br>pipe.model = deepspeed.init_inference(<br>    pipe.model,<br>    tensor_parallel=&#123;<span class="hljs-string">&quot;tp_size&quot;</span>: world_size&#125;,<br>    dtype=torch.<span class="hljs-built_in">float</span>,<br>    injection_policy=&#123;T5Block: (<span class="hljs-string">&#x27;SelfAttention.o&#x27;</span>, <span class="hljs-string">&#x27;EncDecAttention.o&#x27;</span>, <span class="hljs-string">&#x27;DenseReluDense.wo&#x27;</span>)&#125;<br>)<br>output = pipe(<span class="hljs-string">&#x27;Input String&#x27;</span>)<br></code></pre></td></tr></table></figure>
<p>然后就是启动 deepspeed 完成推理：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">deepspeed --num_gpus 2 inference.py<br></code></pre></td></tr></table></figure>
<h3 id="init-inference">init_inference</h3>
<p>简单地解释一下 <code>deepspeed.init_inference</code> 这个重要的 API。该 API 装入并初始化推理模型。第一个参数是传入的推理模型；<code>tensor_parallel</code> 传入推理时用的张量并行度参数，一般就是显卡的数量；第三个参数是运算类型；第四个 <code>injection_policy</code> 参数给那些不支持 deepspeed 内核的模型准备的，用户需要手动指定模型的“注入策略”，即 Transformer 层的两个特定线性层：1）attention output GeMM 和 2）layer output GeMM。deepspeed 内部会根据用户提供的层，增加必要的 all-reduce 通信以便将各个 GPU 上的计算结果合并起来。上面的 <code>t5-v1_1-small</code> 就是例子。当然不是所有的 transformers 库内的模型都可以这样做，<a target="_blank" rel="noopener" href="https://www.deepspeed.ai/tutorials/automatic-tensor-parallelism/#supported-models">官方文档中列出了目前支持的模型</a></p>
<p>但具体到 <code>injection_policy</code> 是如何确定的，又是如何实现并行的，我们还要接着往下看。但机敏的读者可以猜到，多半是根据 <a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/v4.40.0/src/transformers/models/t5/modeling_t5.py#L646">t5 模型源码里的 <code>T5Block</code> 类</a>内的 layer(nn.Module) 确定，实现 <code>SelfAttention.o</code>, <code>EncDecAttention.o</code>, <code>DenseReluDense.wo</code> 的计算并行。</p>
<p>此外，还有一些比较重要的参数，但在这个例子中没有体现，比如 <code>replace_with_kernel_inject=True</code> 可以将模型内的部分 kernel 替换成 deepspeed 内开发的高性能 kernel。</p>
<p>有时还需要完成 DeepSpeed 配置文件（通常为<code>ds_config.json</code>），指定使用的推理优化策略、参数等资源等，详细的配置说明可参考<a target="_blank" rel="noopener" href="https://www.deepspeed.ai/docs/config-json/">官方文档</a>。</p>
<h2 id="deepspeed-推理引擎">deepspeed 推理引擎</h2>
<p>简单尝试过使用 deepspeed 推理模型后，接下来我们就要分析底层原理，探究更深的奥秘。我们紧接上文，从 <code>init_inference</code> 这个 API 出发，发现其内部简化后也就是很简单的几句 python：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">init_inference</span>(<span class="hljs-params">model, config=<span class="hljs-literal">None</span>, **kwargs</span>):<br>  ds_inference_config = DeepSpeedInferenceConfig(**config_dict)<br>  engine = InferenceEngine(model, config=ds_inference_config)<br>  <span class="hljs-keyword">return</span> engine<br></code></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://deepspeed.readthedocs.io/en/latest/inference-init.html#deepspeed.inference.config.DeepSpeedInferenceConfig">DeepSpeedInferenceConfig</a> 就是配置 deepspeed 推理时的 config 数据。真正要启动的 <code>InferenceEngine</code> 是我们比较关心的。</p>
<h3 id="InferenceEngine-之-forward">InferenceEngine 之 forward</h3>
<p>找到 <code>InferenceEngine</code> 实现后，首先关注一下它的前推函数（有删减）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, *inputs, **kwargs</span>):<br>  <span class="hljs-string">&quot;&quot;&quot;Execute forward propagation</span><br><span class="hljs-string">  Arguments:</span><br><span class="hljs-string">    *inputs: Variable length input list</span><br><span class="hljs-string">    **kwargs: variable length keyword arguments</span><br><span class="hljs-string">  &quot;&quot;&quot;</span><br>  start = <span class="hljs-literal">None</span><br>  <span class="hljs-keyword">if</span> self.model_profile_enabled:<br>    <span class="hljs-comment"># 开始推理的计时</span><br>    get_accelerator().synchronize()<br>    start = time.time()<br><br>  <span class="hljs-keyword">if</span> self._config.enable_cuda_graph <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.local_cuda_graph:<br>    <span class="hljs-comment"># 如果可以的话 使用 cuda graph 推理</span><br>    <span class="hljs-keyword">if</span> self.cuda_graph_created:<br>      outputs = self._graph_replay(*inputs, **kwargs)<br>    <span class="hljs-keyword">else</span>:<br>      self._create_cuda_graph(*inputs, **kwargs)<br>      outputs = self._graph_replay(*inputs, **kwargs)<br>    <span class="hljs-comment"># 不行就直接推</span><br>  <span class="hljs-keyword">else</span>:<br>    outputs = self.module(*inputs, **kwargs)<br><br>  <span class="hljs-keyword">if</span> self.model_profile_enabled <span class="hljs-keyword">and</span> self._config.enable_cuda_graph:<br>    <span class="hljs-comment"># 结束计时</span><br>    get_accelerator().synchronize()<br>    duration = (time.time() - start) * <span class="hljs-number">1e3</span>  <span class="hljs-comment"># convert seconds to ms</span><br>    self._model_times.append(duration)<br><br>  <span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure>
<p>代码中提到的 <a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/cuda-graphs/">cuda graph</a> 是 cuda10 中为了加速模型计算流程而提出的优化特性，简单地说，CUDA Graphs 将整个计算流程定义为一个图，通过提供一种由单个 CPU 操作来启动图上的多个 GPU kernel 的方式减少 kernel 的启动开销。</p>
<p>另外从代码中看到的比较有用的东西，在这里插一句：transformers 中定义的模型基本都有 <code>register_forward_pre_hook</code> 和 <code>register_forward_hook</code> 这两个 hook 函数，可以很方便地让程序员在前推模型之前和之后插入自己的函数，方便调试或计时。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">profile_model_time</span>(<span class="hljs-params">self, use_cuda_events=<span class="hljs-literal">True</span></span>):<br>  <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.model_profile_enabled <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self._config.enable_cuda_graph:<br>    self.module.register_forward_pre_hook(self._pre_forward_hook)<br>    self.module.register_forward_hook(self._post_forward_hook)<br></code></pre></td></tr></table></figure>
<h3 id="tensor-parallelism-与-injection-policy">tensor_parallelism 与 injection_policy</h3>
<h4 id="三种模式">三种模式</h4>
<p>看了上小节的推理代码，你可能会想，这和一般的推理过程也没区别啊。别急，deepspeed 的优雅之处就是在于，它可以近似无伤地优化模型，但模型内部已经被 deepspeed 改动过了。就如同我们上文的 <code>t5-v1_1-small</code> 模型，它的很多 layer 层已经被注入高性能的 deepspeed kernel。</p>
<p>总的来讲，deepspeed 推理引擎支持三种模式：</p>
<ul>
<li>用户指定的张量并行策略   user specified policy for tensor-parallelism.</li>
<li>高性能 kernel 注入      kernel injection (replace_with_kernel_inject)</li>
<li>自动化张量并行          automatic tensor parallelism if tp_size &gt; 1.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 1. User specified Tensor Parallelism</span><br><span class="hljs-keyword">for</span> client_module, injection_policy <span class="hljs-keyword">in</span> self.injection_dict.items():<br>  <span class="hljs-comment"># 1.1 construct the tuple and pass that instead of a string or dict.</span><br>  config.injection_policy_tuple = injection_policy<br>  self._apply_injection_policy(config, client_module)<br><span class="hljs-comment"># 2. DeepSpeed Kernel Injection</span><br><span class="hljs-keyword">if</span> config.replace_with_kernel_inject:<br>  self._apply_injection_policy(config)<br><span class="hljs-comment"># 3. Automatic Tensor Parallelism</span><br><span class="hljs-keyword">elif</span> config.tensor_parallel.tp_size &gt; <span class="hljs-number">1</span>:<br>  <span class="hljs-comment"># tp_parser model</span><br>  parser_dict = AutoTP.tp_parser(model)<br>  <span class="hljs-keyword">for</span> client_module, injection_policy <span class="hljs-keyword">in</span> parser_dict:<br>      config.injection_policy_tuple = injection_policy<br>    self._apply_injection_policy(config, client_module)<br></code></pre></td></tr></table></figure>
<p>明显看得出来，<code>_apply_injection_policy</code> 函数在支持张量并行时起到了关键作用。<strong>三种模式都离不开该函数的支持</strong>。它获取了 <code>injection_dict</code> 中的数据，以前文 <code>t5-v1_1-small</code> 模型为例，<code>injection_dict</code> 就是 <code>T5Block: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')</code>。再次回顾一下，<code>injection_dict</code> 中输入的线性层要求是两个特定线性层中的一种：1）attention output GeMM 和 2）layer output GeMM。</p>
<p>我在运行 <code>t5-v1_1-small</code> 模型时将它的 layer 信息打印了出来，我们可以看看到底是哪些 kernel 会被 inject：</p>
<p><img src="/img/LLM/T5_inject_policy.png" srcset="/img/loading.gif" lazyload alt="T5 网络架构的部分截图"></p>
<p>从打印信息看到，encoder/decoder 的 attention 层计算，以及 DenseFFN 网络会被完成注入替换。</p>
<p>接下来的重头戏就是，我们要搞清楚它是如何完成注入替换的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_apply_injection_policy</span>(<span class="hljs-params">self, config, client_module=<span class="hljs-literal">None</span></span>):<br>  <span class="hljs-comment"># client_module is only passed when using the injection_dict method.</span><br>  checkpoint_dir = config.checkpoint<br>  checkpoint = SDLoaderFactory.get_sd_loader_json(checkpoint_dir,<br>                                                  self.checkpoint_engine) <span class="hljs-keyword">if</span> checkpoint_dir <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>  generic_injection(self.module, dtype=config.dtype, enable_cuda_graph=config.enable_cuda_graph)<br><br>  <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(self.module, torch.nn.Module):<br>    <span class="hljs-comment"># config is our DeepSpeedInferenceConfig and self.config is the HF model config</span><br>    replace_transformer_layer(client_module, self.module, checkpoint, config, self.config)<br></code></pre></td></tr></table></figure>
<p>从上面代码可以了解到，该函数将 kernel inject 任务划分为两部分：</p>
<ul>
<li><code>replace_transformer_layer</code> 完成 <code>nn.Module</code> 类的替换</li>
<li><code>generic_injection</code> 完成非 <code>nn.Module</code> 类的替换</li>
</ul>
<h4 id="replace-transformer-layer">replace_transformer_layer</h4>
<p>多数情况下大家的模型代码继承自 <code>nn.Module</code>，因此我们先来看看 <a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepSpeed/blob/v0.14.2/deepspeed/module_inject/replace_module.py#L182-L497"><code>replace_transformer_layer</code> 的源码</a>，看它是怎么处理 <code>nn.Module</code> 类的替换。</p>
<p>先回顾一下前文，避免被弄晕。在运行模型推理时，我们需要通过 <code>init_inference</code> API 中的 <code>injection_policy</code> 参数将模型内的某些 layer 层替换为 deepspeed 提供的高性能层。而用户传入的类名就是 <code>_apply_injection_policy(config, client_module)</code> 中的 <code>client_module</code>，也即是 <code>replace_transformer_layer</code> 的 <code>orig_layer_impl</code>。用 <code>t5-v1_1-small</code> 为例，<code>orig_layer_impl</code> 就是 <code>T5Block</code>，其中的 <code>injection_policy</code> 则是 ‘SelfAttention.o’, ‘EncDecAttention.o’, ‘DenseReluDense.wo’ 。而 <code>model</code> 是用户传入的整个模型。</p>
<p><img src="/img/LLM/replace_transformer_layer.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p>上面列出的函数调用栈（从上往下）可以看到，用户传入的 <code>injection_policy</code> 字典中，其键表示要执行 inject 的类，而其值对应了如何对该类进行优化的策略 <code>policy</code>：它在用户提供的 layer 层和 deepspeed 内有的高度优化的推理 transformer 层之间搭建起了一个映射。deepspeed 官方提供了不少基于 transformer 的高度优化过的推理层，如果用户提供的 layer 层恰好与这些相匹配，那么可以直接借鉴官方内部的优化策略。</p>
<p>下面来看一下这部分的源码（有删减）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">replace_transformer_layer</span>(<span class="hljs-params">orig_layer_impl, model, checkpoint_dict, config, model_config</span>):<br>  <span class="hljs-comment"># ...</span><br>  mp_replace = ReplaceWithTensorSlicing(mp_group=config.tensor_parallel.tp_group,<br>                                        mp_size=config.tensor_parallel.tp_size)<br>  <span class="hljs-keyword">if</span> checkpoint_dict <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> config.replace_with_kernel_inject:<br>    <span class="hljs-comment"># AutoTP shard loading</span><br>    checkpoint = checkpoint_dict[<span class="hljs-string">&quot;checkpoints&quot;</span>]<br>    pbar = tqdm.tqdm(total=<span class="hljs-built_in">len</span>(checkpoint), desc=<span class="hljs-string">f&quot;Loading <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(checkpoint)&#125;</span> checkpoint shards&quot;</span>)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(checkpoint)):<br>      checkpoint_file = os.path.join(config.base_dir, checkpoint[i])<br>      replaced_module = replace_module(model=model,<br>                                        orig_class=orig_layer_impl,<br>                                        replace_fn=replace_fn,<br>                                        _replace_policy=config.injection_policy_tuple,<br>                                        checkpoint=checkpoint_file)<br>    replaced_module = set_lm_head(replaced_module)<br>  <span class="hljs-keyword">else</span>:<br>    replaced_module = replace_module(model=model,<br>                                    orig_class=orig_layer_impl,<br>                                    replace_fn=replace_fn,<br>                                    _replace_policy=config.injection_policy_tuple)<br>  <span class="hljs-comment"># ...</span><br></code></pre></td></tr></table></figure>
<p>上面的源码中，最重要的就是这个函数：<code>replace_module</code>。它负责扫描整个模型，找到所有是 <code>orig_layer_impl</code> 类的实例，然后调用 <code>replace_fn</code> 函数将它们替换掉，<code>_replace_policy</code> 则是针对该 layer 层的优化策略。</p>
<p>这些源码篇幅较长，不适合全部粘贴放在此处，建议读者将本博客与 <a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/module_inject/replace_module.py#L573-L608">deepspeed replace_module的源码</a>一起服用，效果更佳。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">replace_module</span>(<span class="hljs-params">model, orig_class, replace_fn, _replace_policy, checkpoint=<span class="hljs-literal">None</span></span>):<br>  <span class="hljs-string">&quot;&quot;&quot; Scan the model for instances of ``orig_clas:`` to replace using ``replace_fn``.</span><br><span class="hljs-string">  Arguments:</span><br><span class="hljs-string">      model (torch.nn.Module): the model to augment</span><br><span class="hljs-string">      orig_class (torch.nn.Module): the module to search for</span><br><span class="hljs-string">      replace_fn (method): a method to convert instances of ``orig_class`` to the</span><br><span class="hljs-string">                            desired type and return a new instance.</span><br><span class="hljs-string">  Returns:</span><br><span class="hljs-string">      A modified ``model``.</span><br><span class="hljs-string">  &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># ... #</span><br>    policy = &#123;&#125;<br>    <span class="hljs-keyword">if</span> orig_class <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        policy.update(&#123;orig_class: (replace_fn, _replace_policy)&#125;)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">for</span> plcy <span class="hljs-keyword">in</span> replace_policies:<br>            <span class="hljs-comment"># instantiate a throw-away policy in order to populate the _orig_layer_class</span><br>            _ = plcy(<span class="hljs-literal">None</span>)<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(plcy._orig_layer_class, <span class="hljs-built_in">list</span>):<br>                <span class="hljs-keyword">for</span> orig_layer_class <span class="hljs-keyword">in</span> plcy._orig_layer_class:<br>                    policy.update(&#123;orig_layer_class: (replace_fn, plcy)&#125;)<br>            <span class="hljs-keyword">elif</span> plcy._orig_layer_class <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                policy.update(&#123;plcy._orig_layer_class: (replace_fn, plcy)&#125;)<br>    replaced_module, _ = _replace_module(model, policy, state_dict=sd)<br>    <span class="hljs-keyword">return</span> replaced_module<br></code></pre></td></tr></table></figure>
<p>总的来说，<code>replace_module</code> 函数的流程是这样的：</p>
<ol>
<li>根据输入的 <code>orig_class</code> 类名（本例中的 <code>T5Block</code>），寻找 deepspeed 中支持的 policy，如果有 deepspeed 能支持的 policy 分割法，那么直接就使用，否则保留。</li>
<li>进入 <code>_replace_module</code> 函数，开始递归处理模型。它会遍历 <code>model</code> 中所有的 layer，并找到 policy 中 layer 层</li>
<li>对于每个找到的 layer，调用 <code>replace_fn</code> 函数做替换</li>
<li>对于不在 policy 中的 layer 层，使用自动化张量并行(AutoTP)的方式处理，并递归调用 <code>_replace_module</code>，处理其子层。</li>
<li>若没有输入 <code>orig_class</code> 类名，那么 <code>replace_module</code> 函数会将所有 deepspeed 内支持的 policy 都存放到字典中，并在递归函数中逐个去比对，比对成功后替换。</li>
</ol>
<p>那么 <code>replace_fn</code> 函数如何将原来的 layer 层做替换的呢？对于“高性能 kernel 注入”的模式，使用 <code>replace_with_policy</code>；对于“用户指定的张量并行策略”模式，使用 <code>replace_wo_policy</code> 处理：</p>
<h4 id="replace-wo-policy">replace_wo_policy</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">replace_fn</span>(<span class="hljs-params">child, _policy, layer_id=<span class="hljs-number">0</span>, prefix=<span class="hljs-string">&quot;&quot;</span>, state_dict=<span class="hljs-literal">None</span></span>):<br>  training = <span class="hljs-literal">False</span>  <span class="hljs-comment"># todo: refactor this part to go in the config</span><br>  <span class="hljs-keyword">if</span> training:<br>    <span class="hljs-comment"># copy relevant state from child -&gt; new module</span><br>    new_module = replace_with_policy(child, _policy, config.triangular_masking)<br>  <span class="hljs-keyword">else</span>:<br>    <span class="hljs-comment"># copy relevant state from child -&gt; new module</span><br>    <span class="hljs-keyword">if</span> config.replace_with_kernel_inject:<br>        new_module = replace_with_policy(child,<br>                                          _policy,<br>                                          config.triangular_masking,<br>                                          inference=<span class="hljs-literal">True</span>,<br>                                          layer_id=layer_id)<br>    <span class="hljs-keyword">else</span>:<br>        new_module = replace_wo_policy(child, _policy, prefix=prefix, state_dict=state_dict)<br>  <span class="hljs-keyword">return</span> new_module<br></code></pre></td></tr></table></figure>
<p>我们来看一下 <code>replace_wo_policy</code> 如何实现的，以及它是如何处理我们上面给出的例子的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">replace_wo_policy</span>(<span class="hljs-params">module, all_reduce_linears, prefix=<span class="hljs-string">&quot;&quot;</span>, state_dict=<span class="hljs-literal">None</span></span>):<br>  <span class="hljs-comment"># 1. 创建 AutoTP 类</span><br>  _autotp = AutoTP(module, all_reduce_linears, prefix, state_dict, linear_layer_setting, orig_layer_impl)<br>  <span class="hljs-comment"># 2. 配置 tensor parallelism config</span><br>  _autotp.set_tensor_parallel_config(config.tensor_parallel.tp_size, config.tensor_parallel.tp_group)<br>  <span class="hljs-comment"># 3. 获得 num_key_heads from model_config.num_key_value_heads</span><br>  num_kv_heads = _autotp.get_model_num_kv_heads(model_config)<br>  <span class="hljs-comment"># 4. When we have num_kv_heads defined, uneven division is possible, otherwise enforce even division</span><br>  set_num_kv_heads(num_kv_heads)<br>  <span class="hljs-comment"># 4.1 Get n_embd</span><br>  n_embd = <span class="hljs-literal">None</span><br>  multi_query_n_embd_names = [<span class="hljs-string">&#x27;n_embd&#x27;</span>]<br>  <span class="hljs-keyword">for</span> name <span class="hljs-keyword">in</span> multi_query_n_embd_names:<br>      <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(model_config, name):<br>          n_embd = <span class="hljs-built_in">getattr</span>(model_config, name)<br>      <span class="hljs-keyword">if</span> n_embd != <span class="hljs-literal">None</span>:<br>          <span class="hljs-keyword">break</span><br>  <span class="hljs-comment"># 4.2 set n_embd</span><br>  set_n_embd(n_embd)<br>  <span class="hljs-comment"># 5. Set linear policies</span><br>  _autotp.update_linear_policies()<br>  <span class="hljs-comment"># 6. Replace modules</span><br>  <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;lm_head&quot;</span> <span class="hljs-keyword">in</span> all_reduce_linears <span class="hljs-keyword">or</span> <span class="hljs-string">&quot;embed_out&quot;</span> <span class="hljs-keyword">in</span> all_reduce_linears:<br>      <span class="hljs-keyword">return</span> _autotp._replace_last_linear_module(module)<br>  <span class="hljs-keyword">return</span> _autotp._replace_module(module)<br></code></pre></td></tr></table></figure>
<p>关于 AutoTP 是怎么实现的，这里不再赘述。但我可以简单地描述一下这里的操作。<code>replace_wo_policy</code> 的参数中，<code>module</code> 就是输入的要被替换的网络层的类名，<code>all_reduce_linear</code> 表示输入的 policy。仍以 <code>t5-v1_1-small</code> 为例，<code>module</code> 就是 <code>T5Block</code>，其中的 <code>all_reduce_linear</code> 则是 ‘SelfAttention.o’, ‘EncDecAttention.o’, ‘DenseReluDense.wo’。<code>_autotp.update_linear_policies</code> 中会将 <code>all_reduce_linear</code> 中记录的 <code>nn.linear</code> 层用自己内部的函数做张量切分，并使用 <code>inference_all_reduce</code> 做运算，因此需要被替换为 <code>LinearAllreduce</code> 类（见下）。因为这是一个 all reduce 操作，所以取名为 <code>all_reduce_linear</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> deepspeed <span class="hljs-keyword">import</span> comm <span class="hljs-keyword">as</span> dist<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LinearAllreduce</span>(nn.Module):<br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, weight, bias=<span class="hljs-literal">None</span>, mp_group=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-built_in">super</span>(LinearAllreduce, self).__init__()<br>    self.weight = weight<br>    self.bias = bias<br>    self.mp_group = mp_group<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):<br>    output = torch.matmul(<span class="hljs-built_in">input</span>, self.weight.transpose(-<span class="hljs-number">1</span>, -<span class="hljs-number">2</span>))<br>    <span class="hljs-keyword">if</span> self.mp_group <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        dist.inference_all_reduce(output, group=self.mp_group)<br>    <span class="hljs-keyword">if</span> self.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        output += self.bias<br>    <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure>
<h4 id="replace-with-policy">replace_with_policy</h4>
<p>最后，我们来看一下 <code>replace_with_policy</code> 如何实现的。因为这里的网络层替换有对应的 policy 类，所以 deepspeed 会根据对应的 model，构建出相应的 policy 类和包含它的 container 类。在 container 类做好张量初始化和拆分后，准备好数据和 config 信息，再实例化一个内部的高性能 model 类，最后返回：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">replace_with_policy</span>(<span class="hljs-params">child, policy_cls, triangular_masking, inference=<span class="hljs-literal">False</span>, layer_id=<span class="hljs-number">0</span></span>):<br>  <span class="hljs-comment"># 0. 构造与 model 对应的 policy 类</span><br>  policy = policy_cls(child, inference=inference)<br>  <span class="hljs-comment"># ...</span><br>  <span class="hljs-comment"># 1. create model-specific container object using the policy object.</span><br>  _container = policy_to_ds_container(policy=policy,<br>                                      config=config,<br>                                      model_config=model_config,<br>                                      layer_id=layer_id,<br>                                      child=child)<br>  <span class="hljs-comment"># 2. Set the tensor parallelism config</span><br>  _container.set_tensor_parallel_config(config.tensor_parallel.tp_size, config.tensor_parallel.tp_group)<br><br>  <span class="hljs-comment"># 3. Initialize tensors</span><br>  _container.initialize_tensors()<br><br>  <span class="hljs-comment"># 4. deal with data types -- needs refactor to use dtype instead of fp16</span><br>  <span class="hljs-keyword">if</span> config.dtype <span class="hljs-keyword">in</span> [torch.float16, torch.bfloat16, torch.int8]:<br>      _container.convert_to_required_dtype()<br><br>  <span class="hljs-comment"># 5. Set the quantization config</span><br>  quantizer = GroupQuantizer(q_int8=quantize)<br>  _container.set_quantization_config(quantizer)<br><br>  <span class="hljs-comment"># 6. create a DS Inference config object</span><br>  _container.create_ds_model_config()<br><br>  <span class="hljs-comment"># 7. use the config and create the module</span><br>  _container.create_module()<br><br>  <span class="hljs-comment"># 8. transpose the weights and bias if needed</span><br>  _container.transpose()<br><br>  <span class="hljs-comment"># 9. deal with tensor parallelism.</span><br>  _container.apply_tensor_parallelism(mp_replace)<br><br>  <span class="hljs-comment"># 10. copy the tensors from the model-specific container to the new module</span><br>  _container.copy_data_to_new_module()<br><br>  <span class="hljs-comment"># 11. set global for generic checkpoint loading</span><br>  <span class="hljs-keyword">global</span> container_g<br>  <span class="hljs-keyword">if</span> container_g <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>      container_g = _container<br>  <span class="hljs-keyword">return</span> _container.module<br></code></pre></td></tr></table></figure>
<p>这里以 llama2 模型为例。首先程序会构造与 llama 对应的 <code>LLAMA2LayerPolicy</code> 类和 <code>DS_LLAMA2Container</code> 类。然后，程序开始依顺序准备模型的 TP、量化等 config，再创建内部的 llama 模型 <code>DeepSpeedLlama2Inference</code>，并将张量数据切分后拷贝到新模型中，最后返回。<code>DeepSpeedLlama2Inference</code> 就是 deepspeed 针对 llama2 开发的高性能推理模型。</p>
<h4 id="generic-injection">generic_injection</h4>
<p>再顺便看看 <code>generic_injection</code> 怎么处理非 <code>nn.Module</code> 类。<a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepSpeed/blob/v0.14.2/deepspeed/module_inject/replace_module.py#L87-L177">从 <code>generic_injection</code> 的源码可以看出</a>，它要替换的注意力块来自于 <a target="_blank" rel="noopener" href="https://github.com/huggingface/diffusers">diffusers 库</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> diffusers<br><span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(diffusers.models.attention, <span class="hljs-string">&#x27;CrossAttention&#x27;</span>):<br>  cross_attention = diffusers.models.attention.CrossAttention<br><span class="hljs-keyword">else</span>:<br>  cross_attention = diffusers.models.attention_processor.Attention<br>attention_block = diffusers.models.attention.BasicTransformerBlock<br>new_policies = &#123;<br>  cross_attention: replace_attn,<br>  attention_block: replace_attn_block,<br>&#125;<br></code></pre></td></tr></table></figure>
<p>使用 deepspeed 自己实现的 <code>DSClipEncoder</code> 替换模型的 <code>text_encoder</code> 部分。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> ..model_implementations.transformers.clip_encoder <span class="hljs-keyword">import</span> DSClipEncoder<br>cg_encoder = DSClipEncoder(module.text_encoder, enable_cuda_graph=enable_cuda_graph)<br><span class="hljs-built_in">setattr</span>(module, <span class="hljs-string">&#x27;text_encoder&#x27;</span>, cg_encoder)<br></code></pre></td></tr></table></figure>
<p>然后就是遍历模型的所有子模块，并一一检查它们是否与先前定义的 <code>new_policies</code> 中的替换策略匹配：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> name <span class="hljs-keyword">in</span> module.__dict__.keys():<br>  <span class="hljs-comment"># 遍历，查看是否match</span><br>  sub_module = <span class="hljs-built_in">getattr</span>(module, name)<br>  policy = _module_match(sub_module)<br><br>  <span class="hljs-keyword">if</span> policy <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_replace_module</span>(<span class="hljs-params">module, policy</span>):<br>      <span class="hljs-keyword">for</span> name, child <span class="hljs-keyword">in</span> module.named_children():<br>        _replace_module(child, policy)<br>        <span class="hljs-keyword">if</span> child.__class__ <span class="hljs-keyword">in</span> new_policies:<br>          replaced_module = new_policies[child.__class__](child, policy)<br>          <span class="hljs-built_in">setattr</span>(module, name, replaced_module)<br>    <span class="hljs-comment"># 找到match的module后做替换</span><br>    _replace_module(sub_module, policy)<br>    new_module = policy.apply(sub_module, enable_cuda_graph=enable_cuda_graph)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;**** found and replaced <span class="hljs-subst">&#123;name&#125;</span> w. <span class="hljs-subst">&#123;<span class="hljs-built_in">type</span>(new_module)&#125;</span>&quot;</span>)<br>    <span class="hljs-built_in">setattr</span>(module, name, new_module)<br></code></pre></td></tr></table></figure>
<p>对于每个匹配的子模块，使用 <code>_replace_module</code> 函数递归地替换其子模块。在这里，<code>new_policies</code> 是一个字典，其中键是类（如 <code>CrossAttention</code> 或 <code>BasicTransformerBlock</code>），而值是一个函数：<code>replace_attn</code> 或者 <code>replace_attn_block</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">replace_attn</span>(<span class="hljs-params">child, policy</span>):<br>  policy_attn = policy.attention(child)<br>  <span class="hljs-comment"># ...</span><br>  <span class="hljs-keyword">return</span> attn_module<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">replace_attn_block</span>(<span class="hljs-params">child, policy</span>):<br>  config = Diffusers2DTransformerConfig()<br>  <span class="hljs-keyword">return</span> DeepSpeedDiffusersTransformerBlock(child, config)<br></code></pre></td></tr></table></figure>
<p>该函数接受原始模块和策略作为参数，并返回替换后的模块。最后，<code>policy</code> 内实现的新模块完成对模型的进一步替换：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">new_module = policy.apply(sub_module, enable_cuda_graph=enable_cuda_graph)<br></code></pre></td></tr></table></figure>
<p>目前 deepspeed 支持的 <code>replace_policy</code> 有很多，具体见其内部实现的 <a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepSpeed/blob/v0.14.2/deepspeed/module_inject/replace_policy.py">replace_policy.py 文件</a>，而 <code>generic_policies</code> 适用的有 <code>UNetPolicy</code>, <code>VAEPolicy</code>。它们具体实现有兴趣的读者可自己查阅代码。</p>
<h1>总结</h1>
<p>DeepSpeed 作为一款强大的深度学习优化库，为研究人员和开发者提供了高效的训练解决方案。网上许多极客对该框架进行了研究，但并不是很深入，多是翻译搬运，少了自己的理解和对源码的深入挖掘。接下来我还会对 deepspeed 展开详细的研究。</p>
<p>下一步，我会继续沿着 DeepSpeed 推理的路径出发，研究其推理的高性能的算子融合，包括其专用和通用的 transformer kernel 实现。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/AI/" class="category-chain-item">AI</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/AI/" class="print-no-link">#AI</a>
      
        <a href="/tags/deepspeed/" class="print-no-link">#deepspeed</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>深入探索 deepspeed（一）</div>
      <div>https://dingfen.github.io/2024/04/20/2024-4-20-deepspeed/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Bill Ding</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年4月20日</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>更新于</div>
          <div>2025年1月26日</div>
        </div>
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="SA - 相同方式共享">
                    <i class="iconfont icon-sa"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/05/15/2024-5-15-deepspeed/" title="深入探索 deepspeed（二）">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">深入探索 deepspeed（二）</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/04/10/2024-4-10-hello-world/" title="将博客切换到 hexo Fluid 主题">
                        <span class="hidden-mobile">将博客切换到 hexo Fluid 主题</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  








    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  var relativeDate = function() {
    var updatedTime = document.getElementById('updated-time');
    if (updatedTime) {
      var text = updatedTime.textContent;
      var reg = /\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:Z|[+-]\d{2}:\d{2})/;
      var matchs = text.match(reg);
      if (matchs) {
        var relativeTime = moment(matchs[0]).fromNow();
        updatedTime.textContent = text.replace(reg, relativeTime);
      }
      updatedTime.style.display = '';
    }
  };
  Fluid.utils.createScript('https://lib.baomitu.com/moment.js/2.29.4/moment.min.js', function() {
    if (!'zh-cn'.startsWith('en')) {
      Fluid.utils.createScript('https://lib.baomitu.com/moment.js/2.29.4/locale/zh-cn.min.js', function() {
        relativeDate();
      });
    } else {
      relativeDate();
    }
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  
<script src="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/Ribbon.min.js"></script>



<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<!-- hexo injector body_end start -->
  <link rel="stylesheet" crossorigin href="https://g.alicdn.com/aliyun-documentation/web-chatbot-ui/0.0.11/index.css" />
  <script type="module" crossorigin src="https://g.alicdn.com/aliyun-documentation/web-chatbot-ui/0.0.11/index.js"></script>
  <script>
    window.CHATBOT_CONFIG = {
      endpoint: "https://web-chatbot-syz-knthhrjfeq.cn-hangzhou.fcapp.run/chat", // 可以替换为 https://{your-fc-http-trigger-domain}/chat
      displayByDefault: false, // 默认不显示 AI 助手对话框
      aiChatOptions: { // 自定义取值参考：https://docs.nlkit.com/nlux/reference/ui/ai-chat#conversation-options
        conversationOptions: { // 自定义取值参考：https://docs.nlkit.com/nlux/reference/ui/ai-chat#conversation-options
          conversationStarters: [
            {prompt: '请问你是谁，能为我做什么？'},
            {prompt: '请介绍一下博客的主人'}
          ],
          layout: 'bubbles'
        },
        displayOptions: { // 自定义取值参考：https://docs.nlkit.com/nlux/reference/ui/ai-chat#display-options
          height: 550,
          // width: 400,
        },
        personaOptions: { // 自定义取值参考：https://docs.nlkit.com/nlux/reference/ui/ai-chat#chat-personas
          assistant: {
            name: '你好，我是本网站的 AI 助手',
            // AI 助手的图标
            avatar: 'https://img.alicdn.com/imgextra/i2/O1CN01Pda9nq1YDV0mnZ31H_!!6000000003025-54-tps-120-120.apng',
            tagline: '输入您的问题，我会尽力帮你解答！',
          }
        }
      }
    };
  </script>
  <style>
    :root {
      /* webchat 工具栏的颜色 */
      --webchat-toolbar-background-color: #1464E4;
      /* webchat 工具栏文字和按钮的颜色 */
      --webchat-toolbar-text-color: #FFF;
    }
    /* webchat 对话框如果被遮挡，可以尝试通过 z-index、bottom、right 等设置 来调整*/
    .webchat-container {
      z-index: 100;
      bottom: 10px;
      right: 10px;
    }
    /* webchat 的唤起按钮如果被遮挡，可以尝试通过 z-index、bottom、right 等设置 来调整。也可以通过 CSS 进一步定制唤起按钮的形状、大小等。 */
    .webchat-bubble-tip {
      z-index: 99;
      bottom: 20px;
      right: 20px;
    }
  </style>
  <!-- hexo injector body_end end --></body>
</html>
